00:00
[음악]
00:06
스탠포드 대학교.
00:11
>> 네 그럼 시작하죠.
00:14
CS224N / Ling 284의 두 번째 클래스에 오신 것을 환영합니다.
00:18
딥러닝을 통한 자연어 처리.
00:22
그래서 이 수업은 지난 수업의 거의 반대로 진행 될 것입니다.
00:28
지난 수업은,
00:29
위에서 아래로 내려가는 일종의 거시적 그림이었습니다.
00:35
자연어 처리와,
00:39
딥러닝은 무엇이며, 왜 둘 다 흥미롭고
00:43
그 둘을 엮고 싶은지에 대한 것이었습니다.
00:46
오늘 수업은 반대쪽 극단으로 갈 것입니다.
00:50
가장 아래쪽에 있는 단어부터 시작해봅시다.
00:54
벡터와 기초 수학을 할 것입니다.
00:58
여러분 중 일부는 지루하고 반복적인 기초 수학처럼 보일 것입니다.
01:04
하지만 아마도 여러분 중 상당수는
01:08
몇 가지 수학 리뷰가 유용할 것입니다.
01:12
그리고 이것이 실제로 다른 모든 것들을 만들어 내는 토대입니다.
01:17
시작할 때 기본 원리를 건너뛰면
01:21
어떻게 아주 단순한 구조에 신경망을 어떻게 사용할 수 있는지와 같은 것을 알 수 있겠습니까?
01:26
모든 것이 시작되는 처음부터 하겠습니다.
01:28
오늘은 천천히 나갈 것입니다.
01:32
신중하게 뉴럴 네크워크를 시작할 수 있는 토대로
01:38
단어 표상을 학습하는 매우 간단한 케이스를 배웁니다.
01:43
그것이 앞으로 나아갈 수 있는 좋은 기초가 되기를 바랍니다.
01:47
그리고 실제로 그것이 토대로 앞으로 계속 나아가기를 바랍니다.
01:51
다음 주 리처드도 수학을 계속할 것입니다.
01:56
바닥부터 시도하니까 딥러닝의 기초 중 일부를 얻는데 도움이 되기를 바랍니다.
02:01
좋아요, 이것이 기본 계획입니다.
02:05
그래서 작은 비트 단어의 의미와
02:11
아니, [웃음].
02:14
>> [웃음] >> 단어 의미의 작은 비트 단위부터 시작합니다.
02:19
Word2vec이라는 단어 벡터 학습 모델을 소개하겠습니다.
02:25
그리고 이것은 Thomas Mikolov가 소개한 
02:29
2013 년 구글 모델입니다.
02:31
다른 많은 방법들도 생각할 수 있습니다.
02:35
단어 표상을 위해서요.
02:37
다음 주 리처드는 다른 메커니즘에 대해 이야기 할 것입니다.
02:41
하지만 오늘은 일종의 배경과
02:45
비교 설명을 할 것입니다.
02:46
그래서 저는 이 일을 할 수 있는 한 가지 방법만을 제시 할 것입니다.
02:50
이것은 연구를 진행할 때 꽤 유용한 방법으로
02:52
알아 두면 좋을 것입니다.
02:55
좋아요, 그 다음에 우리는 첫 번째로
02:58
이 수업의 특징 중 하나가 될 텐데요,
03:01
이미 증명된 것처럼 학생들이 75 분 동안 집중할 수 없다고 판단했습니다.
03:07
그래서 우리는 우리가 조금씩 섞어서 하기로 결정했습니다.
03:11
이렇게 하면 감각을 더 많이 얻을 수 있는 기회를 제공할 수 있을 것입니다.
03:16
매달 딥러닝에서 나오는 흥미진진한 새로운 작품에 대해서요.
03:22
그래서 우리가 할 일은 매번 한 번씩 TA 발표를 하는 것입니다. 연구가 약간 강조됩니다.
03:27
일종의 말로 하는 블로그 게시물처럼 될 것입니다.
03:30
최근 논문과 그것이 흥미로운 이유에 대해 조금 더 설명하겠습니다.
03:35
오늘 Danqi부터 시작하겠습니다.
03:38
그 후에,
03:39
나는 워드벡터로부터 목적 함수 그래디언트(objective function gradient)까지 할 것입니다.
03:44
최적화에 대한 약간의 환기와, 과제를 언급할 것입니다.
03:47
기본적으로는 모두 Word2vec에 관한 이야기가 될 것입니다. 괜찮나요?
03:51
일종의 워드 벡터로
03:55
지난 번 언급 한 단어 의미의 모델을 만드는 것은
03:59
사실 꽤 논쟁의 여지가 있는 아이디어입니다.
04:02
본격적으로 들어가지 전에 컨텍스트에 대해 몇 마디
04:07
하려고 합니다.
04:08
사전에서 의미에 대해 찾으면
04:12
결국 사전은 단어 의미의 창고니까요.
04:17
웹스터의 사전에서 말하는 의미는 다음과 같이 표현되는 아이디어입니다.
04:21
단어, 구 등에서
04:24
단어나 기호 등을 사용하여 사람이 표현하고자 하는 아이디어라고 정의하고 있습니다.
04:30
어떤 면에서는
04:34
의미에 대한 가장 일반적인 언어적 방법과 상당히 비슷하죠?
04:38
언어학에서의 표준은, 우리가 단어라는 언어적 기호를 가지고 있다는 것입니다.
04:45
그리고 그것이 이 세상에 어떤 것을 의미하게 됩니다.
04:50
그래서 안경이라고 말을 하면
04:55
지금 제 앞에서 볼 수 있는 많은 안경을 포괄하는 의미가 있습니다.
05:01
그리고 이것이 의미일 것입니다,
05:05
안경이라는 용어의 의미로 말입니다.
05:09
하지만 그렇게 하는 것은 사람들에게 매우 쉬운 의미의 개념이지만
05:15
언어를 다루기 위한 계산 시스템에서는 입증되지 못했습니다.
05:21
따라서 실제로, 만약 계산 시스템이 무엇을 했는지 살펴보려면
05:25
지난 수십 년 동안의 단어의 의미 파악을 위해
05:29
가장 일반적으로 사람들이 많이 시도한 것은
05:33
분류된 리소스(taxonomic resource)를 이용하여 단어의 의미를 다루는 것이었습니다.
05:38
영어의 경우 가장 유명한 분류 리소스는 WordNet입니다.
05:42
웹스터만큼은 아닐지 모르지만 꽤 유명하죠.
05:46
전산 언어학자들 사이에서는 유명합니다.
05:48
사본을 무료로 다운로드 할 수도 있습니다.
05:51
선반에 Webster의 사본을 갖는 것보다 훨씬 유용하고
05:55
단어에 관한 분류 정보를 많이 제공해 줍니다.
06:01
여기 몇 줄의 파이썬 코드가 있습니다.
06:03
이 중 하나로 nltk를 사용하여 워드넷을 보여주려고 하는데.
06:07
자연어 처리를 위한 주요 파이선 패키지로
06:10
제가 판다라는 단어를 물으면
06:13
파이선 패키지 Panda가 아니라 판다요
06:17
제가 말하면,
06:18
상의어와 같은 것을 알려 줍니다.
06:23
판다의 경우 육식 동물, 태반,
06:28
포유류 등이 대상의 추상 유형으로 분류되어 있습니다.
06:31
또는 오른쪽에
06:34
‘좋다’라는 단어의 동의어를 찾아 보았습니다.
06:38
여기서 알 수 있는 것은, WordNet이
06:42
‘좋다’라는 단어에 여러 센스를 보여 준다는 것입니다.
06:45
여기에 각 센스에서의, 동의어가 있습니다.
06:49
가령, 센스중 하나로, 두 번째 것은 일종의 좋은 사람의 센스입니다.
06:54
‘존경’과 ‘공경’ 같은 동의어를 제안하고 있습니다.
06:58
그러나 여기는 ‘먹기 좋다’의 의미쌍이 있고
07:04
‘잘 익은’의 의미도 있습니다.
07:08
좋아요, 이제 일종의 의미라는 것의 센스를 알게 되었을 것입니다.
07:12
이것은 훌륭한 리소스였고
07:16
사람들이 실제에서 발견한 자원이기도 합니다.
07:20
여기서 벗어나고 싶다고 해도 가치 있는 것을 얻는 것은 매우 어렵습니다.
07:26
왜 그걸까요?
07:27
여러 가지 이유가 있겠지만
07:30
그 중 한 가지는 이러한 종류의 분류학적 관계 맺기의 수준에서는,
07:35
엄청난 양의 뉘앙스를 잃게 됩니다.
07:38
따라서 ‘좋다’의 동의어 세트 중  숙달되었다, 전문가다, 훌륭하다, 단련되었다,
07:43
능숙하다, 숙련되었다.
07:44
등이 있는데, 이 의미들은 꽤  다른 것 같지 않나요?
07:49
제가 “저는 딥러닝 전문가입니다”라고 말하면
07:53
“저는 딥러닝에 능숙합니다”라고 말하는 것과 좀 다른 같아요.
07:59
많은 뉘앙스 차이가 있습니다.
08:01
WordNet의 불완전성은 사람들이
08:06
단어를 더 유연하게 쓰기 때문이기도 합니다.
08:10
그래서 제가 만약 저는 딥러닝 – 닌자입니다.
08:14
라고 말하면 WordNet에는 그 단어가 없을 것입니다.
08:18
이 동의어 세트 중 어떤 것들은 매우 주관적입니다.
08:23
맞나요?
08:23
어느 의미로 구분하고 어떤 일을 하는지
08:27
똑같지도 않고, 대체로 매우 불분명합니다.
08:29
그런데 이 정도 만들어지는 것조차도,
08:32
많은 인력이 필요하고
08:37
그리고 최종 작업시까지도
08:43
단어 유사성을 정확하게 얻는 것이 어렵습니다.
08:47
proficient는 good보다는 expert에 가까운 것 같은데
08:53
WordNet에서는 이런 것은 알 수가 없습니다.
08:58
이런 것은 일종의
09:03
이산형과 범주형의 일반적 문제이고
09:08
제가 지난 시작할 때 썼던 표현도 같은 문제입니다.
09:13
그래서, 근본적으로 주목해야 할 것은
09:16
자연어처리와 별도로 현대의 딥러닝은
09:21
1980 년대에 완성 된 자연어처리 뉴럴 네트워크의 일부는,
09:25
호텔, 회의, 도보와 같은 것을 원자화된 기호로 사용한다는 것입니다.
09:31
그리고 우리가 우리의 종류의 편견이 가까운 신경망 방향에서 생각한다면,
09:36
원자화된 기호를 사용하는 것은
09:40
하나의 위치에서 다른 위치까지 떨어져있는 0 이 많은 큰 벡터를 쓴다는 것입니다.
09:45
그러면 우리는 단어의 상징으로 얼마나 많은 같은 값을 가지고 있을까요?
09:49
여기에 벡터의 위치에 하나를 넣을 것입니다.
09:53
아마도 특정 호텔, 아마도 호텔을 표현하는 값이 되겠죠?
09:55
그리고 이 벡터들은 정말로 길게 될 것입니다.
09:59
얼마나 긴 가는 여러분이 무엇을 보느냐에 달려 있습니다.
10:01
때로는 음성 인식기에 2 만 단어 어휘가 있을 수도 있습니다.
10:05
그렇다면 그 정도의 길이일 것입니다.
10:07
그러나 우리가 기계 번역 시스템을 구축한다면,
10:10
50 만 단어 정도의 어휘를 사용하기 때문에 매우 길어 집니다.
10:16
그리고 구글은 1 테라 바이트의 웹 크롤링 코퍼스를 발표했는데.
10:21
이것은 자연어처리에서 많이 사용하는 리소스로.
10:24
그 어휘의 크기는 1300 만 단어이므로
10:27
정말, 정말로 길어 집니다.
10:29
아주 커다란 벡터입니다.
10:33
그렇다면 이 벡터들은 왜 문제가 됩니까?
10:37
죄송합니다, 슬라이드가 기억 나지 않아서 슬라이드를 먼저 말해야겠습니다.
10:43
네, 신경망 코딩에서 이것을 one-hot이라고 말합니다.
10:47
왜냐하면 벡터에 0이 있기 때문입니다.
10:51
그래서, 그것은 지역적인 표현의 예시입니다.
10:57
그러면 왜 이것이 문제가 됩니까?
10:59
문제가 되는 이유는
11:04
단어들 간의 관계에서 내재된 개념에 대해서 아무 것도 알려 주지 않기 때문입니다.
11:07
보통 우리가 일반적으로 알고 싶은 것은 언제
11:12
단어나 구의 의미는 서로 비슷하게 쓰이는가 하는 것입니다.
11:15
예를 들어, 웹 검색 응용 프로그램에서 사용자가
11:19
델 노트북 배터리 크기라고 하면
11:21
델 랩톱 배터리 용량을 나타내는 문서와 일치시키고 싶을 것입니다.
11:26
그래서 우리가 알고 싶은 것은 노트북과 랩톱이 비슷하다는 것입니다.
11시 29 분
크기와 용량도 비슷하므로 이 값은 동일해야 합니다.
11시 33 분
호텔과 모텔의 의미가 비슷하다는 것을 알고 싶을 수 있습니다.
11:37
문제는 만약 원핫 벡터 인코딩을 사용한다면,
11:42
유사성에 대한 자연스러운 개념이 없어지게 됩니다.
11:45
그래서 이 두 개의 벡터에서,
11:48
벡터 사이의 내적은 무엇입니까? 0입니다.
11:51
그들은 본질적으로 유사성에 대한 개념을 가지고 있지 않습니다.
11:56
그리고, 제가 중요하게 생각하고 강조하고 싶은 것은,
12:00
이 상징 체계를 인코딩하는 문제는 전통적인
12:05
논리 기반의 자연어 처리나
12:10
또는 확률적 통계 기반의 기계 학습 자연어 처리를 포함한
12:15
모든 작업에 기본적으로 적용됩니다.
12:20
라틴어 모델은 보통 실수였이지만
12:25
그럼에도 불구하고, 컨텍스트의 어딘가에서 발생할 활률을 
12:29
그들은 상징의 표상 위에 세웠습니다.
12:32
결국, 단어와 단어 사이의 관계를 포착한 것은 없고
12:37
각 단어는 개념 그 자체로 있을 뿐입니다.
12:42
이건 잘못되어 있고 뭔가 해야 합니다.
12:46
지금 말했듯이, 우리가 그것에 대해 할 수 있는 것이 하나 이상 있습니다.
12:53
여기서 한 가지 가능한 답은,
12:56
단어 사이에 유사성에 대한 관계 맺기가 필요하다는 것입니다.
12:59
일단 여기서는
13:00
단어 사이의 유사성 관계를 완전히 별도로 다루어 봅시다.
13:05
당연히 그렇게 할 수 있겠죠?
13:06
여기서 구체적으로 이야기하지는 않겠습니다.
13:09
대신에 제가 제안 하고 싶은 것은
13:14
우리가 이 직접 접근법으로 탐색 할 수 있다는 것입니다.
13:19
단어의 표현은 그 의미를 부호화하는 방법으로 이루어지는데
13:25
우리가 이 표현들을 직접 읽는 방식처럼
13:29
표현과 단어 사이의 유사성에서부터 출발해서
13:34
이 벡터들의
13:36
내적을 구할 것입니다.
13:38
그렇게 하면 단어들 사이의 유사성에 대한 감각을 기를 수 있을 것입니다.
13:44
그런 다음에 무엇을 하면 될까요?
13:46
우리가 할 일은
13:53
매우 간단하지만 매우 심오하고 광범위하게 사용되는,
13:57
분산 유사성 (distributional similarity)이라고 하는 자연어처리의 아이디어를 쓰는 것입니다.
14:00
이것은 정말로 강력한 개념입니다.
14:03
우리는 분산 유사성의 개념에서 많은 가치를 얻을 수 있습니다.
14:09
컨텍스트를 보고 단어의 의미를 나타내는 것으로
14:14
컨텍스트를 바탕으로 해서 뭔가를 할 수 있습니다.
14:19
제가 banking이라는 단어가 무엇 뜻인지 알고 싶다면,
14:22
우리가 해야 할 일은 텍스트에 있는 수천 건의 단어 banking의 인스턴스가
14:28
각각 나타나는 환경을 살펴볼 것입니다.
14:31
그리고 나서 부채 문제, 정부,
14:35
규제, 유럽, 통합된 말하기 등,
14:39
이 모든 것들이 나타나는 것들을 어떻게든 세어 볼 것입니다.
14:43
그래서 은행의  의미 표현을 컨텍스트의 바탕에서 하려고 할 것입니다.
14:49
어디서나 읽을 수 있는 가장 유명한 슬로건으로
14:55
영국 언어 학자였던 JR Firth가 말한 분배 유사성은 
15:01
회사 이름으로도 계속 쓰고 있어서 알고 있을 것입니다.
15:06
이것은 실제로 비트겐슈타인이 제안한 것과 똑같은 개념입니다.
15:11
그의 후기 저작물에서 쓰임 위주의 의미 이론을 제안했는데
15:17
다소 논쟁의 여지가 있고 이것이 의미론의 주요 흐름이 아니지만.
15:21
그는 단어의 의미를 생각하는 올바른 방법은
15:26
텍스트에서의 사용법을 이해하는 것이라고 하였습니다.
15:29
따라서 본질적으로 어떤 컨텍스트의 맥락을 예측하고 싶다면,
15:33
그 단어가 나타났을 때 그 의미를 이해하면 되는 것입니다.
15:38
이것이 우리가 하려고 하는 것입니다.
15:41
그래서 우리가 하고 싶은 것은 우리가 생각해 낸 단어 하나하나가 나타났을 때
15:46
그것은 dense vector가 될 것이고 그  벡터를 고르면 될 것입니다.
15:52
이 단어의 컨텍스트에 따라 단어를 예측하는 것은
15:58
어떻게 하면 될까요?
15:59
음, 그 단어들은 각각 양 옆에 단어가 있을 것입니다.
16:03
그 유사성을 측정하기 위해서
16:07
두 벡터 사이의 내적을 계산할 것입니다.
16:08
그리고 잘 예측할 수 있도록
16:11
조금씩 바꿔 보겠습니다.
16:14
모든 종류가 다소 재귀적(recursive)이거나 순환적(circular)이겠지만
16:17
이 영리한 알고리즘이고
16:21
특정 단어를 컨텍스트 안에서 예측할 수 있고 그 반대도 마찬가지로 가능할 것입니다.
16:25
그래서 저는 그것에 대해 조금 더 이야기 하도록 할 텐데,
16:30
조금만 더 강조해 보도록 하겠습니다.
16:34
이전 슬라이드에 등장한 용어로
16:39
우리는 두 개의 키워드를 보았습니다.
16:42
하나는 분산이었고
16:47
다른 하나는 분산 표현이었습니다.
16:51
여기에 단어의 의미를 나타내는 dense vectors가 있습니다.
16:55
사람들은 그 두 단어를 혼동하는 경향이 있는데,
16:59
이렇게 혼란스럽게 된 두 가지 이유는
17:02
먼저, 둘 다 분산으로 시작하기 때문에 비슷합니다.
17:08
두 번째로 혼란스러운 이유는 그것이 매우 강하게 동시 발생(co-occur)하기 때문입니다.
17:16
분산된 표현과 의미는 거의 항상
17:22
지금까지는 분산의 유사성을 이용하여 구축되었습니다.
17:26
하지만 저는 서로 다른 개념으로 묶기를 원했습니다. 맞나요?
17:31
따라서 분산적 유사성의 이론은 단어의 의미에 관한 이론입니다.
17:37
즉, 여러분이 의미의 사용을 봄으로써 단어의 의미를 설명하는 것으로
17:42
그 단어가 나타나는 컨텍스트를 이해하려고 하는 것입니다.
17:45
분산적 차이는 여기서 좀 전에 이야기한 것으로,
17:50
지시에 대해서 말했었는데, 그렇죠?
17:53
단어 의미에 대한 지시적 개념으로서의
17:57
안경의 의미는 주변에 있는 한 쌍의 안경 세트로
18:02
분산의 의미와 다르고
18:05
원 핫 워드 벡터와 대조적인 분포입니다.
18:10
원 핫 워드 벡터는
18:15
여러분이 한 곳에 저장하고 있는 로컬 표현입니다.
18:16
여기에 기호인 안경이 있다고 말하면
18:19
분산 표현에서는 여기 저장되어 있다고 하지만
18:23
큰 벡터 공간에서는 얼룩 같은 의미일 뿐입니다.
18:27
네, 이게 하나의 파트였고
18:32
이제 우리는 2 부로 향해 가보겠습니다. 그것은 Word2vec입니다.
18:38
네, 바로 설명하자면,
18:42
우리가 뉴럴 워드 임베딩에서 하는 것은
18:47
일종의 조리법 같은 것입니다.
18:49
우리는 다음 중 어느 것을 예측할 것인가에 대한 모델을 정의할 것입니다.
18:55
중심 단어와 컨텍스트에서 나타나는 단어는
19:00
여기 있는 것처럼, 분포상의 워딩으로.
19:03
일종의
19:06
컨텍스트에서 주어진 확률 측정이거나 예측일 것입니다.
19:10
그리고 나면 손실함수와 같은 것을 써서
19:14
잘 예측할 수 있도록 할 것입니다.
19:17
그렇게 하면 이상적으로는 단어와 주위의 단어를 완벽하게 예측할 수 있습니다.
19:21
마이너스 t는 단어 색인 t가 아니라 t 주변의 단어를 의미합니다.
19:27
우리가 t를 완벽하게 예측할 수 있다면 우리는 확률 1을 가질 것입니다.
19:31
손실(loss)이 없어야 하지만 일반적으로 그렇게 할 수는 없습니다.
19:34
그리고 우리가 4 분의 1 확률을 주어진다는 것은 3/4 의
19:37
손실 정도를 가진다는 것을 의미합니다.
19:38
이렇게 손실 함수를 얻을 수 있는데,
19:40
큰 코퍼스의 여러 위치에서 이것을 얻게 되고
19:44
결국, 우리의 목표는 단어의 표현을 이렇게 바꾸는 것입니다.
19:49
우리는 손실을 최소화 할 수 있습니다.
19:51
그리고 이 시점에서 기적이 발생합니다.
19:55
놀라운 일이지만, 우리가 할 수 있는 것은 진짜로 더 이상 없습니다.
20:01
이런 종류의 예측 목표를 설정하는 것보다.
20:05
모든 단어 벡터의 작업으로 만들어서
20:10
자신의 컨텍스트에 나타나는 단어를 예측할 수 있으며 그 반대의 경우도 가능합니다.
20:15
그저 아주 단순한 목표를 가지고 이것이 어떻게든
20:20
달성 될거야 라고 기도하면서 딥러닝의 마법에 의존하면
20:26
기적이 일어나고
20:28
놀랍도록 강력한 단어벡터의 결과로 
20:33
모든 일들에 유용한 단어의 의미 표현이 가능할 것입니다.
20:38
그럼 무슨 일이 일어났는지에 대해 좀 더 자세히 알고 싶을 것입니다.
20:43
오케이.
20:48
여기서 표현은  wt를 제외한 모든 단어를 의미합니다.
20:54
그럼 이  w  마이너스  t 는 무슨 의미였습니까?
20:59
저는 실제로 이 강의에서 다시 이 표기법을 사용하지는 않을 것입니다.
21:01
그러나 w 마이너스 t에서, 마이너스는 t를 제외한 모든 것을 의미하곤 합니다.
21:06
그래서 wt가 초점 단어이고, w 마이너스 t에는 컨텍스트 안에 있는 모든 단어가 포함됩니다.
21:14
자,  이 아이디어는 저차원 벡터 표현을 학습(Low dimensional vector representations)할 수 있는
21:19
뉴럴 네트워크에서 역사적인 아이디어로 여겨집니다.
21:23
그것은 확실히 1980 년대에도 존재했지만,
21:26
병렬 분산 처리 작업에 의해
21시 30 분
Rumelhart가 역전파 오차(back-propagating errors)로 표현 학습을 했고
21:34
선구자적으로 단어 표상을 증명하였습니다.
21:42
2003 년 Yoshua Bengio의 초기 논문과 뉴럴 확률론적 언어 모델이 있었지만
21:47
당시 많은 사람들이 실제로 이 논문에 많은 관심을 보이지는 않았고,
21:52
딥러닝 붐이 시작되기 이전에 것들입니다.
21:58
하지만 실제로 이 논문들을 통해서
22:03
단어와 단어의 분산 표현을 통해 얻을 수 있는 가치와
22:08
컨텍스트를 통해서 다른 단어를 예측할 수 있다는 것을 알게 되었습니다.
22:11
이 아이디어들이 튀어 나와서
22:17
되살아 난 것은
22:18
2008 년 Collobert와 Weston가 현대적 방향으로 말하기 시작하면서부터입니다.
22:23
글쎄요, 우리가 단지 좋은 단어 표현을 원한다면,
22:27
반드시 예측 가능한 확률론적 언어 모델을 만들 필요는 없을지도 모릅니다.
22:31
우리는 단어 표상을 배우는 방법이 필요합니다.
22:35
이제 제가 모델을 계속 볼 텐데,
22:40
지금은 word2vec 모델을 보겠습니다.
22:42
word2vec 모델의 강조하고 싶은 점은 아주 단순하고,
22:47
확장성이 뛰어나고 모델 훈련이 빨라서 십억 이상의
22:53
텍스트 단어에도 좋은 단어 표현을 산출한다는 점입니다.
23:00
네, word2vec입니다.
23:03
word2vec이 시도하는 기본 작업은 의미 이론을 사용하여
23:08
모든 단어와 컨텍스트 단어 사이를 예측한다는 것입니다.
23:12
이제 word2vec는 소프트웨어입니다. 제 말은
23:15
사실 word2vec 안에는 일종의 가족이 있는데
23:19
단어 벡터를 생성하기 위한 두 개의 알고리즘과
23:24
두 가지 효율적인 훈련 방법이 그것입니다.
23:28
그래서 이 수업을 통해서 제가 할 일은
23:32
알고리즘 중 하나인 skip-gram 방법이나
23:36
적당히 효율적인 훈련 알고리즘에 관한 것이 아니라
23:40
희망이 없는 비효율적인 훈련 알고리즘에 대해서 말하려고 합니다.
23:43
이것은 뭔가 작동하기 위한 방법의 개념적 근거의 일종으로
23:48
알맞게 효율적인 것은, 마지막에 언급할 것입니다.
23:52
그런 다음 여러분들이 실제로 확장 가능한 프로세스로 만들면
23:55
빨리 달려갈 수 있을 것입니다.
23:57
그리고 오늘은 과제를 나눠주는 날이기도 합니다.
24:02
여러분들이 과제 수행에 주요 부분 중
24:07
하나는 효율적인 훈련 알고리즘 중 하나를 실행하는 것이고
24:11
그것을 위해 효율적인 훈련 알고리즘 방법으로 작업해야 할 것입니다.
24:16
이것은 skip-gram 모델의 그림입니다.
24:19
이 skip-gram 모델의 아이디어는
24:24
각 추정 단계에서 한 단어를 중심 단어로 사용합니다.
24:30
그래서 여기 있습니다, 제 단어가 banking이라면 무엇을 해야 할까요?
24:36
여러분은 윈도우 사이즈로 컨텍스트 안에 단어를 시도하고 예측하려고 한다면
24:42
모델은 다음과 같은 확률 분포를 정의할 것입니다.
24:46
주어진 상황에서 이 중심 단어가 나타날 확률로 말입니다.
24:52
그리고 우리는 단어의 벡터 표현을 선택하면
24:57
우리는 그 확률 분포를 최대화하려고 시도할 수 있을 것입니다.
25:01
그리고 되돌아오면
25:04
하나의 확률 분포의 중요성을 알게 될 것입니다.
25:09
이 모델은
25:10
단어 하나는 왼쪽에, 단어 하나는 오른쪽에 있는 것과 같은
25:12
그런 확률 분포가 아닙니다.
25:16
컨텍스트 단어의 확률 분포는
25:20
우리가 output이라고 부르는 것의 하나입니다. 왜냐하면
25:24
중심 단어와 가까이에 있는 컨텍스트의 등장으로 output을 생성하기 때문입니다.
25:29
이해 되나요?
25:32
네? 오케이. 
25:37
우리가 하려고 하는 것은, 여기에 반지름 m이 있다면
25:42
그 다음에
25:47
중심 단어 앞의 m 위치와 중심 단어 뒤의 m 위치로 주변 단어를 예측할 것입니다.
25:52
그리고 여기저기서 많은 시간을 할애할 것입니다.
25:56
그리고 우리는 선택하고 싶은 워드벡터는
26:00
우리의 예측 확률을 극대화하는 것이어야 합니다.
26:05
그래서 손실 함수 또는 목적 함수는 실제로 여기 J 프라임입니다.
26:14
J 프라임에 대해 말하자면, 우리가 다루어야 할 크고
26:18
많은 양의 텍스트로, 위키피디아 전체나 뭐 그런 종류의 것이 될 것입니다.
26:23
컨텍스트 안에 크고 긴 순서를 가지는 단어들이 있습니다.
26:28
그리고 우리는 실제 실행되는 텍스트로 각 텍스트 안에서의 위치를 살펴보려고 합니다.
26:33
그러려면 텍스트의 각 위치에 윈도우가 있어야 될 것입니다.
26:37
크기 2m니까 앞에 m 단어와 뒤에 m 단어가 있겠네요.
26:41
그리고 우리에게 확률을 제공할 
26:47
컨텐스트에서 중심 단어로 나타나는 단어의 확률 분포가 있을 것입니다.
26:52
여기서 우리가 하고자 하는 것은 우리 모델의 매개 변수를 설정할 것입니다.
26:57
실제로 이 단어들의 확률은
27:00
이 컨텍스트에서 중심 단어가 나타날 가능성을 가능한 높게 하는 것입니다.
27:05
그래서 이 세타 모형의 매개 변수들이 여기에 있습니다.
27:10
이 슬라이드가 끝나면 여기에 세타는 더 이상 이야기하지 않더라도
27:13
여러분은 여기에 세타가 있다고 가정하시기 바랍니다.
27:17
이 세타는 무엇일까요?
27:17
세타란 무엇입니까?
27:20
그것은 단어의 벡터 표현이 될 것입니다.
27:24
이 모델에서는 각 단어의 벡터 표현에 대한 유일한 매개 변수입니다.
27:29
이 모델에는 다른 매개 변수가 전혀 없습니다.
27:34
얼핏 봐도 그렇게 보이죠?
27:34
개념적으로는 이것은 우리의 목적 함수입니다.
27:39
우리는 이 예측 확률을 극대화하고자 합니다.
27:45
실제로도 약간 조정될 뿐입니다.
27:47
먼저, 우리가 확률로 작업할 때 더 이상 하기 어려울 만큼
27:52
극대화되기를 원합니다. 우리는 실제로 이것을 로그 확률로 바꿀 것입니다.
27:57
그러면 모든 결과의 합계로 바뀝니다.
27:59
수학은 작업하기 훨씬 쉽게 해줍니다. 그래서 제가 여기서 다룬 것입니다.
28:09
(질문)좋은 지적입니다.
28:10
질문은, 잠깐만요. 속인 게 아니냐는 거죠? 윈도우 사이즈는
28:14
이 모델의 매개변수가 아닌가 하는 질문인데요.
28:16
옳습니다. 이 모델의 매개변수입니다.
28:18
제가 거기서 약간 느슨했던 것 같네요.
28:22
실제 모델에는 여러 가지 하이퍼 파라미터가 있으므로
28:26
제가 속임수를 쓴 거네요.
28:26
모델의 몇 가지 하이퍼 파라미터가 있습니다.
28:31
하나는 Windows 크기이며 우리는 몇 가지
28:35
다른 퍼지 팩터 같은 것은 나중에 강의에서 보겠습니다.
28:37
그리고 그 모든 것들은 조정할 수 있는 하이퍼 파라미터입니다.
28:42
하지만 당분간은 무시하고
28:43
그냥 상수라고 가정 해 봅시다.
28:46
그리고 그러한 것들이 조정되지 않는다면,
28:49
이 모델의 유일한 매개 변수는 단어의 팩터 표현으로
28:54
제가 말하고 싶은 것은
28:59
자체의 매개 변수가 있는 확률 분포는 없다는 것입니다.
29:02
좋은 지적이었고
29:03
높이 사겠습니다.
29:05
이제 우리는 로그 확률과 합계에 도달했습니다.
29:11
그리고 전체 코퍼스의 확률보다는,
29:18
각 포지션에서의 평균을 구할 수 있으므로 여기서 T분에 1을 얻습니다.
29:26
그리고 이것은 일종의 단어 단위의 정규화(normalization)라고 할 수 있습니다.
29:32
따라서 최대값에는 영향을 주지 않습니다.
29:35
그리고 머신러닝을 하는 사람들은
29:39
최대화보다는 최소화하는 것을 정말로 좋아합니다.
29:43
항상 최대화와 최소화 사이를 전환 할 수는 있습니다.
29:46
여러분이 플러스와 마이너스 기로에 있을 때 그 앞에 마이너스 사인을 넣어서.
29:51
음의 로그 가능도(negative log likelihood)를 얻을 수 있습니다.
29:55
모델에 따라 negative log likelihood를 적용할 것이며
29:59
그 결과 우리는
30:03
우리의 목적 함수를 공식적으로 최소화할 수 있을 것입니다.
30:08
따라서 목적 함수, 비용 함수, 손실 함수가 모두 동일하다면,
30:12
이 negative log likelihood  기준은 실제로
30:17
크로스 엔트로피 로스로 다음 주부터 사용할 것입니다.
30:21
지금은 깊이 다루지 않겠습니다.
30:23
여기서 트릭은 원핫 타겟이 있기 때문에,
30:26
이것으로 실제로 발생한 단어를 예측할 수 있다는 것입니다.
30:30
이 기준에 따라 남은 것은
30:34
크로스 엔트로피 로스로 실제 클래스의 음의 로그 가능도 규준 (negative log likelihood criterion)입니다.
30:39
글쎄, 우리가 이걸 어떻게 실제에 쓸 수 있을까요?
30:43
어떻게 이 단어 벡터를 사용하여
30:47
그 negative log likelihood를 최소화할 수 있겠습니까?
30:52
방법은
30:55
아마도 컨텍스트 워드의 분포와 주어진 중심 단어로,
31:00
워드 벡터를 구성할 수 있을 것입니다.
31:06
그리고 이것은 확률 분포가 어떻게 생겼는지를 나타내 줍니다.
31:09
우리가 앞으로 사용할 용어를 명확하게 하기 위해서
31:14
이제부터는
31:15
c와 o는 워드 타입으로  단어 유형을 나타내는 표지로 합니다.
31:22
그래서 여기 위에, t와 t 플러스 j는 저의 텍스트 안에 위치해 있습니다.
31:29
그것은 워드의 일종이며, 763 개의 워드와 766 개의 텍스트가 있습니다.
31:34
하지만 여기 o와 c는 워드 타입으로 제 단어 워드에 속해 있습니다.
31:39
그래서 저는 73 개의 단어와 47 개의 단어를 제 어휘로 사용하려고 합니다.
31:45
여기서 각각의 워드 타입은 관련 벡터를 갖게 될 것입니다.
31:53
u o는 색인 o인 컨텍스트 워드와 연관된 벡터이며,
31:59
v, c는 중심 단어와 연관된 벡터입니다.
32:04
그리고 이 확률 분포를 찾는 방법으로 우리가 사용할 것은
32:10
Softmax 양식이라고 불리는 것입니다, 여기서 우리는
32:16
두 단어의 벡터를 구한 다음 Softmax 형식으로 변환합니다.
32:22
최대로 천천히 살펴보려고 하는데, 괜찮나요?
32:25
우리는 두 개의 단어 벡터를 가지고 있습니다. 그리고 그 내적을 구하기 위해서
32:30
해당 용어를 취해서
32:33
곱하고 나서 일종의 모두 합치는 방식으로 정리할 것입니다.
32:37
내적은  일종의 느슨하게 유사성을 측정하는 것으로,
32:42
벡터의 항목들이 점점 서로 유사해지면
32:46
그 수는 더 커질 것이다.
32:48
그래서 내적을 통해 유사성을 측정하는 것입니다.
32:52
결국, 우리는 단어들 사이에 내적을 만들어 냈습니다.
32:56
이것을 Softmax 양식에 넣을 것입니다.
33:00
그러면 이 Softmax 양식은 일반적으로
33:03
숫자를 확률 분포로 바꿔줄 것입니다.
33:07
그래서 우리가 내적을 계산할 때, 그것은 숫자로, 실수입니다.
33:12
마이너스 17 또는 32일 수도 있습니다.
33:14
우리는 그것을 확률 분포로 직접 변환 할 수는 없습니다.
33:19
쉽게 할 수 있는 일은 지수로 만드는 것입니다.
33:23
왜냐하면 만약 여러분이 양수로 지수 계산하면
33:27
모두 양수로 될 것이기 때문에
33:29
이렇게 하는 것은 확률 분포를 하는 좋은 기저가 될 것입니다.
33:34
그리고 양수 위치에 수 많은 숫자가 있는 경우
33:39
그 숫자의 크기에 비례하는 확률 분포로 바꾸고 싶을 것입니다.
33:43
정말 쉬운 방법이 있습니다.
33:47
모든 수를 합쳐서 합계로 나누면 됩니다.
33:52
그러면 곧바로 확률 분포를 알 수 있게 됩니다.
33:55
이것은 확률을 부여하기 위해 정규화된 것입니다.
34:00
그래서 이것들을 합쳐 놓으면 우리가 사용하는
34:05
Softmax 형식은 추정 확률을 제공해 줍니다.
34:10
그렇게 해서 추정 확률을 알 수 있게 되는 것입니다.
34:13
그리고 이것은 전적으로 워드 벡터 표현이라는 용어 위에 만들어 졌었습니다.
34:18
여기까지 괜찮나요?
34:19
네.
34:24
매우 좋은 질문입니다.
34:29
잠시지만 질문이 있으므로 이야기해 보겠습니다.
34:33
네, 한 단어에 벡터 표현이 하나만 있어야 한다고 생각할 수 있습니다.
34:41
그리고 정말로 원한다면 그렇게 할 수 있었지만, 지금 밝혀진 것으로는
34:47
실제로 수학을 쉽게 하려면 실제로 각각의 단어를 합성할 때
34:53
하나의 벡터 표현을 되는 두 개의 벡터 표현을 할 수 있습니다.
34:57
그리고 컨텍스트 단어 일 때 또 다른 벡터 표현을 합니다.
35:02
그러면 공식적으로 여기서는
35:04
v는 중심 워드 벡터가 되고 u는 컨텍스트 워드 벡터가 됩니다.
35:10
그리고 이것이 수학을 훨씬 쉽게 만들어 줄뿐만 아니라,
35:13
두 표현은 분리되어 있기 때문에
35:16
최적화를 할 때 서로를 묶을 수도 있습니다.
35:19
실제로 경험적으로는 조금 더 잘 작동합니다.
35:24
여러분의 삶에 이것이 더 쉽고 더 좋다면 누가 선택하지 않을까요?
35:28
그리고 네 , 각 단어에 두 개의 벡터가 있습니다.
35:32
다른 질문?
35:52
네, 문제는 잠깐만요,
35:55
방금 이것이 모든 것을 긍정적으로 만드는 방법이라고 말했습니다.
35:59
사실 여러분은 또한 동시에 스케일을 상당히 망쳤습니다.
36:03
사실입니다, 맞죠?
36:05
이것을 Softmax 함수라고 부르는 이유는
36:09
최대 함수에 가깝기 때문입니다. 지수로 만드니까요.
36:14
그래서 점점 커지고 정말로 주도적인 것이 됩니다.
36:18
그리고 실제로 max 함수의 방향으로 불어납니다.
36:23
그러나 완전히는 아닙니다.
36:24
여전히 ​​일종의 부드러운 성향이 있습니다.
36:27
그래서 여러분이 나쁜 방법이라고 생각할 수도 있지만
36:30
이와 같이 일을 하는 것은 많은 수학의 기초가 되는 가장 기본적인 표준입니다.
36:34
매우 일반적인 로지스틱 회귀를 포함하여,
36:37
이렇게 하는 것을 또 다른 수업에서 보게 될 것입니다.
36:40
알아 두면 좋을 것이지만
36:41
사람들은 분명히 다른 방법으로도 많은 노력을 기울였습니다.
36:44
여러분이 재미있다고 생각할 수도 있는데
36:46
지금 이걸 다루지는 않겠습니다.
36:48
네?
37:00
(질문) 질문은 컨텍스트 단어를 다룰 때
37:04
제가 단지 위치나 항등원(identity)에 관심을 기울였었나요?
37:08
위치는 모델과는 아무 관련이 없습니다.
37:11
윈도우에서 워드의 항등원은 어디에 있나요?
37:15
단 하나의 확률 분포가 있고
37:19
컨텍스트 워드의 한 표현이 있었습니다.
37:21
이제 알다시피, 그것은 반드시 좋은 아이디어는 아닙니다.
37:25
위치와 거리에 절대적으로 주의를 기울이는 다른 모델이 있습니다.
37:30
그리고 어떤 목적을 위해서는, 특별히 의미론이 아닌 더 통사론적인
37:34
것도 실제로 많은 도움이 됩니다.
37:38
그러나 여러분이 단어 의미에 좀 더 관심이 있다면,
37:43
위치에는 주의를 기울이지 않는 것이
37:45
여러분을 해치기보다는 도움이 될 수 있을 것입니다.
37:50
네.
38:05
그래, 문제는 잠깐만요, 여기에 독특한 해결책이 있을까요?
38:10
다른 회전이 똑같이 좋을 결과를 보일 수 있습니까?
38:15
대답은 '네'입니다.
38:19
하지만 이것에 대해 토론하는 것은 연기합시다.
38:24
왜냐하면 신경망 최적화에 대해서는 흥미진진한 새로운 작업이 많습니다.
38:29
그리고 하나의 제목으로 묶는다면 모두 희소식입니다, 사람들은 많은 시간을 보내면서
38:34
최소화하하는 일은 큰 문제라고 말하였지만 아닌 것으로 판명되었습니다.
38:39
모두 작동합니다.
38:40
그러나 우리는 그것에 대해 좀 더 자세히 이야기하는 것은 다음에 하는 것이 좋다고 생각합니다.
38:45
네. 그래서
38:50
네, 이것은 skip gram 모델이 어떻게 보이는지에 대한 제 그림입니다.
38:56
다소 혼란스럽고 읽기가 어렵고
38:59
왼쪽에서 오른쪽으로 그렸습니다.
39:01
맞아요, 우리에게 원핫 벡터라는 중심 워드가 있습니다.
39:06
그 다음 우리에게는 중심 워드 표현의 행렬이 있습니다.
39:13
그래서 이 행렬에 그 벡터 곱셈을 하기 위해서
39:22
우리는 이 행렬의 열을 선택합니다.
39:26
그것이 중심 워드의 표현이 될 것이니까요.
39:31
그런 다음 두 번째 행렬에서 할 것은
39:35
컨텍스트 워드의 표현을 저장하는 것입니다.
39:39
그리고 컨텍스트의 각 위치에 대해,
39:42
충분히 혼란스러울  것이기 때문에 여기서는 세 개만 보여 드리겠습니다.
39:46
우리는 이 행렬에 벡터를 곱하려고 하는데
39:51
그것은 컨텍스트 워드의 표현입니다.
39:55
여기서 우리는 일종의 내적을 골라 낼 것입니다.
40:00
중심 워드와 각 컨텍스트 워드에서요
40:04
이것은 각 위치에 대해 동일한 행렬입니다. 맞습니까?
40:07
우리는 하나의 컨텍스트 워드 행렬만 가지고 있습니다.
40:10
그리고 이 내적은
40:12
소프트맥스의 확률 분포로 바뀔 것입니다.
40:17
그래서 생성 모델로서 우리 모델은 다음과 같은 가능성을 예측합니다.
40:22
각 워드는 특정 워드가 중심 워드인 상황에서 나타나고
40:29
생성적으로 사용한다면,
40:32
음, 여러분이 사용해야 할 워드는 여기에 있을 것입니다.
40:35
그러나 컨텍스트 워드가 무엇인지와 같은 실제적인 배경에서는,
40:41
여기서 실제적인 단어가 나타나게 됩니다.
40:46
그리고 여러분은 그 단어에 0.1의 확률 추정치를 줄 것입니다.
40:50
이것이 기초로 여러분이 예측에서 확실한 작업을 하지 않았다면,
40:54
약간의 손실이 있을 것입니다, 그렇죠?
40:57
이것이 우리 모델에 대한 그림입니다.
40:59
좋아요, 그래서 우리가 하고 싶은 것은 지금부터 배울 것입니다.
41:04
매개 변수, 워드 벡터의 방법으로 우리가
41:09
가능한 한 예상할 수 있는 좋은 일을 할 것입니다.
41:16
우리가 이러한 일을 할 때 표준이 되는 것으로 우리가
41:21
모델에서 모든 매개 변수를 가져 와서 큰 벡터 세타에 넣을 것입니다.
41:26
그런 다음 매개 변수를 변경하기 위한 최적화를 수행해서
41:31
우리 모델의 객관적인 기능을 극대화할 것입니다.
41:35
각각의 단어 매개 변수에는
41:38
작은 d 차원의 벡터가 있을 것입니다.
41:43
때때로 중심 워드로 쓰일 수도 있고 컨텍스트 워드로 쓰일 수도 있습니다.
41:47
그래서 우리에게 일정한 크기의 어휘가 있다면
41:50
컨텍스트 워드로 aardvark 의 벡터를 갖게 될 것입니다.
41:54
(여기) 컨텍스트 워드로서의 a의 벡터가 있고
41:57
우리가 aardvark의 벡터를 중심 워드로 사용하려고 할 때도 있고
42:00
(여기는) 중심 워드로 a의 벡터가 있습니다.
42:02
이 벡터의 총 길이는 2dV가 될 것입니다.
42:06
표시되는 내용을 모두 포함하는 크고 긴 벡터네요.
42:10
이것은 전에 행렬에서 본 적이 있을 것입니다.
42:12
이제 우리가 최적화에 대해 말한 차례입니다.
42:15
좀 쉬고 나서
42:19
구체적으로 어떻게 최적화하는지 알아보겠습니다.
42:23
그러나 휴식 시간 전에
42:25
특별 게스트 Danqi Chen과 함께  막간 시간을 보내겠습니다.
42:30
>> 안녕하세요, 여러분.
42:32
저는 Danqi Chen입니다. 저는 이 수업의 수석 TA입니다.
42:36
오늘 저는 첫 번째 연구의 하이라이트 세션을 시작하겠습니다.
42:39
Princeton에 논문을 소개하겠습니다.
42:42
제목은 문장 임배딩을 위한  A Simple but Tough-to-beat Baseline입니다.
42:46
오늘 우리는 단어 벡터 표현을 배우니까
42:50
이 벡터들이 단어 의미를 부호화 할 수 있기를 바랄 것입니다.
42:53
이것은 자연어 처리에 핵심 질문이면서 이 수업 역시,
42:58
의미를 인코딩하는 벡터 표현을 어떻게 할까 입니다.
43:02
자연어 처리는 재미있습니다.
43:08
라는 문장 표현이 있다면
43:12
두 벡터의 내적으로 문장의 유사성을 계산할 수 있을 것입니다.
43:18
또 예문이 멕시코는 시민의 안전을 보장하고자 한다와,
43:22
멕시코는 더 많은 폭력을 피하려고 한다.
43:25
라면 우리는 벡터 표현을 사용하여 이 두
43:29
문장은 꽤 비슷하다는 것을 예측할 수 있을 것입니다.
43:32
또한 이 문장 표현을 사용하여 
43:35
일부 문장 분류 작업을 수행하는 기능으로도 사용할 수 있을 것입니다.
43:39
예를 들어 감성 분석으로
43:41
자연어 처리는 재미있습니다.
43:45
를 벡터 표현 위에서 분류하여
43:49
긍정적인 감성인지 예측할 수 있고
43:51
이것이 맞기를 희망할 것입니다.
43:54
따라서 다양한 측정 방법으로
43:58
워드 벡터 표현을 문장 벡터 표현으로 구성할 수 있습니다.
44:02
가장 간단한 방법은 단어 주머니(bag-of-words)를 사용하는 것입니다.
44:06
그래서 단어 주머니를
44:09
자연어 처리의 벡터 표현처럼 하는 것입니다.
44:11
3 개의 단일 단어 벡터 표현의 평균으로
44:15
자연어를 처리하고 다룰 수 있습니다.
44:18
이번 분기의 후반에 우리는 복잡한 모델을 배울 텐데, recurrent
44:24
neural nets나 recursing neural nets 및 convolutional neural nets등이 포함될 것입니다.
44:29
그러나 오늘 프린스턴의 이 논문에서 저는
44:34
아주 간단한 비감독 방법을 소개합니다.
44:39
본질적으로는 단지 가중치가 있는 단어 주머니(bag-of-words) 문장
44:43
표현에 더 하는 것은 특별한 방향을 제거하는 뿐입니다.
44:47
설명을
44:50
두 단계로 나눠서 하면.
44:52
첫 번째 단계는 벡터 표현의 평균을 계산하는 것과 같습니다.
44:57
이렇게 하면서, 각 단어를 별도의 가중치로 계산합니다.
45:03
이제 여기서 a는 상수입니다.
45:05
그리고 p (w)는 이 단어의 빈도를 의미합니다.
45:09
따라서 기본적으로는
45:12
자주 나타나는 단어에는 가중 평균한 것이.
45:15
1 단계로 매우 간단합니다.
45:19
2 단계에서, 우리는 이 모든 문장 벡터 표현을 계산 한 후에
45:23
첫 번째 주성분을 계산하고
45:28
이 첫 번째 주성분에 대한 투영(projection)을 뺍니다.
45:35
CS 229를 들었거나
45:40
PCA를 배웠다면 잘 알고 있을 것입니다.
45:42
이것이 전부입니다.
45:43
이러한 접근 방식입니다.
45:46
그러니까 이 논문에서 하고 싶었던 것은
45:50
확률론적으로 해석하는 것입니다. 
45:55
그래서 기본적인 생각은 문장 표현이 주어진다면, 확률적
46:00
제한이나 단일 단어의 경가능도, 단어의 빈도와 관련이 있습니다.
46:05
또한 단어가 이 문장 표현과 얼마나 가까운지와 관련이 있습니다.
46:12
또한 C0라는 용어가 있는데 공통 담화 벡터(common discourse vector)를 의미하는 것입니다.
46:17
이는 대개 일부 구문과 관련이 있습니다.
46:21
마지막으로, 결과는
46:24
먼저 문장의 유사성에 대해 context parents를 취합니다.
46:29
그리고 이 간단한 접근법이 단어 벡터 평균이나
46:34
TF, IDF 등급 및
46:37
다른 정교한 모델의 모든 성능보다 뛰어나다는 것을 보여줍니다.
46:43
또한 문장 분류와 같은 감독 학습이나,
46:47
함의나 감성 분석 과제에서도 꽤 잘하고 있습니다.
46:52
이상입니다. 감사합니다.
46:54
>> 감사합니다.
46:55
>> [웃음] >> [박수]
47:00
>> 오케이.
47:08
돌아가 봅시다.
47:17
좋아요. 이제 우리 모델을 통해 일종의 실제적 작업을 하고 싶습니다.
47:23
이렇게 생긴 걸 갖고 있습니다. 맞죠?
47:26
음의 로그 가능도(negative log likelihood)를 최소화하고자 하는 목적 함수를 사용했습니다.
47:33
그리고 이것은 위의 확률 분포의 한 형태로
47:38
중심 워드 벡터와 컨텍스트 워드 벡터를 모두 포함하는 워드 벡터 종류입니다.
47:44
그리고 우리 아이디어는 벡터의 매개변수를 변경하면,
47:50
negative log likelihood 항목을 최소화하고 예측 확률은 최대화될 것입니다.
47:56
그래서 하고 싶은 것은,
48:00
매개변수를 변경하는 방법입니다.
48:11
경사도(Gradient), 네, gradient를 사용할 것입니다.
48:13
그래서 이 시점에서 우리가 해야 할 일은
48:18
미적분을 통해 우리는 숫자를 어떻게 바꿀 수 있는지 보는 것이 좋겠습니다.
48:24
정확하게 말하면, 우리가 하고 싶은 것은, 음,
48:29
로그 확률을 계산하는 용어가 있죠.
48:36
워드 t(가 주어졌을 때) 워드 t 더하기 j의 확률 로그를 얻을 수 있습니다.
48:44
음, 그 형태는 무엇입니까?
48:46
음, 여기 있습니다.
48:47
 v의 로그가 있... (이렇게 하면)아마 몇 줄 덜 써도 되겠네요.
48:54
로그를 써서
49시
이것을 바꾸고 싶은 것인데
49:06
우리는 최대화, 미안합니다, 이 오브젝트를 최소화하기 위해서
49:11
이것을 일종의 중심 벡터로 보겠습니다.
49:15
그런 다음에 편미분을 해 나갈 것입니다.
49:21
이 중심 벡터에 관해서,
49:26
어떻게 하면
49:34
목적 함수를 최소화하는 벡터로 바꿀 수 있을까요?
49:38
좋아요, 이걸 해결해야겠네요.
49:41
이것을 좀 더 간단하게 하기 위해 우선 뭘 해야 할까요?
49:47
빼기, 네.
49:49
이것을 나눗셈의 로그이고 우리는 이것을 뺄셈의 로그로 바꿀 수 있습니다.
49:55
그런 다음 편미분으로 따로 처리 할 수 ​​있습니다.
50:00
그래서 vc 미분과
50:05
로그의 exponential 
50:11
u0 ^ T vc 에서 
50:16
w의 합에 로그를 뺍니다.
50:21
1 에서 V 까지에서 exponential u w^T vc 
50:27 
그리고 그 시점에서 우리는 두 부분으로 분리할 수 ​​있게 되었죠.
50:32
왜냐하면 더하기나 빼기는 별도로 할 수 있기 때문입니다.
50:37
그래서 우리는 이 1번 부분을 할 수도 있고
50:42
2번 부분의 편미분을 구할 수도 있습니다.
50:47
1번 부분이 보기 쉬우니까 여기서 시작합시다.
50:52
그래서 이것을 더 간단하게 하기 위해 무엇을 해야할까요?
50:57
쉬운 질문.
51:01
다른 것들을 취소하고 로그와 x가 서로 반대로 바뀌면 그냥 사라질 수 있습니다.
51:08
그래서, 1에 대해,
51:13
u0 ^ Tv Vc에 대한 편미분이 될 것입니다.
51:18
네, 이제 좀 더 단순 해 보이네요.
51:25
vc에 관해서
51:30
편미분한다는 것은 무엇입니까?
51:35
u0, 그래서 여기에 u0가 나옵니다.
51:40
그러니까, 제 말은, 사실, 이 정도가 우리가 해야 할 미적분의 수준이라는 것입니다.
51:47
그리고 오늘 나온 과제 중 하나에 잘 적용할 수 있어야 합니다.
51:52
생명을 위협할 정도는 아니죠. 여러분이 전에 이것을 해 보았기를 바랍니다.
51:58
아무튼 우리는 여기에 벡터와 함께 미적분을 사용하고 있습니다.
52:04
그래서 여기 있는 vc는 하나의 숫자가 아니라 전체 벡터입니다.
52:09
그래서, 그것은 일종의 수학 51, CME 100 같은 종류의 내용입니다.
52:16
자, 원한다면, 모든 것을 분리 할 수도 ​​있습니다.
52:21
그리고 편미분으로 해결할 수 있을 것은
52:26
어떤 인덱스, k인 Vc에 관한 것입니다.
52:30
그리고,
52:33
이것도 할 수 있습니다.
52:38
l = 1 의 합이 되는 d와
52:44
(u0) l (Vc) l.
52:49
그리고 이것을 하나의 인덱스에 대해서 적용하고 
52:55
그런 다음 이 모든 용어를 k가 l 인 것과는 별개로 진행하면 것입니다.
53:01
그러면 이것의 (uo) k 에서 끝날 것입니다.
53:09
그리고 일이 혼란스럽고 복잡해 질 때, 실제로 여러분의 두뇌가 저처럼 작다면,
53:14
실제로 내려가서 정렬하는 것이 유용 할 수도 있습니다.
53:19
실수와 실제로 모든 지수가 있는 것으로 작업하는 것입니다.
53:24
여러분은 절대적으로 그렇게 할 수 있고 값도 똑같이 나옵니다.
53:28
하지만  오랫동안 편리한 것으로 여겨진 것은
53:32
이 벡터 레벨에 머물면서 벡터 함수를 푸는 것입니다.
53:37
이제는 쉬운 부분이었고
53:41
오른쪽으로 다시 갑시다, 
53:44
그럼, 더 까다로운 부분인 2 번으로 넘어갑니다.
53:52
자, 이제 우리가 마이너스 사인을 잠시 무시하면
53:58
그러니까,
54:02
우리는 나중에 이것을 뺄 것이고,
54:07
편미분을 할 수 있을 것입니다.
54:14
합의 로그 vc와 관련하여
54:19
w =1에서 v까지 exponential u w ^ T vc를 얻었습니다.
54:26
그럼,이 반으로 어떻게 할 수 있을까요?
54:39
그래요, 그렇게 하기 전에 뭘 해야 할까요?
54:44
체인 규칙, 좋아요, 우리가 사용법을 알아야하는 핵심 도구죠
54:49
우리는 여기저기서 체인 규칙을 사용할 것입니다, 맞습니까?
54:55
그래서, 신경망에서 역전파(backpropagation)에 대해 많이들 이야기합니다만
55:00
역전파는 일부의 수량을 효율적인 저장하는
55:07
체인 규칙에 불과하다는 것이 밝혀졌습니다.
55:12
그렇지만 같은 양을 반복해서 계산하지 않도록 주의하세요.
55:16
체인 규칙은 memorization과 같은 종류이고,
55:20
이것이 역전파 알고리즘입니다.
55:22
이제 핵심 도구는 체인 규칙입니다. 체인 규칙은 무엇입니까?
55:30
말하자면,
55:34
전반적으로 우리가 해야 할 것은
55:38
f (g(u)) 의 기능입니다.
55:45
그리고 이 내부에 z가 있습니다.
55:49
우리가 해야 할 일은
55:54
내부의 값을 외부의 도함수로 가져오는 것입니다.
56:01
우리가 앞으로 할 일은 도함수를 내부로 함께 가져가는 것입니다.
56:06
여기, 외부 부분은 여기에 F.
56:11
그리고 여기 우리 내부 부분에 Z가 있습니다.
56:14
그래서 바깥 부분은 로그 함수인 F가 있습니다.
56:19
그래서 로그 함수의 미분인 X 함수입니다.
56:23
그럼 우리는
56:29
이것이 w의 합 1입니다.
56:34
1에서 V까지exponential uw ^ Tv와 같습니다.
56:39
그리고 나서 곱을 하면 무엇을 얻을 수 있을까요.
56:54
그래서 vc와 관련하여, 편미분을 하면
57:06
내부 부분에서
57:10
다음과 같이 구할 수 있습니다.
57:17
그 합계는 조금 더 까다롭습니다.
57:23
여기서는 인덱스에 주의해야 합니다.
57:25
우리가 여기에 W가 있다면 혼란스러울 테니까, 여기서 W를 재사용하겠습니다.
57:32
우리는 그것을 다른 무엇으로 바꿔야겠죠.
57:34
여기 X가 1에서 V인 
57:37
exponential UX의 
57:45
transpose VC 입니다.
57:49
이제 조금 진전되었습니다.
57:52
여기를 조금만 더 진전시켜봅시다.
57:56
다음에 할 일은 무엇입니까?
58:04
미분!
58:06
몇 가지만 추가하면 됩니다.
58:09
우리는 미분할 때 각 부분을 별도로 처리하는 동일한 트릭을 수행 할 수 있습니다.
58:15
따라서 X는 1에서 큰 V까지의 편미분으로
58:20
VC는 exponential (ux ^ T vc)를 구할 수 있습니다. 
58:24
좋아요, 계속하죠. 다음에 할 수 있는 일은 무엇입니까?
58:33
다시 체인 규칙이죠. 맞아요.
58:36
이것은 또한 우리 F의 형태입니다. 여기에 우리의 F가 있습니다.
58:40
내부 값 V는 일종의 함수입니다.
58:45
네, 체인 규칙을 두 번 적용 할 수 있습니다.
58:50
그래서 X의 미분이 필요합니다.
58:55
X의 미분은 무엇입니까?
58:57
X입니다, 네 여기 이 부분이 있어서.
59:02
X의 합은 편미분 1에서 V까지에서
59:06
잠깐만요. 아니네요.
59:08
여기가 아니라 안쪽으로 움직이면
59:10
아직도 exponential UX T VC의 값
59:18
편미분으로
59:24
UXT VC의 VC와 관련된 미분입니다.
59:30
우리는 조금 더 진전을 이루었습니다.
59:33
이제 이것이 무엇인지 알아내야 합니다.
59:36
그럼 이게 뭐죠?
59:40
맞아요, 그래서 여기에 다시 돌아오는 것과 같습니다.
59:43
이 시점에서 UX로부터 나올 것이고,
59:49
그리고 여기는 여전히
59:54
X는 1에서 V까지인 UX T VC인 X로 있습니다. 
60:01
이 시점에서 이것을 저것과 합쳐서 쓰고 싶습니다.
60:08
왜냐하면 저기서 쓰는 것을 멈추었었는데
60:11
여기에 쓰면
60:15
W는 1에서 V까지의 합은
60:20
exponential (UW transpose VC)이 될 것이기 때문입니다.
60:26
그럼 이걸 다 합쳐서 더 예쁘게 쓸 수 있을까요?
60:50
이걸 움직여서 합 안쪽에 넣을 수 있습니다.
60:54
왜냐하면 이 숫자들은 일종의 이 분포의 승수(multiplier)이기 때문입니다.
61:00
특히 제가
61:05
재미있다고 느끼는 부분은.
61:10
이 형식이 비슷한 형식을 재구성한 것처럼 보인다는 것입니다.
61:15
죄송합니다. 이 부분은 제쳐두고 합시다.
61:17
이건 시작 부분의 Softmax 양식과 매우 비슷합니다.
61:23
그래서 저는 이렇게 말할 수 있을 것 같습니다.
61:28
이것은 X는 1에서 V까지의 합은
61:33
exponential UX의 transpose VC
61:39
W 1에서 V까지의 합이다.
61:44
이것이  X의 다른 변수로서 X와 W인 것이 중요합니다.
61:50
UW의 transpose VC, UX로 바꿉니다.
61:59
그리고 그 시점에서 재미있는 것은
62:03
이것은 제가 시작한 형태와 정확히 일치한다는 것입니다.
62:09
softmax 확률 분포.
62:13
그래서 우리가 해야 할 일은
62:19
이 부분이
62:26
1에서 V까지의 X에 대한 합 
62:32
의 확률[안들림] 
62:38
잠시만요.
62:39
확률 O 의
62:44
(C가 주어졌을 때 X) UX의 확률이
62:50
분모입니다.
62:54
그런데 여기 아직도 분자가 남아 있네요.
62:56
분자는 U0 였습니다.
63:00
여기 남아 있는 최종 형태는 저것을 U0에서 빼는 것입니다.
63:07
만약에 여기를 본다면 이것은 일종의 우리가
63:12
항상 softmax 스타일 공식에서 얻던 형태입니다.
63:17
이것이 우리가 관찰 한 것입니다.
63:19
여기서 실제 출력 컨텍스트 워드가 나옵니다.
63:25
그리고 이것이 기대하는 형태입니다.
63:28
우리가 여기서 할 일은.
63:31
기대값을 계산하는 것입니다.
63:35
모든 가능한 단어가 컨텍스트에 나타날 확률로,
63:39
그 확률에 따라 UX의 부분을 차지하게 됩니다.
63:44
이것이 기대 벡터입니다.
63:49
가능한 모든 컨텍스트 벡터에 대한 평균으로
63:52
발생 확률에 가능도에 의해 가중치가 부여됩니다.
63:56
그것은 미분의 한 형태입니다.
63:59
우리가 하고자 하는 것은 우리 모델의 매개 변수를 변경하는 것입니다.
64:05
그런 방법으로 똑같은 모델이 된 것은
64:10
우리가 최소화 할 수 있는 최대값과 최소값을 찾아야 하기 때문입니다.
64:15
[안들림] 네. 그렇게 하면 그 모델에서 도함수를 얻을 수 있습니다.
64:21
이해되나요?
64:25
네 질문이겠죠?
64:27
어쨌든, 그래서
64:28
이와 같은 일을 정확하게 하는 것은 여러분이 과제 1을 위한 것이었습니다.
64:34
질문을 받기 전에  한 가지만 언급하겠습니다.
64:37
그래서 이 경우에는 VC에 대해서만 이 작업을 수행했습니다.
64:41
중심 벡터는
64:46
모델의 모든 매개 변수에 이 작업을 수행합니다.
64:49
이 모델에서 유일하게 다른 매개 변수는 컨텍스트 벡터입니다.
64:53
거기서도 똑같이 해야 합니다.
64:55
방정식의 형태를 보면 매우 비슷하기 때문입니다.
64:59
둘 사이에 비슷한 대칭이 있습니다.
65:01
해야 하지만 지금은 하지 않고
65:05
여러분에게 맡기겠습니다.
65:07
질문 있나요?
65:09
네. >> [안들림]
65:24
>> 여기서부터 여기까지.
65:25
오케이
65:26
그래서.
65:28
네, 이것이 합이죠? 맞습니까
65:32
그리고 이것은 최종적으로 얻은 숫자일 뿐입니다.
65:36
그래서 이 합의 모든 용어를 그 수로 나눌 수 있습니다.
65:41
그게 제가 한 일입니다.
65:43
이제는 이 숫자로 나누어진 모든 용어에서의 합을 얻을 수 있습니다.
65:48
그리고 나서, 잠깐만요, 여기 있는 조각의 형태는
65:54
정확히 softmax의 확률 분포로,
65:58
c가 주어졌을 때 x의 확률입니다.
66:02
그런 다음에 저는 c가 주어졌을 때 x의 확률로 다시 써 넣을 것입니다. 
66:06
의미인 곳에서, 두 번의 기능을 다한 것입니다.
66:10
의미의 일종으로, 우리는 c가 주어졌을 때 x의 확률로
66:14
이 확률 형태를 사용하면 될 것입니다.
66:16
>> [안들림]
66:26
>> 네,
66:27
중심 단어 c의 컨텍스트 워드로 x가 나타날 확률.
66:32
>> [안들림] >> 글쎄요,
66:36
우리는 방금 고정 된 윈도우 크기 M을 가정했습니다.
66:39
어쩌면 창 크기가 5 일 수도 있고 그래서 10개의 단어도 고려할 수 있습니다.
66:44
왼쪽 5 개, 오른쪽 5 개의 단어
66:48
그것은 hypergrameter이고, 여기에는 없습니다.
66:52
우리는 그 문제를 다루지 않았습니다, 우리는 하나님께서 우리를 위해 그것을 고정했다고 가정합니다.
66:59
그 문제는 위치로 해결할 수 있습니다.
67:02
모든 위치에서, 모두 동등하게 취급됩니다.
67:09
어떤 위치에서의 워드 x의 확률인데
67:14
이 윈도우 내에서
67:19
중심 워드 C가 주어진 위치에서 발생합니다.
67:26
네?
67:27
>> [안들림]
67:40
>> 좋아요, 질문은,
67:43
왜 우리는 확률 측정의 기저에
67:46
내적을 사용하느냐 하는 것입니다.
67:50
그 대답으로는 필연적인 이유가 없다고 생각합니다.
67:58
분명히 다른 것들이 있습니다.
68:02
또 할 수 있는 것이 있을 것입니다.
68:07
다른 한편으로는, 저 생각에는
68:13
벡터 대수학 (Vector Algebra)은 가장 분명하고 간단한 일을 하는데.
68:20
유사성과 관련성을 측정합니다.
68:28
나는 그것이 일종의 느슨하게 말해서 그것은 벡터들 사이의 유사성의 척도라는 것을 의미합니다.
68:32
누군가 저에게, “잠깐만요.
68:37
벡터의 크기를 제어하지 않으면,
68:41
그 숫자를 원하는 만큼 크게 만들 수 있어요” 라고 할 수 있습니다. 사실입니다.
68:44
벡터 사이의 유사성에 대한 공통 척도는 코사인 척도입니다.
68:50
분자에서 뭔가 하면 됩니다.
68:53
내적을 취한 다음 벡터의 길이로 나눕니다.
68:58
따라서 스케일과 분산이 있으며
68:59
벡터를 더 크게 만들어서 속일 수는 없습니다.
69:02
그리고 더 큰 것은 유사성 측면에서 더 나은 척도입니다.
69:08
하지만 그렇게 하기 위해서는 더 많은 수학을 해야 합니다.
69:12
실제로 그렇게 필요가 있어 보이지는 않습니다. 왜냐하면 여러분은
69:15
모든 단어를 다른 모든 단어와 비교하여 예측할 것이기 때문입니다.
69:18
한 가지 벡터를 아주 크게 만들어서
69:22
워드 k가 큰 확률을 만들 수 있습니다.
69:26
그럼 결과는 모든 다른 단어의 확률이 같아지려면
69:30
똑같이 커야 할 것입니다.
69:31
결과적으로 여러분은 벡터를 길게 하는 것으로 속임수를 쓸 수는 없습니다.
69:34
이런 것은 내적을
69:38
유사성 측정의 도구로 씀으로써 벗어날 수 있는 일입니다.
69:39
답이 만족이 되었나요?
69:56
네.
69:58
제 말은, 그럴 필요가 없다는 것입니다,
70:01
논쟁하려면, 할 수는 있겠지만.
70:04
그렇게 하는 것은 미친짓입니다. 왜냐하면 구조적으로
70:08
이것은 워드의 컨텍스트에서 나타나는 가장 가능성 있는 워드 그 자체라는 것을 의미합니다.
70:15
그래서 좋을 결과를 가져올 것 같지 않습니다.
70:17
[웃음] 아마도 다른 단어가 나올 것입니다.
70:21
자, 더 복잡한 것을 하자고 말할 수 있겠죠.
70:28
각각의 컨텍스트가 잘 드러나도록 두 벡터 사이를 중재하는 어떤 것을 행렬에 넣지 않는가?라는 질문은
70:32
그럴 필요가 없는 것으로 밝혀져 있습니다.
70:39
왜냐하면 당연하지만 컨텍스트와 중심 워드 벡터에 대한 표현이 다르기 때문입니다.
70:44
반드시 같은 단어가
70:49
두개의 다른 표현에서도 가장 높게 나타난다는 가정이 필요한 것은 아닙니다.
70:53
그러나 실제로는 종종 그렇게 나타날 수 있습니다.
70:57
정말 그것이
71:00
꽤 잘 작동하는 이유이기도 합니다.
71:04
왜냐하면 여러분이 해볼 가능성이 없다는 것은 사실이지만
71:07
컨텍스트 안에 정확하게 동일한 낱말을 넣으면,
71:10
실제로 꽤 의미가 비슷한 단어를 얻을 가능성이 큽니다.
71:13
그리고 워드가 중심 단어와 강하게 연관되어 있으면
71:18
첫 단어로 컨텍스트 단어를 얻을 가능성이 높습니다.
71:22
그리고 일종의 매크로 수준에서, 여러분은 실제로
71:25
동일한 효과가 양쪽 모두에 나타나는 효과를 볼 수 있을 것입니다.
71:30
질문이 더 있나요? 네, 두 명 더 있네요.
71:32
어떻게 할까요?
71:32
뒤에 있는 사람부터 질문을 받고 그 다음에  앞에 있는 받을까요?
71:34
[웃음]
71:52
우리는 아직 경사하강법(gradient descent)을 다루지 않았습니다.
71:54
좀 뒤에 해야 할 것입니다.
71:58
오케이? >> [안들림]
72:00
>> 네.
72:11
>> 사실은,
72:12
우리가 막대한 양의 텍스트를 클릭했고.
72:15
그래서 우리 워드가 어디에 위치하건, 우리는 왼쪽에 있는 단어 다섯 개와
72:20
오른쪽에 있는 단어는 다섯 개를 알 수 있다는 것이 사실입니다.
72:23
이를 통해서 측정할 수 있는 것은
72:26
모든 단어가 그 컨텍스트에서 나타날 확률이고
72:28
그래서 실제로는 등장한 가족 워드처럼 될 것입니다.
72:32
어떤 확률로 그 결과를 얻었는가에 따라 몇 가지 대답이 있을 수 있습니다.
72:36
사실은
72:38
말할 시간이 거의 없네요.
72:43
끝내기 전에 최적화를 할 것입니다.
72:48
미분 역시
72:53
워드 벡터 문제를 해결하는 데 도움이 될 수 있니다.
72:57
그런데 아주 짧은 시간을 할애 할 것이기 때문에
73:03
221, 229 또는 유사한 수업을 통해서 배우시기를 바랍니다.
73:08
약간의 최적화를 배웠고 경사하강법의 일부를 배웠습니다.
73:15
이것은 아주 간단한 리뷰일 뿐입니다.
73:18
일단 한 아이디어는 점 x에서 경사도를 설정하고
73:22
그 일정 정도의 경사도를 빼려고 하면
73:27
그것이 최소가 되는 방향으로 내리막길을 움직여야 할 것입니다.
73:31
그래고 다시 경사도를 분수 계산으로 빼면
73:36
최소가 되는 방향으로 걷기 시작할 것입니다.
73:42
이것이 경사하강법의 알고리즘입니다.
73:46
그래서 일단 목적함수가 있으면 그 목적 함수의 미분을 구할 수 있을 것입니다.
73:51
모든 매개 변수에 대해서, 경사 하강
73:56
알고리즘은 현재 매개 변수 값을 가지고 있다고 가정하고.
74:02
그 위치에서 경사도를 계산합니다.
74:05
우리는 그 분수를 빼고
74:09
그러면 새로운 기대값으로 매개 변수 값을 얻을 수 있습니다.
74:14
우리에게 더 낮은 객체값을 주기 위해서, 최소가 되는 방향으로만 갈 것입니다.
74:21
그리고 일반적으로 효과가 있다는 것이 사실입니다.
74:28
파이썬 코드로 작성하고 싶으면,
74:31
진짜 루프를 돌리는 것은 정말 간단합니다.
74:36
경사하강도를 평가할 때 정지 조건이 있어야 하는데.
74:41
주어진 목적 함수나, 코퍼스와 진행되고 있는 매개 변수들로,
74:45
세타 그래디언트가 있을 것입니다. 이제
74:50
현재의 매개 변수 다음에 세타 그라디언트의 작은 분수를 빼는 것을 반복합니다.
74:55
그래서 이 그림에사 빨간 선은 일종의
75:00
목적 함수 값의  등고선의 일종이 될 것입니다.
75:03
경사도를 계산할 때 해야 할 일은
75:07
가장 가파른 정도와 방향에 따라 매번 조금씩 걷도록 하는 것입니다.
75:12
여러분은 최소한으로 부드럽게 걷기를 바라게 될 것입니다.
75:18
잘 작동하지 않는다면 그 이유는 첫 번째 스텝에서
75:22
발산해서 미니멈으로부터 벗어낫기 때문일 것입니다.
75:26
따라서 알파가 충분히 작아서 천천히
75:31
최소로 내려간 다음 작동하는 것이 중요합니다.
75:35
경사 하강도는 minimize functions이 가장 기본적인 도구입니다.
75:40
개념적으로는 가장 먼저 알아야 할 것이지만 마지막에 합니다.
75:46
제가 설명하고 싶은 것은
75:49
우리 코퍼스에 400 억 개의 토큰이 있을 수 있다는 것입니다.
75:54
그리고 목적 함수의 경사도를 계산해야 한다면
75:59
400 억 워드의 코퍼스라면 ‘영원히’ 라는 시간이 걸릴  수도 있습니다.
76:04
첫 번째 경사도 업데이트를 하기 전에 한 시간 정도를 기다려야 합니다.
76:09
따라서 현실적인 시간 내에 모델을 교육 할 수는 없습니다.
76:13
그래서 기본적으로, 모든 신경망에 나이브 배치
76:17
경사 하강 알고리즘을 사용할 수는 없습니다.
76:22
아니, 실제에 사용하기 어렵습니다.
76:24
대신에, 우리가 하는 것은 확률적 경사 하강법입니다.
76:28
확률적 경사 하강법 또는 SGD가 우리의 핵심 도구입니다.
76:34
그래서 그 의미가 무엇인지는, 텍스트에서 한 가지 입장을 취할 수 있는데,
76:40
그래서 우리는 하나의 중심 워드와 그 주위의 워드들이 있다면
76:45
위치를 조절하여 모든 매개변수와 관련한
76:49
경사도를 얻고
76:52
그리고 그 위치에서 경사도의 추정치를 사용하여
76:57
그 방향으로 조금씩 움직여 가는 것입니다.
77:00
단어 벡터 학습과 같은 일을 할 때,
77:05
이 경사도 추정치는 엄청나게 들쭉날쭉할 것입니다. 왜냐하면
77:09
우리는 한 위치에서는 몇 개의 워드에 대해서만 하기 때문입니다.
77:14
그래서 우리 모델의 매개 변수의 대다수는 전혀 보지 못한 것입니다.
77:18
그래서, 그것은 경사를 내려가는 것은 믿을 수 없을 정도의 들쭉날쭉한 추정치입니다.
77:22
그 방향으로 조금 내리막길을 걷는 것이
77:26
매우 들쭉날쭉해 보일 것이 확실합니다.
77:29
그러나 실제로, 이것은 보석처럼 작동합니다.
77:33
아니 사실, 더 잘 작동합니다.
77:36
다시 말하지만, 이기는 것입니다.
77:37
이런 식으로 일하는 것은 단지 규모에 따라 진행하는 것은 아니지만
77:42
배치 그래디언트 디센트보다 빠릅니다,
77:44
왜냐하면 모든 센터 워드 위치를 살펴본 후에 업데이트를 할 수 있기 때문입니다.
77:50
신경망 알고리즘은 잡음을 좋아한다는 것이 밝혀졌습니다.
77:55
따라서 이 경사도 하강 예측이 들쭉날쭉한 사실은,
78:00
실제로 SGD가 최적화 알고리즘으로 더 잘 작동하도록 도와준다고 보면 됩니다.
78:05
뉴럴 네트워크 학습은.
78:07
실제로 우리가 항상 사용하는 것입니다.
78:09
화재 경보기가 울리지 않더라도 오늘은 여기서 멈춰야 하겠습니다.
78:14
감사합니다.