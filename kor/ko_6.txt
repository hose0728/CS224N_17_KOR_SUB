00:00
스탠포드 대학교.
00:07
>> CS224N을 다시 시작하겠습니다.
00:10
딥러닝을 통한 자연어 처리.
00:14
오늘은 교대 수업으로 새로운 것을 배웁니다.
00:20
오늘 강의에서 주로 보게 되는 것은 통사,
00:26
문법 및 의존 파싱입니다.
00:28
오늘 저의 희망은 이 강의에서 
00:33
의존 문법과 구문 분석을 충분히 배워서
00:38
과제 2의 주요 부분을 모두 할 수 있게 하는 것입니다.
00:42
그래서 강의 초반 부분에는
00:46
문법과 의존 문법에 관한 약간의 배경 지식을 제공하겠습니다.
00:49
그런 다음 특정 종류의 의존 문법,
00:53
전이 기반 의존 파싱에 대해 이야기 하는 시간을 갖겠습니다.
00:58
아마 15 분 정도 밖에 안 걸릴 것이고 그 후에는
01:03
구체적으로 뉴럴 네크워크 콘텐츠로 되돌아와서 강의하겠습니다.
01:09
Danqi와 제가 몇 년 전에 썼던 의존성 파서에 대해 이야기하겠습니다.
01:14
상기시킬 것이 있는데,
01:16
제 1 과제가 오늘 마감일임을 기억하고 있기를 바랍니다.
01:21
그리고 저는 이 단계에서 여러분이 진보하기도 하고 아닐 수도 있을 것 같지만
01:26
기억할 수 있도록 알려 드리겠습니다.
01:32
제 생각엔 매년 꽤 많은 사람들이
01:36
특별한 이유 없이 첫 번째 과제에 늦게 내는데
01:39
좋은 전략이 아닙니다.[웃음].
01:43
잘해서 [웃음] 여러분이 과제를
01:47
주말 전에 제출하기를 바랍니다.
01:52
좋아요, 두 번째로 오늘은 새로운 과제가 나오는 날이기도 합니다.
01:59
다음 주 수업 시작 전까지는 못 만나겠지만
02:02
준비 되어 있겠죠.
02:04
몇 가지 새로운 것들이 포함될 것인데,
02:10
그 중 많은 부분은 다음 화요일 강의가 끝날 때까지 시작하지 못할 수도 있습니다.
02:15
과제를 위해서 두 가지 중요한 것들을 다룰 것입니다.
02:19
1번 과제와 상당히 다른데 텐서플로를 사용하여
02:25
2 번 과제를 수행해야 합니다.
02:29
아마 주말에 시작하고 싶지 않을 거예요. 왜냐하면 화요일에,
02:33
텐서플로에 대한 소개가 있을 것이기 때문입니다.
02:36
그러니까 그 다음에 시작하면 조금 더 좋은 결과를 얻게 될 것입니다.
02:40
그리고 과제 2에서 크게 다른 점은 우리가
02:45
보다 실질적인 자연 언어 처리 내용을 다룬다는 것입니다.
02:50
특히, 여러분은 뉴럴 의존성 파서를 만들 것이며
02:55
오늘 그렇게 하기 위해 알아야 할 모든 것에 대해 수업할 수 있기를 바라지만
03:00
또한 웹 사이트에서도 일부 읽을거리를 찾을 수 있을 것입니다.
03:03
여기서 저에게 곧바로 모든 것을 배우지 못해도
03:06
몇 가지 코멘트가 가능할 것입니다.
03:08
좋아요, 최종 프로젝트
03:13
게시물을, 내일이나 잘하면 주말에 포스팅할 수 있을 것입니다.
03:17
과제 4에 있는 개요의 일종으로,
03:21
4 번 과제를 수행할 때나
03:26
최종 프로젝트를 할 때 의미 있는 선택을 할 수 있는 정보가 될 것입니다.
03:27
과제 4의 영역은, 만약 여러분이 그것을 한다면,
03:31
SQuAD 데이터 세트의 문답입니다.
03:35
그 의미가 무엇인지 설명하기 위해 종류의 페이지와 설명이 있으니
03:39
찾아 볼 수 있을 것입니다.
03:40
최종 프로젝트에 관심이 있다면
03:44
최종 프로젝트 멘토 중 한 명과 만날 것을 권합니다.
03:50
여기 주변의 유능한 사람이 최종 프로젝트 멘토가 될 수 있을 것입니다.
03:55
그러니
04:00
모두 초록을 내기 전에 최종 프로젝트 멘토를 만나길 바랍니다.
04:03
그리고 정말로
04:06
여러분이 가능한 한 빨리 시작하기를 바랍니다.
04:09
여러분 중 일부가 이미 이야기를 나누었다는 것을 알고 있습니다.
04:12
개인적으로 저에게는 내일 최종 프로젝트 근무 시간이 있습니다.
04:18
오후 1 시부 터 3 시까지에 저는 여러분이 오기를 바랍니다.
04:22
그리고 다시, 리처드가 언급한 것과 같이,
04:25
모두가 리처드나 저를 최종 프로젝트의 스승으로 삼을 수는 없습니다.
04:30
게다가, 박사인
04:34
TA들이 최종 프로젝트 멘토가 된다는 것은 큰 혜택입니다.
04:36
왜냐하면, 텐서플로에서 시간을 많이 보냈기 때문에,
04:41
제가 하는 것보다 훨씬 많은 것을 할 수 있습니다.
04:43
Danqi, Kevin, Ignacio,
04:45
Arun은 딥러닝을 통해 NLP 연구를 해본 경험이 많습니다.
04:51
그러니 훌륭한 멘토가 되어 줄 것입니다.
04:55
찾아서 최종 프로젝트 조언을 듣기를 바랍니다.
04:58
마지막으로 분명히 많은 문제가 있다는 것입니다.
05:03
여러분이 상담 시간을 기다리는데도 불구하고 
05:08
대기 상태 표지가 정기적으로 통제에서 벗어나 있다는 것을 깨달았습니다.
05:13
불편하게 해서 미안합니다.
05:16
더 잘할 수 있는 방법을 찾기 위한 노력으로
05:21
알려지지 않았지만 두 번째 과제에
05:26
몇 가지 변화를 있음을 알려 드립니다.
05:28
여러분이 더 나은 조언을 받을 수 있도록
05:33
어떻게 하면 메시지를 자유롭게 보내고
05:38
Piazza 제안해서 더 잘 작업할 수 있도록 할까 생각하다가
05:41
어제 Percy Liang에게 물어 보았어요. Percy,
05:46
CS221을 어떻게 할까요?
05:49
이 일을 더 잘하기 위한 비밀이 있습니까?
05:52
그러나 불행하게도 어떠한 종류의 큰 비밀도 없는 것 같습니다.
05:57
“대기 상태인지 보고 Huang 지하층을 사용합니다. 그 밖의 무엇을 할 수 있을까요?” 라고 했기 때문입니다.
06:02
그렇지만 저는 여전히 신성한 통찰력을 찾고 있습니다. [웃음]
06:05
이 문제를 더 잘 해결할 수 있는 방법을 알려주면 좋겠습니다.
06:09
좋은 아이디어가 있다면 언제든지 공유해 주세요.
06:12
우리도 
06:17
다음 주는 좀 더 잘 통제된 상태에서 훨씬 더 잘하도록 노력하겠습니다.
06:22
질문 있나요? 아니면 바로 시작하는 게 나을까요?
06:29
좋아요.
06:32
그럼. 오늘 우리는
06:37
인간 언어로 문장을 구조화하는 법을 배웁니다.
06:43
제가 보여줄 모든 예는 영어에 대한 것이지만 원칙적으로,
06:47
모든 언어에 대해 적용 할 수 있는 동일한 기술로,
06:53
문장이 어떻게 구성되어 있는지 밝혀 낼 것입니다.
06:58
아이디어는 문장을 이루는 조각들로 구조를 만드는 데는 
07:04
함께 묶는 규칙이 있습니다.
07:09
"고양이"나 "개"처럼 아직 문장이 아닌 아주 단순한 것들로 시작해도
07:14
거기에서도 약간의 구조를 찾을 수 있는 것처럼 보입니다.
07:19
언어학자들이 종종 한정사(determiner)라고 부르는 관사
07:23
뒤에는 명사가 옵니다.
07:25
그리고 구에는,
07:28
명사구라고 부르는 것이 있는데 보통 무엇인가를 서술합니다.
07:31
더 길게 만들 수도 있는데 그렇게 하는 데에는 규칙이 있습니다.
07:37
또 관사와 명사 사이에 형용사를 넣을 수도 있습니다.
07:42
큰 개나 짖는 개 또는 껴 안고 싶은 개, 그런 것들을 말할 수 있습니다.
07:48
그리고, 명사 다음에 전치사구라고 부르는 것을 통해서
07:53
"a large dog in a crate" 같은 문장을 얻을 수 있습니다.
07:59
전통적으로 언어학자와 자연어 처리는
08:04
인간 언어의 구조를 설명하고 싶어 했습니다.
08:09
사람들은 이  작업을 수행하는 데 실제로 사용 된 두 가지 중요한 도구가 있습니다.
08:15
핵심 도구 중 하나는
08:18
제 생각에 일반적으로 
08:23
컴퓨터 과학 용어로는 문맥 자유 문법으로 불리고
08:28
언어학자들은 구 구문문법(phrase structure grammars)이라고 하는 것이 있습니다.
08:33
구성의 개념을 참조하면
08:37
우리가 하고 있는 일은 문맥 자유 문법 규칙을 작성하는 것입니다.
08:42
스탠포드 학부 강좌 등에서는
08:45
103 년의 오랜 시간이 흐르는 동안
08:46
문맥 자유 문법과 그 규칙에 대해 배우면서 전체 강의를 보냈습니다.
08:52
그래서 저는 명사구를 말하기 시작하면
08:56
관사 또는 명사로 이동하라는 것과 같은 규칙을 쓰기 시작할 것입니다.
08:58
그런데 저는 명사구가 좀 더 복잡하다는 것을 깨닫습니다.
09:02
그래서 저는 이 새로운 규칙을 생각해 냈습니다. 명사구는
09:05
종단에 선택적으로 명사가 삽입된 것이고 선택적 전치사구는
09:10
전치사의 뒤에 다른 명사구가 나오는 것입니다.
09:14
왜냐하면, a crate, 또는 a large crate라고 말할 수 있기 때문입니다.
09:18
또는 a large crate by the door.
09:20
좀 더 나아가서
09:25
a large barking dog by the door in a crate.라고 말 할 수도 있습니다.
09:31
여기에 여러 개의 형용사를 넣을 수도 있습니다.
09:35
여러 전치사 구를 붙일 수 있습니다. 그래서 저는
09:40
여러분도 볼 수 있도록 조금 어색하지만 별을 이용해 표시해 보겠습니다.
09:41
정규식에서 0 또는 임의의 수를 가질 수 있다는 뜻을 나타내죠.
09:46
좀 더 확장할 수도 있습니다. talk to the cuddly dog.
09:52
또는 look for the cuddly dog.
09:54
그리고, 전치사구가 뒤에 동사가 나오기도 합니다.
09:59
이렇게 구성 문법을 구축 할 수 있습니다.
10:03
이것이 문장 구조를 구성하는 한 가지 방법이며,
10:10
20 세기, 21세기 초에
10:16
미국에서는 이것이 지배적인 방법이었습니다.
10:21
아마 여러분은 Intro CS 수업에서 
10:27
정규 언어 및 문맥 자유 언어 및 문맥 의존 언어에 대해 주로 배울 것입니다.
10:33
그리고 Noams Chomsky가 만든 Chomsky 계층 구조를 공부하게 될 것입니다.
10:39
CS under grads SCS 103 학급에서 다루는 공식적인 내용으로
10:44
실제로 촘스키가 그 계층 구조를 만든 것은 아닙니다.
10:50
촘스키 계층 구조의 원래 목적은
10:54
인간 언어의 복잡성에 대한 논쟁이었습니다.
11:02
여러분이 더 광범위하게 보면, 미안합니다. 
11:07
Noam Chomsky의 연구는 지난 50 년 동안 미국의 언어학을 지배하고 있었습니다.
11:12
그러나 그보다 더 광범위하게 보면,
11:17
문장의 구조에 대한 이해에 사용되는 구문 설명이
11:19
실제로 지배적인 형태는 아닙니다.
11:22
그러면 또 무엇이 있을까요?
11:23
언어 구조에 대한 다른 대안적인 견해로 참고할 수 있는 것은
11:27
의존 구문입니다. 의존 구조는
11:31
각 단어로 이루어지는 문장의 구조로
11:37
의존을 설명합니다.
11:39
여기서, 단어는 일종의 수식어이거나
11:43
다른 단어의 논항입니다. 그것은 단어에 의존적입니다.
11:48
그래서 barking dog에서 barking은 dog에 의존적인 이유는 수식어이기 때문입니다.
11:54
Large barking dog, large 역시 수식어로 dog에 의존적입니다.
11:59
dog by the door에서 by the door는 dog에 의존적입니다.
12:05
우리는 단어들 사이에 의존성을
12:07
보통 화살표로 표시합니다.
12:11
그래서 우리는 어떻게 표현할 수 있는지와 관련하여 
12:16
다음과 같은 문장에 의존 구조를 그릴 수 있습니다.
12:19
첫 번째 수업에서 저는 모호한 문장의 예를 들었습니다.
12:24
많은 모호한 문장에서 우리는 의존성에 관해 생각할 수 있습니다.
12:29
“scientists study whales from space”라는 문장 기억하나요?
12:35
이것은 모호한 기사제목이었습니다.
12:38
이 문장이 왜 모호한 제목입니까?
12:40
두 가지 가능성이 있기 때문에 모호합니다.
12:44
그런데 두 경우 모두 주동사는 study 입니다.
12:49
연구하고 있는 것은 scientist로 study의 주어인 논항입니다.
12:53
연구되고 있는 것은 whales로 study의.
12:57
목적어인 논항입니다
12:58
우주에서 무엇을 하고 있느냐고 할 때 큰 차이점은
13:04
study를 수식어로 볼 것인지 아니면 whales를 수식어로 볼 것인에 달려 있습니다.
13:10
그리고, 만약 여러분이 제목을 빨리 읽는다면
13:13
It's 가 기저에 있는 것처럼 들리는 군요, 그렇죠?
13:15
It's whales from space
13:18
정말로 흥미롭게 들립니다.
13:20
하지만 [웃음] 그 기사가 의도 한 바는,
13:24
고래의 움직임을 추적하기 위해 인공위성을 사용할 수 있다는 것이었습니다.
13:27
그러므로 첫 번째 것인 from space가 수식어였습니다.
13:31
그들이 어떻게 연구되고 있는지.
13:33
많은 부분에서 문장의 모호성에 대해 생각할 수 있습니다.
13:39
무엇이 무엇을 수식하는지에 대해서는 이렇게 의존 구조의 관점에서 볼 수 있습니다.
13:43
그리고 이것은 자연어에서 정말로 흔한 일입니다.
13:47
무엇이 무엇을 수식하는지에 대한 질문은 해석을 할 때 
13:52
가장 많게 됩니다.
13:56
여기 이런 종류의 문장은
13:58
매일 아침 월스트리트 저널을 읽을 때도 찾을 수 있습니다.
14:02
The board approved its acquisition by Royal Trustco Limited of Toronto for
14:08
$27 a share at its Monthly meeting.
14:10
대괄호로 표시하면 좋겠습니다.
14:15
이 문장의 구조는 주어, 동사, 목적어로 시작해서
14:20
The board approved its acquisition,
14:21
뒤에 문장 전체인 전치사구
14:26
By Royal Trustco Ltd, of Toronto, for $27 a share, at its monthly meeting가 있습니다.
14:33
문제는 여기서 무엇이 수식하고 있느냐는 것인데
14:39
acquisition한 것은 Royal Trustco Ltd입니다.
14:45
Royal Trustco Ltd는 바로 앞에 있는 것을 수식하고 있습니다.
14:50
그리고 토론토는 Royal Trustco Limited
14:56
즉, 바로 앞에 오는 것을 수식합니다.
15:00
이게 쉽다고 생각할 수도 있습니다.
15:02
모든 것은 단지 그 직전에 오는 것을 수식합니다.
15:06
하지만 이제 더 이상 그것은 사실이 아닙니다.
15:08
그럼, for $27 a share를 수식하는 것은 무엇입니까?
15:14
네, 몇 명의 후보들 중에서 
15:16
acquisition이 
15:19
수식하고 있습니다. 그리고 실제로 it's monthly meeting은
15:24
Toronto the the Royal Trustco Ltd나 acquisition이  아니고
15:30
approval이 맨 위의 꼭대기에서부터 점프해서 내려옵니다.
15:35
이런 상황에서 동사나
15:40
명사구가 있다면, 이렇게 전치사구가 나타날 것입니다.
15:45
전치사구는
15:49
명사구나 동사를 수식할 수 있습니다,
15:51
여기 두 번째 전치사 구가 있습니다.
15:55
그런데 이 전치사 구 안에는 또 다른 명사구가 있었습니다.
15:58
그래서 지금
15:59
세 가지 선택이 있습니다.
16:00
이 명사구나 저 명사구 또는 동사구를 수식할 수 있습니다.
16:04
하나가 더 생겼으니까
16:06
이제 네 가지 선택이 있습니다.
16:08
그런데 완전히 자유롭게 선택할 수 있는 것은 아닙니다,
16:14
왜냐하면 내포 제약을 받기 때문입니다.
16:16
따라서 for $27 a share을 acquisition했다면,
16:22
다음은 일반적으로 전치사구로
16:25
acquisition이나 approved를 참조합니다.
16:28
일반적이라고 말한 이유는 예외가 있기 때문입니다.
16:31
그리고 저는 나중에 그것에 대해서 실제로 이야기 할 것입니다.
16:33
그러나 영어에서는 대부분이 사실입니다.
16:35
같은 것을 참조하거나 더 뒤로 참조하게 될 때,
16:38
내포 관계를 얻습니다.
16:40
그러나 비록 여러분이 내포 관계에 인정하더라도 결과적으로
16:45
문장을 기반으로 하면
16:51
전치사 구의 수가 많을수록 문장의 끝에 붙기 때문에 지수승의 모호함을 가질 수 있습니다.
16:55
그래서 일련의 기하급수적인 시리즈로 이 카탈랑 수를 얻습니다.
17:01
그래서 카탈랑 숫자는 실제로
17:03
이론 컴퓨터 과학의 많은 곳에서 나타납니다.
17:07
어떻든 비슷한 종류의 구조이기 때문에,
17:12
이러한 제약 조건을 넣으면 카탈랑 시리즈를 얻게 됩니다.
17:15
여러분들 중 CS228수업 듣는 사람 있나요?
17:20
네, 거기서
17:21
카탈랑 시리즈가 나오는 또 다른 곳은 벡터 그래프를
17:26
삼각형화할 때 그 벡터 그래프를 삼각 측량하는 방법의 수로
17:32
카탈랑 숫자가 나옵니다.
17:37
인간 언어는 매우 모호합니다.
17:41
그러고 오늘 의존성을 바탕에서부터
17:45
그려볼 수 있기를 바랍니다.
17:48
중요한 개념 중 하나로
17:50
이 시점에서 제가 소개하고 싶은 것은
17:55
언어학적인 사고에서 데이터를 주석 처리한 트리 뱅크 형태입니다.
18:01
정확하게 보기에는 부족할 수 있지만
18:05
여기 문장이 있습니다.
18:09
이것은 실제로 Yahoo Answers에서 온 문장입니다.
18:14
사람들이 둘러앉아서
18:19
문장의 통사 구조에 의존성 그래프를 그리는 일을 한 것을
18:24
우리가 트리 뱅크라고 부릅니다.
18:28
그래서 시작해서 일어난 정말 흥미로운 일입니다.
18:33
1990 년은 사람들이
18:38
이러한 종류의 주석 처리 된 트리 뱅크를 구축하는데 필요한 리소스와
18:43
나중에 설명하겠지만 다른 종류의 주석 처리 된 언어 리소스를 썼습니다.
18:47
이제 어떤 면에서는 2017 년 현대 기계 학습의 관점에서 볼 때,
18:52
이건 완전히 놀랄 만한 것은 아닙니다.
18:55
왜냐하면 우리는 항상 라벨이 붙여진 데이터를 원하는데
18:59
감독 분류기(supervised classifier)로 처리하면 좋은 결과를 얻을 수 있기 때문입니다.
19:04
그러나 여러 면에서 이것은 놀라운 일로
19:08
나머지 전체 역사와는 조금 다릅니다. 맞습니까?
19:13
그 밖의 모든 역사에서 
19:18
여기서 우리가 할 수 있는 일은 언어 구조를 
19:22
기술하는 문법 규칙을 작성하는 것이었습니다. 
19:27
이제 우리는 더 이상 문법 규칙을 쓰려고 하지 않고
19:31
그냥 몇 가지 문장을 주면
19:33
이 문장들로 다이어그램으로 만들 것이고 그 구조가 무엇인지를 보여줄 것입니다.
19:37
그리고 내일 더 많은 문장을 주면 또 여러분을 위해서 다이어그램으로 만들 것입니다.
19:41
생각해보면, 이런 방식은 어떤 면에서는 처음에는
19:46
쓸데없는 일인 것 같았습니다. 왜냐하면 그냥 구조를 씌우는 것으로
19:51
하나 하나의 문장은 정말로 비효율적이며 느렸기 때문입니다.
19:56
반면 문법을 쓰는 경우,
19:58
여러분은 이 일을 일반화한 것을 쓰고 있는 거죠?
20:00
문법의 요점은 여러분이 작은
20:03
유한 문법을 써서
20:03
무한한 수의 문장을 묘사할 수 있다는 것으로
20:06
확실히 노동 절약적입니다.
20:10
그러나 약간 놀랍게도, 어쩐 일인지
20:16
기계 학습이  매우 성공적인 것으로 판명 되었습니다.
20:21
명시적으로 주석이 달린 트리뱅크는
20:25
우리에게 많은 것을 주었습니다.
20:28
그리고 여기에 몇 가지 장점이 있습니다.
20:31
첫째, 그것은 여러분에게 노동의 재사용성을 제공합니다.
20:34
그러나 필기 문법의 문제는 거의 재사용 할 수 없다는 것입니다. 
20:38
왜냐하면 실제로 쓸 때 모두가 다르게 쓰고
20:43
문법에 대한 생각도 다르기 때문입니다.
20:45
그래서 사람들은 1 년 정도 작업하고 누구도 그것을 사용하지 않게 되었습니다.
20:50
이 트리 뱅크는 좀 더 효율적으로 재사용이 가능한 도구였습니다.
20:54
사람들은 모든 자연어를 처리 도구를 만들기 위해 위에까지 쌓아 올렸습니다.
20:58
거기에는 품사 태거 및 파서 및 그와 같은 것들을 포함합니다.
21:03
그들은 언어학자들에게 실제로 유용한 자료로 밝혀졌습니다.
21:07
왜냐하면 그것은 일종의 실제로 통용된 언어이고 통사적으로 완성되어 있기 때문입니다.
21:12
모든 종류의 양적 언어학에서 분석할 수 있습니다.
21:17
광범위한 정보를 제공하는 진정한 데이터로 사람들이
21:20
영어의 문법 규칙은 무엇인가에 대한 직관을 가지고 작업했습니다.
21:23
사람들은 어떤 것들을 생각하지만 다른 것들은 생각하지 않습니다.
21:25
그래서 이것은 실제로 
21:29
나타난 모든 것을 발견하는 더 나은 방법입니다.
21:30
확률론적이거나
21:33
기계 학습에서 발생할 가능성 뿐 아니라
21:37
공기 관계 경향성에 대한 빈도 그리고
21:41
모든 종류의 분포 정보를 주는데 이것은 매우 중요하다고 할 수 있습니다.
21:44
그리고 결정적으로, 결정적으로, 결정적으로, 우리는 과제 2번에
21:48
시스템을 평가할 수 있는 방법을 제공한다는 점에서 매우 좋습니다.
21:54
지금 표준 데이터로 지상의 진리처럼 취급되는
21:59
정답이 있습니다.
22:01
즉, 도구를 재현하면서 도구가 얼마나 좋은지 평가할 수 있을 것입니다.
22:06
좋아요, 여기까지는 일반적인 광고였습니다.
22:09
그리고 이제 좀 더 신중하게
22:14
15 분 동안 의존성 문법과 의존 구문이 무엇인지 보겠습니다.
22:19
곧바로 이야기하면,
22:21
어쩌면 제가 말하지 않았을 수도 있지만,
22:25
문맥 - 자유 문법 문법의
22:29
관점과 의존 문법 관점이 있음을 언급했었습니다.
22:33
오늘, 의존에 대해서만 이야기할 것입니다.
22:36
그리고 과제 2에서 우리가 하는 일도 모든 의존입니다.
22:39
구성과 구문에 대한 개념으로 돌아갈 것입니다.
22:43
몇 주 후에 나중 수업에서도 다시 보게 될 것입니다.
22:48
그러나 이것은 우리가 오늘 해야 할 일입니다.
22:50
그리고 그것은 완전히 임의의 선택이 아닙니다.
22:53
그것은 대부분의 언어학에서 지난 50 년 동안 일어난 것과는 달리
22:57
자연 언어 처리의 지난 10 년 동안,
23:02
의존 문법을 주도적으로 사용해 왔으며,
23:06
의존 문법이 적절한 프레임워크로
23:10
의미론적 표현을 구축하여
23:14
언어에 대한 이해에 쉽게 접근하는데 가장 적절하다고 판정했습니다.
23:18
매우 빠르게 만들 수 있고
23:21
효율적인 파서인데, 나중에 설명하겠습니다.
23:24
그리고 지난 10 년 동안,
23:26
여러분은 자연 언어 처리가 변화하는 거대한 바다를 보았을 것입니다.
23:30
반면, 1990 년대에 컨퍼런스 볼륨을 가면 기본적으로
23:35
모든 구 구조 문법이고 의존 문법에 대한 논문이 한 두 개였습니다.
23:39
그런데 지금 하나를 집어 들면
23:41
여러분은 통사 표현에 대한 논문의
23:46
80 %가 의존 표현을 사용하고 있는 것을 알 수 있을 것입니다.
23:50
오케이.
23:52
>>(질문) 구 구조 문법이란 무엇입니까?
23:53
구 구조, 구 구조 문법은 정확히 동일한데
23:56
언어학자가 말할 때 문맥 자유 문법이라고 합니다.
23:59
[웃음] 네, 예전에는 문맥 자유 문법이었습니다.
24:04
좋아요, 그럼 의존 구문은 무엇입니까?
24:09
따라서 의존 구문의 개념은 통사적 모델로
24:14
어휘 항목 사이의 관계를 나타내는데,
24:19
단어와 어휘 항목 중에서, 오직 어휘 항목 사이에서만의
24:23
이진적이고 비대칭적인 관계로 이는 우리가 화살표를 그려야 함을 의미합니다.
24:28
화살로 의존 관계를 나타낼 문장은
24:31
bills on ports and
24:36
immigration were submitted by Senator Brownback, Republican of Kansas입니다.
24:41
오케이 여기서 시작해서 의존 파싱을 해 봅시다.
24:47
조금 더 해보면
24:50
일반적으로 우리는 의존성에 이름을 부여하여
24:56
문법적 관계로 의존성을 입력합니다.
25:00
저는 이것을 주어라고 부르겠습니다. 실제로는 수동 주어입니다.
25:05
그리고 이것은 보조 동사 수식어(auxiliary modifier)입니다.
25:08
Republican of Kansas은 Brownback의 뒤에 나오는 어구입니다.
25:14
우리는 이렇게 의존 문법을 사용합니다.
25:18
그리고 흥미롭게 다룰 수 있는
25:23
몇 가지 재미있는 수학을 것입니다. 만약 여러분이 이것을 안다면,
25:28
표기법은 매우 다르지만,
25:31
문맥 자유 문법도, 실제로는 동일하게
25:37
제한된 종류의  문맥 자유 문법을 한 번 더 추가 할 수 있습니다.
25:41
하지만 일단 의존성 레이블을 타이핑하면 상황이 조금 달라지는데
25:46
자세히 설명하지는 않을 것입니다.
25:51
따라서 의존성 문법의 실질적인 이론을 위해
25:55
몇 가지를 결정해야 합니다.
25:59
우리가 할 일은 두 가지 사이에서 화살을 그리는 것입니다.
26:03
좀 더 많은 용어를 언급하겠습니다
26:07
여기 화살을 가지고 있고, 끝부분을 테일이라고 부를 것입니다.
26:14
그리고 여기에 있는 이 단어는 일종의 머리어입니다.
26:16
따라서 bills은 논항으로 나와 있고  were는 auxiliary modifier로 나와 있습니다.
26:23
이 단어는 일반적으로 머리어(head) 또는 지배소(governor), 
26:27
superior 또는 regent라고 불립니다.
26:31
저는 그것을 보통 머리어라고 부를 것이다.
26:34
그리고 나서 화살표의 다른 뾰족한 끝에 있는 단어를
26:37
의존소(dependent)로 부를 것이다.
26:40
때때로 다른 단어로 modifier, inferior, subordinate로도 불립니다.
26:46
의존성 문법에는 이렇게 실제로 계급주의적 들어간 개념인
26:50
superiors와 inferiors가 있지만 저는 머리어(heads)와 의존소(dependents)의 개념을 쓰겠습니다.
26:55
이제 여러분은 절의 머리어와
26:59
의존소 논항을 보고 있습니다.
27:01
그리고 다음과 같은 문구에서는
27:05
by Senator Brownback, Republican of Texas.
27:10
Brownback이라는 머리어가 있고
27:14
그 다음 단어가 있습니다.
27:16
결국 오늘 배울 의존성 문법의 주요 부분 중 하나는
27:21
여러분은 어떤 단어가 머리어이고
27:25
어떤 단어가 특정 구문의 의존소인지 결정하는 것입니다.
27:31
이 다이어그램에서 제가 여러분들에게 보여드릴 것은
27:35
제가 몇 페이지를 보여준 것과 보여주고 있는 것들은
27:39
보편적인 의존성(universal dependencies)에 따른 분석입니다.
27:43
보편적인 의존성은 새로운 트리뱅크 노고의 결과이고
27:47
저는 실제로 매우 강하게 개입했었습니다.
27:50
몇 년 전에 시작해서
27:52
이전 슬라이드에는 장표가 몇 개 있고
27:55
배우고 싶다면 웹 사이트에서 보편적 의존성에 대해 많은 것이 있습니다.
27:59
제 생각에 이것은 야심적인 시도일 수 있는데
28:01
아주 많은 언어에서 작동하는 공통적 의존성 표현이 있어야 합니다.
28:06
저는 그것에 관해서 한참 떠들 수 있고
28:08
수업이 끝날 때 쯤 시간을 낼 수 있겠지만
28:13
아마 그렇게 하지 않을 것입니다. 사실 이번에 그것에 대해 많이 말하지 않고
28:17
여러분이 매우 빨리 알만한 것들만 언급할 것입니다.
28:22
오늘 주어진 시간에는 
28:26
다른 곳에서도 보았겠지만 보편적인 의존성의 분석으로
28:32
전치사를 다르게 취급하게 될 것입니다.
28:37
영어 문법에 대한 이야기를 많이 들었거나 영어 수업을 들으면
28:42
전치사와 목적어를 들어 보았을 것입니다.
28:46
보편적인 의존성에서 전치사는 어떤 의존성도 갖지 않습니다.
28:52
전치사는 대소문자 표지처럼 취급됩니다.
28:56
그런데 독일어, 또는
28:59
라틴어, 힌디어 같은 언어를 아는 경우,
29:04
by는 Brownback의 격표지처럼 취급됩니다.
29:09
Brownback 상원의 수식어로
29:13
그리고 실제로 Brownback을 머리어로 여기고 있습니다.
29:17
전치사 by는 격표지 의존소와 같습니다.
29:21
이것은 전세계
29:25
다른 언어에 더 많은 병렬 처리를 하기 위한 것 중의 하나였습니다.
29:26
이 점만 언급하겠습니다.
29:29
의존성의 오래된 또 다른 속성은 일반적으로 트리를 형성한다는 것입니다.
29:35
즉, 공식적으로
29:37
즉, 단일 헤드를 가지고, 비순환적이며, 연결되어 있습니다.
29:45
따라서 일종의 그래프 이론과 같은 속성이 있습니다.
29:49
이미 언급했듯이
29:52
의존성이 세계의 대부분을 지배하고 있는데
29:55
그것도 아주 빨리 그렇게 하고 있습니다. 
29:58
유명한 언어학자는 파니니는
30:03
기원전 5 세기 경 산스크리트 문법을 썼습니다.
30:08
파니니가 한 대부분의 작업은 사운드 시스템과 같은 작업으로
30:13
우리가 첫 번째 수업에서 언어적 수준으로 언급한
30:16
단어, 음운 및 형태를 구성했습니다.
30:20
그리고 그는 문장의 구조에 대해서 약간의 작업을 했습니다.
30:24
하지만 그가 사용했던 표기법으로 보면
30:26
문장의 구조는 근본적으로 단어 간의 관계가 의존적으로 표지되는
30:31
의존성 문법이었습니다.
30:37
(질문) 음 질문인가요?
31:12
네, 문제는 CFG와 PCFG(Probabilistic context-free grammar)를 잘 비교하고 있는데
31:17
의존성 문법은 강하게 어휘적으로 보입니다.
31:22
단어들 사이에 관계를 일반화하는 것은 좀 더 어려워서
31:28
솔직히 저는 지금 그 정의를 할 수 없다는 생각이 드는데
31:31
질문 부분이 오늘 강의 나머지 부분에 나올 것입니다.
31:33
하지만 두 가지 의견이 있을 수 있을 것입니다.
31:36
자연스럽게 의존성 문법은 분명히
31:41
강하게 어휘적으로 표현되어 단어들 사이의 관계를 그려 내고 있습니다.
31:46
문맥 자유 문법을 생각하는 가장 간단한 방법은
31:49
카테고리의 관점에서 규칙으로
31:51
명사구는 한정 명사(determiner noun)나 선택적인 전치사구(prepositional phrase) 같은 것으로 다룰 것입니다.
31:56
이것은 큰 차이로
32:00
양쪽 방향에서 진행됩니다.
32:03
그래서, 보통 자연 언어 처리를 하는 사람들은 실제로
32:07
문맥 자유 문법은 종종 문법적으로
32:11
보다 정확한 확률적 예측을 할 수 있고 그 반대도 마찬가지라고 합니다.
32:15
일반화와 의존성 문법을 하고 싶다면,
32:18
최소한 여전히 품사 개념을 사용할 수 있고
32:21
더 많은 카테고리화와 같은 것을 일반화 수준에서 할 수 있습니다.
32:25
그럼에도 불구하고, 자연스럽게 확률적 방법으로 이끌고 있는데
32:29
그 반대편인 기계 학습 모델은 매우 다릅니다.
32:33
그럼에도 불구하고
32:36
그 사이에 관계된 몇 가지 결과가 있습니다.
32:37
그러나 저는 거대한 탈선을 하지 않는 것이 좋을 것이라는 생각이 드네요.
32:40
또 다른 질문이 있습니까?
32:44
즉, 명사구와 같은 범주를 가지기보다는
32:49
개가 표제로 하는 명사구와 같은 카테고리로 어휘 사전으로 표시된다고 한 것입니다.
32:54
잠시 이 문제는 넘어 가도록 합시다.
32:58
[웃음] 좋아요, 그럼.
33:02
그것이 파니니입니다. 전체적으로 긴 역사가 있죠?
33:05
근본적으로 라틴 문법 주의자들이
33:09
라틴어의 문법을 그리 개발하지 않았고
33:13
주로 형태론을 했지만
33:15
본질적으로 일종의 의존성 분석을 했습니다.
33:18
첫 번째 밀레니엄에 아랍 문법 학자의 꽃이 피었고
33:23
그들도 본질적으로 의존 문법을 했습니다. 
33:25
문맥 자유 문법과 대조되는 구문 문법도
33:32
거의 20 세기 후반에 만들어졌습니다.
33:39
실제로 원래 발명 한 것은 촘스키가 아니라,
33:41
영국에서 약간의 초기 연구가 있었지만 그 10 년 전쯤 
33:47
프랑스 언어학자인 Lucien Tesniere가 있었습니다.
33:53
그는 종종 현대 의존 문법의 아버지라고 불리며,
33:57
1959 년부터 책을 썼습니다.
33:59
의존 문법은 더 자유로운 어순 언어에서 매우 인기가 있었습니다.
34:06
왜냐하면 문맥 자유 문법과 같은 개념은
34:10
영어와 같이 어순이 매우 고정되어 있는 언어에 잘 잘동했습니다. 그러나
34:13
세계의 많은 다른 언어들은 훨씬 더 자유로운 어순을 가지고 있습니다.
34:19
그리고 그것은 종종 의존 문법으로 자연스럽게 기술됩니다.
34:24
흥미롭게도, 최초의 자연 언어 파서는
34:28
의존 파서로 미국에서 개발되었습니다.
34:33
데이빗 헤이스는 미국 최초의 전산 언어학자 중 한 사람입니다.
34:37
회의 논문 등을 출판하는 주요 학술 단체인
34:42
전산 언어학 협회의 설립자 중 한 사람이고
34:46
1962 년 영어에 대한 의존 파서를 실제로 만들었습니다.
34:55
자, 의존 문법은 긴 역사가 있습니다.
34:58
따라서 표기법에 대해 알아 두어야 할 점들이 있습니다.
35:03
사람들이 화살표를 그리는 방식이 항상 일관적이지는 않은데
35:07
저는 항상 머리어에서 의존소로 그립니다.
35:12
그것은 테스니에르가 그린 방향과 같습니다.
35:14
그러나 다른 방향으로 화살을 그리는 사람들이 있습니다.
35:18
그들은 의존소에서 머리어를 향해 그립니다.
35:20
그래서 사람들이 무엇을 하고 있는지 봐야 합니다.
35:23
매우 일반적으로 행해지는 다른 일은 파싱을 하는 것입니다,
35:27
여러분은 ROOT 또는 WALL 등으로 되어 있는 이 매꿈 단어(pseudo-word)를 붙이거나
35:32
또는 그와 같은 다른 이름을 문장의 시작 부분에 넣으면 됩니다.
35:37
그것이 이런 종류의 수학과 형식주의를 쉽게 합니다.
35:42
왜냐하면 모든 문장은 루트로 시작하고 무언가는 루트에 의존하기 때문입니다.
35:4
또는 다른 방향에서 생각하면 의존 문법으로 파싱한다는 의미는 
35:53
문장에 있는 모든 단어에 대해 무엇이 의존적인지 말하려는 것입니다.
35:56
왜냐하면 다 끝났을 때
35:59
여러분은 문장의 의존 구조를 알 수 있을 것이기 때문입니다.
36:02
이제 여러분은
36:07
문장에서 다른 단어에 의존적으로 되거나 또는 매꿈 단어 ROOT에
36:10
의존적으로 될 것인데 전체 문장의 머리어가 되는 것이 의미하는 것은 무엇이냐고 말하고 싶을 것입니다.
36:16
이제 수업의 후반부에는
36:22
의존성 분석의 세부 사항을 살펴보고
36:24
여러분이 잘 생각해야 할 것들을 설명할 것입니다.
36:27
어떤 단어가 무엇에 의존하는지 어떻게 결정할 수 있습니까?
36:33
그리고 우리가 생각할 수 있는 다양한 정보 소스가 있습니다.
36:38
그것은
36:42
단어 관계로 생각되는 의존 표현이 자연스럽습니다.
36:44
정말 좋은 점은 우리가 이미 한
36:48
분산된 단어 표현과 잘 어울린다는 것입니다.
36:49
그래서 실제로, 이런 식으로 일하는 것이 잘 맞는
36:53
몇 가지 도구 사용법을 이미 알고 있습니다.
36:57
이 문제에 대한 토론할만한 이슈는
37:00
어휘적 의존성을 표시할 적절한 이유가 있습니까? 인데
37:04
실제로 사용할 많은 정보가 있고
37:07
우리가 사용하고자 하는 다른 정보 소스도 있습니다.
37:11
의존성 거리, 때로는 의존성 관계가 있으며
37:16
긴 문장이 생기면 20 단어 떨어져 있는 단어들 사이의 문장의
37:21
이전 절로 돌아가 참조할 수 있습니다.
37:23
이런 경우는 드물기는 하죠.
37:24
대부분의 의존성은 짧은 거리를 선호하기 때문에
37:30
많은 경우에 의존성이 특정 종류의 것을 넘어가지는 않습니다.
37:35
따라서 형용사 수식어와 같이 명사구 안에서 발생하는 종류의 의존성이 있다면,
37:40
동사를 넘어서지는 않습니다.
37:44
많은 종류의 의존성이 구두점을 건너는 것이나
37:49
동사와 주어 사이에 구두점을 사용하는
37:53
것과 같은 일도 매우 드뭅니다.
37:54
따라서 중간에 있는 것들을 살펴보면 단서를 얻을 수 있습니다.
37:57
그리고 정보의 최종 소스가 머리어라고 생각하면
38:03
몇 개나 의존하게 되는지, 어떤 면에서 의존 하는지와
38:09
같은 종류의 정보가 있습니다. 네
38:13
the와 같은 단어는 기본적으로 의존성을 가질 가능성이 거의 없습니다.
38:18
그렇다면 여러분은 놀랄 것입니다.
38:21
명사와 같은 단어는 의존어를 가질 수 있으며, 상당한 수의 의존어를 가지고 있습니다.
38:26
그들은 왼쪽에 한정사나 형용사 같은 종류가 있고
38:31
오른쪽에는 전치사 구와 같은 다른 종류가 있을 것입니다.
38:34
동사는 의존성이 많습니다.
38:36
그래서 단어가 달라지면 의존성의 패턴도 달라지는데
38:40
이것이 우리가 모으기를 희망하는 정보입니다.
38:45
네 여기서는 이미 첫 번째 요점을 말한 것 같네요.
38:50
의존 구문 분석은 어떻게 합니까?
38:52
원칙적으로는 정말 쉽습니다.
38:56
문장에 모든 단어를 취해서
39:01
이 단어가 어떤 단어의 루트인지를 결정하고
39:07
몇 가지 제약 조건을 걸면 됩니다.
39:10
그래서 우리는 한 단어만이 루트에 의존할 수 있다고 정하고
39:16
어떤 사이클도 허용하지 않을 것입니다.
39:19
그 두 가지를 모두 했다면,
39:23
트리 구조의 의존성을 보장한 것입니다.
39:27
그리고 일반적으로 의존성을 트리 구조로 만들고 싶어 합니다.
39:32
그리고 하나 더 언급하고 싶은 특질이 있습니다.
39:37
제가 여기 있는 것처럼 의존성을 그리게 되면
39:42
모든 의존성은 단어 위에 루프로 그려지게 됩니다.
39:48
단어의 일부를 단어 아래에 그려 넣는다면 다른 것입니다.
39:52
어떻게 이렇게 그릴 수 있는지에 대한 질문이 있습니다.
39:56
이렇게 멋지고 작은 내포 구조를 가지고 있지만
40:00
어느 것도 서로 교차하지 않아야 합니다.
40:02
또는, 제가 여기에 있는 이 두 가지처럼,
40:07
서로 건너가는 것을 피할 수 없을 때도 있습니다.
40:12
그러나 대부분의 언어 특히 영어에서는
40:17
대다수의 의존 관계는
40:22
선형 순서로 내포 구문 관계에 있습니다.
40:26
그리고 의존성 트리가 완전히 내포되어 있다면,
40:29
이것은 투영 의존성 트리(projective dependency tree)라고 불립니다.
40:32
이쪽 면에 그것을 배치하면, 일종의 내포 관계에 있게 됩니다.
40:37
이런 구조는 거의 없는데
40:41 
영어에서 내포되지 않았고 아직 교차하지도 않는 것이 있을 수 있습니다.
40:45
그래서 다음 문장은 자연스러운 예입니다.
40:47
I'll give a talk tomorrow on bootstrapping.
40:50
명사 수식어로 할 수 있는 일, 특히
40:55
bootstrapping이나 techniques of bootstrapping과 같이 긴 단어는
40:59
문장의 끝 부분으로 그들을 옮길 수 있습니다.
41:03
I'll give a talk on bootstrapping tomorrow 라고 말해도
41:07
꽤 자연스럽죠. 하지만 I'll give a talk tomorrow on bootstrapping에서
41:12
on bootstrapping은 아직도 talk을 수식하고 있습니다.
41:15
언어학자들이 외치변형이라고 하는 것을 참조하면
41:20
그런 종류의 오른쪽 방향으로의 이동은
41:23
교차 선으로 끝나게 됩니다.
41:26
이것을 비 - 사상 의존 트리(non-projective dependency tree)라고 부릅니다.
41:31
중요한 것은, 만약 여러분이
41:35
선형 차수의 제약 조건을 무시한다면 아직도 트리라는 것입니다.
41:39
이론 전산학에 그래프 이론에서도 아직 트리입니다.
41:43
단어의 선형 순서에 이것을 여분인 것으로 고려할 때만
41:48
선을 가로 질러야 합니다.
41:51
그래서 실제로는 잘 볼 수 없는 이 특징은
41:54
그래프에서의 전산학에서의 이론적 토론 후에
41:57
이 속성을 투영으로 언급하고 있습니다.
42:00
예. >> [안들림]
42:08
>> 질문은
42:10
의존성 트리에서 단어의 순서를 복구하는 것이 가능하냐는 것입니다.
42:14
주어진 의존성 트리를 정의하는 방식에 대한 엄격한 대답은 “아니오” 입니다.
42:20
이것은 순서에 대한 것은 전혀 주지 않았습니다.
42:22
이제 실제로, 사람들은 문장의 단어를 순서대로 적고 이 교차괄호를 둡니다.
42:27
그리고 비투영 일 때 화살표를 건너면 됩니다.
42:33
그리고 물론 단어를 색인하는 것은 간단합니다.
42:37
분명히, 선형 순서를 갖는 것은 언어에 대한 진정한 의미일 수 있고
42:41
그것을 부정 할 수 없지만
42:42
의존성 구조를 정의했으므로
42:47
여러분은 이것으로부터 단어의 순서를 실제로 회복 할 수는 없습니다.
42:50
다음 슬라이드를 하기 전에 막간 시간을 갖겠습니다.
42:56
네, 그리고 수업의 후반부에
42:58
의존성 분석 방법에 대해 이야기 하겠습니다.
43:04
잠깐만 말하면
43:08
의존 구문 분석을 수행하는 방법에 대해 설명합니다.
43:12
의존 구문 분석을 수행하는 가장 눈에 띄는 방법은 동적
43:17
프로그래밍 방법입니다.
43:18
이것은 보통 사람들이 구문 문법에 사용했던 것입니다.
43:22
두 번째 방법은 그래프 알고리즘을 사용하는 것입니다.
43:27
의존 구문 분석을 하는 일반적인 방법은 
43:32
Minimum Spanning Tree algorithms인, MST 알고리즘을 사용하는 것입니다.
43:33
그리고 그것은 실제로 매우 성공적인 방법입니다.
43:36
여러분은 제약 조건 만족 문제를 볼 것입니다.
43:40
이제까지 그렇게 해 왔습니다.
43:43
하지만 최근에는 네 번째 방법도 씁니다.
43:47
일반적으로 전환 기반 파싱이라고도 하지만 처음
43:51
소개될 때는 종종 결정론적 의존 파싱이라고 불렸었습니다.
43:56
그 아이디어는 탐욕적 방법과 같은 종류로
44:01
각 단어가 의존되어 있는 단어를 결정하고,
44:07
기계 학습 분류기로 가이드를 받는 방식입니다.
44:11
그리고 이것이 과제 2에 사용할 방법입니다.
44:14
이것에 대해 제안할 수 있는 방법으로
44:17
이 수업을 통해 우리는 두 개의 망치를 가지고 있습니다.
44:22
하나는 단어 벡터이며 그것으로 많은 것을 할 수 있을 것입니다.
44:27
또 다른 망치는
44:32
최고에 softmax가 있는 피드포워드 뉴럴 네트워크로
44:36
두 개의 다양한 클래스를 구분할 수 있을 것입니다.
44:40
이 두 개의 망치를 이용해서
44:43
의존성 분석을 할 수 있을 것이고 실제로 잘 작동할 것입니다.
44:47
따라서 과제 2에서 사용할 수 있는 훌륭한 접근 방법입니다.
44:51
그리고 과제 2에 대한 훌륭한 접근 방식일 뿐만이 아니라
44:54
4번째 방법이 요즘의 지배적인 방법입니다.
44:59
의존성 파싱은 확장성이 매우 뛰어납니다.
45:07
탐욕적 단어(greedy word)로 
45:12
다른 방법들이 없다면 선형 시간 알고리즘이라고 말하는 방법이 있습니다.
45:15
웹 스케일 파싱이 현대 세계에서,
45:18
가장 좋아하는 방법이 되고 있습니다.
45:21
그래서 잠시 후에 이것에 대해서 좀 더 이야기할 것입니다.
45:24
그러나 그걸 하기 전에,
45:25
우리는  Ajay의 연구를 들으면서 단어 벡터에 대한 마지막으로 한 번 더 살펴보겠습니다.
45:33
>> 들리나요?
45:35
의존성 파싱을 잠시 중단하고
45:38
워드 임베딩이라는 우리가 많이 알아야 할 것에 대해 이야기하겠습니다.
45:43
오늘 연구의 하이라이트는
45:48
‘워드 임베딩에서 배운 교훈으로 분포의 유사성 개선하기’라는 제목의 논문을 살펴볼 것입니다.
45:54
Levy, et al.이 저술했습니다.
45:58
우리는 수업 시간에 단어 벡터를 생성하는 두 가지 주요 패러다임을 배웠고
46:03
카운트 기반 분산 모델을 배웠는데
46:07
본질적으로 공기 관계 행렬을 사용하여 단어 벡터를 생성하는 것이었습니다.
46:13
그리고 우리는 SVD (Singular Value Decomposition)를 배웠습니다.
46:17
그리고 우리는 아직 PPMI에 대해 이야기하지 않았습니다.
46:20
그러나 사실상,
46:21
여전히 ​​공기관계 행렬을 사용하여 단어에 대한 희소 벡터 인코딩을 생성합니다.
46:26
우리는 또한 뉴럴 네트워크 기반 모델을 배웠고,
46:28
이제 여러 가지 경험을 해야 할 차례입니다.
46:31
그래서 구체적으로 우리는 Skip-Gram의 네거티브 샘플링과
46:37
CBOW 방법에 대해 이야기했습니다.
46:39
GloVe는 뉴럴 네크워크 기반 모델입니다.
46:43
뉴럴 네트워크 기반 모델이
46:48
카운트 기반 모델보다 우수하다는 것을 알고 있을 것입니다.
46:51
그러나, Levy 등은 하이퍼 파라미터 및
46:56
시스템 설계 선택이 내장 알고리즘 자체보다 더 중요하다는 가정을 했습니다.
47:00
그래서 그들은 이 대중적인 대회에 도전하고 있습니다.
47:04
그리고 본질적으로, 그들이 논문에서 말하고 있는 것은
47:09
구현할 때 수많은 하이퍼 파라미터를 제안하고, 
47:15
튜닝할 때 백분율 기반 분산 모델로
47:21
뉴럴 네트워크 기반 모델에서 일관성이 없는 지점까지의 성능에 거의 접근했습니다.
47:24
그들이 시도한 여러 가지 작업을 통해 더 나은 선택을 했고
47:29
하이퍼 파라미터들은 실제로
47:32
스킵 그램 (Skip-Gram)과 같은 뉴럴 네트워크 기반 모델에서 영감을 얻은 것입니다.
47:37
상기해 본다면, 이것은 모두 매우 친숙한 내용일 것입니다.
47:40
Skip-Gram에는 2 개의 하이퍼 파라미터가 있습니다.
47:44
샘플링 중인 네거티브 샘플 수와
47:47
unigram 분포 평활 지수, 우리는 3/4에 고정했습니다.
47:51
그러나 더 많은 시스템 설계 선택을 생각해 낼 수 있을 것입니다.
47:57
그리고 이것들은 카운트 기반의 variants로 이전 될 수도 있습니다.
48:00
좀 빨리 훑어 보면
48:03
하나의 하이퍼 파라미터로 Levy et al.이 제안한
48:07
가장 큰 영향을 미칠 만한 것은
48:12
Context Distribution Smoothing로 Context Distribution Smoothing와 유사한 방법입니다.
48:16
unigram 분포 평활화 상수는 여기 3/4 입니다.
48:23
그리고 두 가지 목표를 달성했는데
48:27
희소한 단어에 불이익을 줄 수 있도록 분포를 부드럽게 정렬했고
48:33
그리고 매우 흥미롭게도 이 하이퍼 파라미터를 사용하여,
48:38
최적의 알파는 정확히 3/4 이라는 것을 밝혀냈습니다.
48:43
이것은 Skip-Gram Unigram 스무딩 지수와 동일합니다.
48:48
그리고 전반적으로 평균 3 점의 성능 개선을 할 수 있었습니다.
48:52
꽤 흥미롭다고 할 수 있습니다.
48:56
그리고 그들은 또한 Shifted PMI를 제안합니다.
48:58
이것에 대해 자세히 설명하지 않겠지만
49:00
네거티브 샘플링과 유사하게
49:05
Skip-Gram에서 네거티브 샘플 수를 선택합니다.
49:10
그리고 총 8 개의 하이퍼 파라미터를 제안했고
49:16
우리는 그 중에 하나인 Context Distribution Smoothing을 설명했습니다.
49:22
여기 결과가 있습니다.
49:24
많은 양의 데이터를 썼는데 여러분들도 혼란스러울 때는 이것이 결론일 수 있습니다
49:29
여기에는 어떤 경향성도 없기 때문에
49:35
저자들은 네 가지 방법을 모두 취했습니다.
49:41
3 개의 다른 윈도우를 선택했고 모든 모델을 테스트했습니다.
49:46
그리고 그것들은 단어 유사성과 유추 작업으로 나뉘어져 있습니다.
49:50
그리고 이 모든 방법들은
49:54
성능을 최적화하기 위한 최상의 하이퍼 파라미터를 찾는 것으로 조정됩니다.
49:57
그리고 최고의 모델은 굵게 표시되어 있습니다. 일관된 최상의 모델은 없습니다.
50:03
사실, 그들은 대중적인 관습에 도전하고 있습니다.
50:09
뉴럴 네트워크 기반 모델은 카운트 기반 모델보다 우수합니다.
50:15
그러나 여기에 유의해야 할 몇 가지 사항이 있습니다.
50:18
첫 번째로, 하이퍼 파라미터를 추가하는 것은 결코 좋은 일이 아닙니다. 왜냐하면
50:23
추가된 하이퍼 파라미터를 훈련시켜야 하는 시간이 걸리기 때문입니다.
50:28
두 번째로, 우리는 여전히 카운트 기반 분포의 문제를 가지고 있습니다.
50:33
구체적으로
50:40
PPM 카운트를 저장하고 SVD를 수행하는 문제로 계산 이슈가 있습니다. 
50:52
따라서 여기서 핵심적인 요지는 이 논문이 전통적인 방법에 도전한다는 것입니다
50:56
실제로 중립 네트워크 기반 모델은 카운트 기반 모델보다 월등합니다.
51:02
두 번째로 모델 디자인이 중요하지만
51:05
하이퍼 패러미터 또한 좋은 결과를 얻기 위한 핵심 요소입니다.
51:09
그래서 이것은 특히 여러분들이
51:13
과제 4번 대신 프로젝트를 수행한다면 의미 있을 것입니다.
51:16
모델을 구현할 수는 있지만 그 방법은 그 중 절반 정도만 수행 할 수 있습니다.
51:21
최적의 하이퍼 파라미터를 찾는 것은 일부 모델에서는 며칠 
51:26
또는 몇 주가 걸릴 수 있습니다.
51:27
그 중요성을 낮추어 보지 마십시오.
51:29
그리고 마지막으로, ML 내에서 내 개인적인 관심은 deep representation learning에 있습니다.
51:35
그래서 이 논문은 특별히 재미있었는데 
51:39
아직 현장에서 해야 할 일이 많이 있음을 알려 주었습니다.
51:44
그리고 마지막으로 도전은 계속된다는 것입니다.
51:49
고맙습니다.
51:50
>> [박수]
51:55
>> 네, 고맙습니다 Ajay.
51:58
이제 뒤로 돌아가서
52:03
전이 기반 의존성 파서를 만드는 방법을 공부하겠습니다.
52:07
어쩌면 103 또는 컴파일러 클래스나 형식 언어 클래스,
52:13
shift reduced parsing이라는 개념이 있습니다.
52:17
몇 명이나 어딘가에서 shift reduced parsing을 보았습니까?
52:21
소수의 사람들입니다.
52:23
전산학에서는 1960 년대에 사용했던 방식으로 형식 언어를
52:28
가르치지 않습니다.
52:29
>> [웃음] >> Jeff Ullman과
52:33
더 많은 시간을 보내야 합니다.
52:34
좋아요, 전 여러분이 전에 보았던 모든 걸 다 가정하지는 않을 겁니다.
52:37
근본적으로 우리가 해야 할 것은,
52:46
이 두 개의 슬라이드를 건너뛰고 곧바로 사진으로 갈 것입니다.
52:51
왜냐하면 그것이 훨씬 더 이해하기 쉽습니다.
52:53
그러나 제가 계속하기 전에 이 페이지에서 그림을 언급 할 것입니다.
52:57
그것은 Joakim Nivre의 사진입니다.
52:59
그래서 Joakim Nivre는 웁살라의 전산 언어학자입니다.
53:03
전이에 기반한 의존성 분석을 개척 한 스웨덴 학자로
53:08
제가 좋아하는 전산 언어학자 중 한 명입니다.
53:11
저는 그가 Ajay가 말한 것과 같이
53:15
인기가 없고
53:19
주류에서 벗어나  있어도 잘 작동할 수 있음을 증명하는 한 예이기도 합니다.
53:23
다른 모든 사람들이 멋진 동적 프로그램을 만들려고 노력했던 시대에
53:28
parsers Joakim은 말하기를 “아니요 저는 그렇게 하지 않고
53:33
연속적인 단어와 분류기로
53:38
다음 단어를 완전히 탐욕적 방법으로 찾을 것입니다. 그것이
53:42
인간이 점진적으로 문장을 처리하는 방식일 것입니다. 그리고
53:45
저는 제가 어떻게 그 일을 얼마나 잘 할 수 있게 될지 알고 싶습니다.”라고 말했고
53:48
그리고 정말로 잘 작동하도록 만들 수 있음이 밝혀졌습니다.
53:51
전환 기반 구문 분석이 이런 종류의
53:56
파싱을 잘 하는 것으로 널리 보급되었습니다.
53:59
결국 뭔가를 하고 싶다면 뭔가 다른 사람들과 다른 것을 찾아내는 것이 좋습니다.
54:05
여러분이 다른 것을 생각하면
54:08
거기서 아이디어를 얻을 수 있을 것입니다.
54:08
그리고 저는 또한 Joakim을 좋아합니다. 왜냐하면 그는 다른 사람과 달리
54:12
실제로 인간의 언어에 관심이 있었고
54:14
실제로 언어학 분야의 소수인 것처럼 여겨지는
54:18
자연어 처리를 한 것입니다.
54:20
좋아요, 여기에 좀 더 형식주의에 대한 설명이 있지만, 지금은 건너뛰고
54:25
아이디어를 제공할 수 있는
54:29
표준 전이 기반 의존성 파서를 할 때 다루겠습니다.
54:36
우리가 분석하고 싶은 문장은
54:41
I ate fish입니다. 파싱을 위해서는 전이 스키마라고 불리는 몇 가지 규칙이 있는데
54:46
너무 작게 쓰여 있어서 읽을 수 있을지 모르겠습니다. 
54:51
이제 시작하겠습니다.
54:53
우리에게는 두 가지가 있는데 스택이 있습니다.
54:56
스택은 그 주위에 회색 반원으로 그렸습니다.
55:00
그리고 우리는 문장을 스택에 두고 구문 분석을 시작합니다.
55:05
한 가지는 여기에 루트 상징이 있습니다.
55:08
좋아요. 스택은 상단 오른쪽으로 향할 것입니다.
55:14
그리고 여기 버퍼라고 불리는 다른 것이 있습니다.
55:18
버퍼는 주황색의 타원으로 그렸고
55:20
버퍼는 우리가 처리해야 하는 문장으로
55:24
상단 왼쪽으로 갈 것입니다.
55:29
왜냐하면 단어들이 벗어나도록 할 것이기 때문입니다. 그렇죠?
55:32
그래서 둘 사이의 상단에 교차점에 있습니다.
55:37
이 전환 기반 파싱을 하기 위해
55:42
우리가 할 수 있는 세 가지 작업이 있습니다.
55:47
세 가지 작업은 Shift, Left-Arc 및 Right-Arc입니다.
55:53
첫 번째 작업은 Shift 작업입니다.
55:57
Shift는 정말 쉽습니다.
55:59
Shift는 버퍼의 상단에 있는 단어를 가져가서
56:04
스택의 맨 위에 놓는 것입니다.
56:07
그 다음에 다시 이동할 수 있습니다.
56:09
버퍼의 맨 위에 있는 단어를 스택의 맨 위로 가져다 놓습니다.
56:14
스택은 맨 위에서 오른쪽으로
56:17
버퍼는 맨 위에서 왼쪽으로 가는 거 기억나죠?
56:20
꽤 쉽습니다, 그렇죠?
56:22
좋아요, 왼쪽 아크와 오른쪽 아크의 아크 표준 전이 스키마에 
56:28
남은 두 가지 다른 작업이 있습니다.
56:33
왼쪽 아크와 오른쪽 아크에
56:38
의존적인 단어를 추가하여
56:42
왼쪽이나 오른쪽 중 하나를 선택해서 붙일 것입니다.
56:45
좋아요, 왼쪽 아크는
56:50
스택에 있고, 즉 맨 위 두 번째에
56:55
스택의 맨 위에 있는 것이 의존되어 있습니다.
57:00
즉, I는 ate에 의존 관계입니다. 그리고 두 번째로 맨 위에 있는 것을 스택에서 제거합니다.
57:06
이것이 왼쪽 아크 연산입니다.
57:09
이제 스택에 [root] ate 이 있는 스택이 되었습니다.
57:13
그리고 우리는 I가 ate와 의존 관계라는 결론을 내렸습니다.
57:18
여기 A가 작은 글씨로 오른쪽에 쓰여 있습니다.
57:22
아직 우리는 fish가 담긴 버퍼를 가지고 있습니다.
57:26
그래서 다음 할 일은 다시 이동하여 fish를 스택에 쌓아 올리는 것입니다.
57:33
이제 우리 버퍼는 비어 있습니다.
57:35
우리는 모든 문장을 스택으로 옮겼고
57:38
root ate fish가 있습니다.
57:41
이제 세 번째 작업은
57:46
오른쪽 아크를 보면 오른쪽 아크는 정확히 왼쪽 아크의 반대입니다.
57:50
오른쪽 아크 연산을 위해서 우리는 스택 맨 위에 있는 것을
57:56
스택에서 두 번째 인 것에 의존적이라고 말합니다.
58:00
우리는 그것을 스택에서 제거하고 아크에 추가합니다.
58:05
이것이 아크로
58:08
fish는 ate에 의존되어 있다고 하면 스택에서 fish를 제거합니다.
58:13
그리고 fish는 ate의존한다는 새로운 의존성을 추가합니다.
58:19
그런 다음 한 번 더 아크를 그립니다.
58:24
이제 ate이 root에 의존합니다.
58:28
그래서 우리는 그것을 스택에서 제거하여 스택에 root만을 남깁니다.
58:33
이제 ate이 root에 의존적이라는 하나의 새로운 의존성을 표시했습니다.
58:38
그래서 이 시점에서 저는 언급 할 것이 있습니다.
58:43
실제로는 버퍼의 몇 가지를 작성하지 않았습니다.
58:47
슬라이드가 꽤 혼잡해졌기 때문입니다.
58:51
하지만 실제로 버퍼는 항상 거기에 있습니다. 맞습니다. 버퍼는
58:55
다시 돌아 왔을 때 사라지는 것이 아닙니다. 항상 있지만 그리지 않은 것입니다.
58:59
마지막 상태에서,
59:01
스택에 하나가 있고, 버퍼에 아무것도 가지고 있지 않습니다.
59:06
문장을 정확하게 파싱하는 것을 끝냈다면
59:08
이것은 우리가 원했던 상태입니다.
59:11
이제 “좋아요, 완성된 상태에서 멈추었습니다.” 라고 말할 수 있습니다.
59:14
거의 다 되었습니다.
59:19
아크 - 표준 변환 기반 파싱으로
59:24
슬라이드 몇 개를 건너뛰고 이전 슬라이드로 돌아가면
59:30
네, 스택과 버퍼가 있습니다. 그리고 우리는
59:35
의존성 아크 (Dependency Arcs) A가 빈 상태로 시작했고 객체를 추가했습니다.
59:41
그리고 일련의 행동을 취해서
59:45
파싱을 했고, 이게 작동하는 방식이었습니다.
59:50
여기에 시작 조건으로, 스택에 ROOT가 있고, 버퍼는 이 문장이며, 아크가 없습니다.
59:56
수행할 작업 세 가지가 있고
60:00
여기에서 정식으로 작성하려고 노력했습니다. 그래서
60:03
세로 막대는 일종의 요소로서 리스트 연산에 추가합니다.
60:09
이것은 wi는 버퍼의 첫 번째 단어를 사용하는 것과 같습니다.
60:16
스택 반대편에는 머리어가 있습니다.
60:20
그리고 우리는 단어를 스택으로 이동시키는 이 시프트 연산을 수행 할 수 있습니다.
60:24
이 두 아크 연산은 새로운 의존성을 추가하고
60:29
스택에서 하나의 단어를 제거합니다. 유일한 종료 조건은
60:34
스택에 있는 것이 루트가 되고 빈 버퍼가 되는 것입니다.
60:39
이것이 일종의 공식 작업입니다.
60:42
그래서 전이 기반 파싱의 개념은 이렇게
60:47
교대로 문장을 파싱하기 위한 움직임의 집합으로
60:52
아크 – 표준 이라고 한 것은
60:55
의존성 집합을 정의 할 수 있는 다른 방법이 있음을 의미합니다.
60:59
그러나 이것은 가장 간단하기 때문에 과제에 사용할 것입니다.
61:02
꽤 잘 작동합니다.
61:04
질문 있나요?
61:06
(질문) 어떻게 알게 되었나요? 앞에 설명한 것으로 가면
61:08
하나만 빼고 모든 것을 말했는데요.
61:11
일련의 가능한 이동 세트를 이야기하고
61:15
언제 이동해야 하는지 말하지 않습니다.
61:18
나머지는
61:22
슬라이드에 있습니다.
61:24
네, 유일하게 남은 것은, 어떤 시점에서든,
61:30
여기 있는 것처럼 어떤 배열을 해야 한다는 것입니다.
61:36
스택에 어떤 것들이 들어 있습니다.
61:40
버퍼에도 어떤 것들은 있습니다.  그리고 이미 만든 아크의 집합이 있습니다.
61:45
그러면 이 작업들 중 어느 것을 다음에 해야 합니까?
61:50
그것이 최종적인 것입니다.
61:52
그리고 이렇게 하기 위해서, Nivre는
61:56
기계 학습 분류기를 만들어야 한다고 제안했습니다.
62:02
우리는 문장들로 구성된 트리뱅크를 가지고 있기 때문에,
62:06
그 구문 분석을 사용하여
62:09
어느 순서의 연산이 문장의 정확한 구문 분석을 제공 할 것인가를 알 수 있습니다.
62:14
저는 지금 그 일을 실제로 처리하지 않을 것입니다.
62:17
그러나 여러분이 문장의 구조를 알 수 있는 트리뱅크가 있다면
62:20
결정적으로 이동과
62:25
축소에 필요한 구문의 순서를 알아낼 수 있을 것입니다.
62:28
각각의 트리 구조는
62:32
왼쪽 아크와 오른쪽 아크로 이동하는 일련의 올바른 구문을 제공해 줄 것입니다.
62:36
그래서 여러분은 트리로 올바른 작동 순서를 읽을 수 있습니다.
62:40
물론 감독 분류(supervised classification) 문제가 있습니다.
62:43
이 시나리오에서 다음에 해야 할 일은 이동해야 한다는 것입니다.
62:48
그래서 그것을 예측하기 위해 분류를 해야 합니다.
62:52
2000 년대 중반에 Nivre와 다른 사람들과 시작된 초기 작업에서,
63:00
이 부분은 종래의 기계 학습 분류기를 통해 이루어졌습니다.
63:05
아마도 SVM, 아마도 퍼셉트론, 일종의 maxent / soft max 분류기 등
63:11
여러 가지로 여러분이 사용할 분류가 있습니다.
63:16
따라서 왼쪽 아크나
63:21
오른쪽 아크로 이동을 결정해야 하니까 여러분은 최대 세 가지 선택을 할 수 있습니다.
63:24
때때로 버퍼에 아무것도 남아 있지 않을 때는 더 적게 선택합니다.
63:28
더 이상 이동할 수 없을 때는 아마도 두 가지 선택만 남을 것입니다.
63:32
그러나 제가 이것을 보여줄 때 언급하지 않은 것은
63:37
아크 세트를 덧붙일 때, 저는 fish가 ate의 목적어라고 말하지 않았습니다.
63:41
저는 의존성이 ate의 목적어라고 말했습니다.
63:45
따라서 의존성 레이블을 포함하려는 경우,
63:49
표준 방법은 
63:54
그 경우는 이 세 가지 선택을 하는 것보다, 왼쪽 아크와 오른쪽 아크의 하위 유형을 포함하는 것입니다.
63:57
약 40 개의 서로 다른 의존성 레이블이 있는 경우.
64:00
우리가 과제 2와 보편적인 의존성(보편문법에서 말하는 의존 트리 구조의 하위 유형)에서 그러 하듯이.
64:04
여러분은 실제로 최종 81가지 분류의 공간이 생깁니다.
64:10
왜냐하면 왼쪽 아크와 같은 이름의 클래스로
64:15
목적어가 있거나 또는 형용사 수식어가 왼쪽 아크에 있을 것이기 때문입니다. 
64:19
과제에서는, 그것을 할 필요가 없습니다.
64:21
과제에서 우리는 타입이 없는 의존성 트리를 하고 있기 때문에
64:25
조금 더 확장 가능하고 쉽게 만들 수 있을 것입니다.
64:28
그래서 여러분이 해야 할 것은 3 가지 방식을 결정하는 것뿐입니다.
64:32
대부분의 실제 응용 프로그램에서는 이러한 의존성 레이블을 갖는 것이 매우 편리합니다.
64:37
여기까지 괜찮나요?
64:38
그러면 자질(features)로는 무엇을 사용합니까?
64:41
전통적인 모델에서는, 주변의 모든 단어를 보았고
64:45
어떤 단어가 스택 맨 위에 있는지 보았습니다.
64:48
단어의 품사는 무엇이고
64:50
버퍼의 첫 번째 단어는 무엇입니까?
64:51
품사는 무엇입니까?
64:53
어쩌면 스택 맨 아래에 있는 것을 보아도 좋습니다.
64:57
어떤 단어와 품사가 있는지를 보고
65:00
그리고 버퍼에서 앞으로 나아가십시오.
65:01
많은 단어를 보고
65:03
품사와 같은 단어의 속성을 보게 될 것입니다.
65:07
그리고 그것은 여러분에게 많은 자질들을 제공합니다.
65:10
고전적인, 범주형의
65:13
전통적인 기계 학습의 희소성의 특징이 있고
65:17
사람들은 이를 통해 분류기를 구축하고 있습니다.
65:20
네 질문인가요?
65:27
(질문) 네, 질문은 “대부분의 트리 뱅크는 품사로 주석을 달았나요” 입니다.
65:32
그리고 대답은 '예'입니다.
65:33
그러니까.
65:35
우리는 지금까지 품사에 관해 거의 이야기하지 않았습니다,
65:38
생물, 명사, 동사 같은 것들.
65:41
따라서 의존 구문 분석을 수행하는 가장 간단한 방법은
65:44
처음에는 품사를 쓰고, 그것에 태그를 붙이고 단어에 품사를 할당합니다.
65:48
그런 다음에 의존 파싱,
65:53
단어의 순서, 품사, 태그 쌍 등으로 통사를 구조화 하고 있습니다.
65:56
다른 작업이 있는데 결합 파싱(joint parsing)으로
66:00
함께 품사 태그를 예측합니다.
66:02
실제로 이점이 있는데, 
66:06
두 가지가 연합되어 있기 때문에 이점이 있습니다.
66:09
결합적 이점을 몇 가지 얻을 수 있습니다.
66:13
Nivre는 탐색할 모델 중 가장 단순한 모델입니다.
66:19
검색이 전혀 없었습니다.
66:21
다음 단어를 듣고 분류기를 실행합니다.
66:23
그리고 그것이 동사의 목적어이면 다음 단어는 무엇입니까?
66:28
네, 그 다음은 명사 수식어입니다.
66:29
이렇게 결정을 내릴 수 있습니다.
66:33
그럼 이제 분명히, 만약 좀 더 조사해 보고
66:37
다른 대안을 찾아보면 조금 더 잘할 수 있다고 생각할 것입니다.
66:39
(질문) 대답은 '예'입니다.
66:41
의존성 분석에는 많은 작업이 있을 수 있습니다.
66:44
다양한 대안을 탐색 할 때 다양한 형태의 빔서치를 사용할 수 있습니다.
66:48
그런데 그렇게 하면 엄청나게 느려집니다.
66:51
반면 결과 면에서 조금 더 좋은 점수를 얻습니다.
66:57
특히 탐욕적 방법으로 끝내거나 작은 빔을 사용할 경우엔 특히 그렇습니다.
67:02
이런 유형의 파싱의 비결은 매우 빠른
67:07
선형 시간 파싱이라는 것입니다.
67:09
아무리 큰 코퍼스라고 하더라도.
67:12
다음 단어는 무엇입니까? 라고 하고
67:14
좋아요, 거기에 붙이세요.
67:15
다음 단어는 무엇입니까?
67:16
거기에 붙이세요.
67:17
그렇게 계속 하면 됩니다.
67:19
사람들은, 남쪽의 교외에 있는 유명한 검색 엔진과 같이,
67:24
웹의 전체 내용을 분석하고 싶어 합니다.
67:26
이럴 때 파서가 매우 빠르기 때문에 파서를 사용합니다.
67:31
오케이.
67:33
탐욕적 방법으로 의존성이 파싱되었고
67:38
정확도는 최상의 의존성 분석보다 약간 낮습니다.
67:44
사실 성능은 거의 비슷합니다.
67:48
그리고 매우 빠르고 확장성이 있다는 사실이
67:51
작은 성능 저하를 보완할 수 있습니다.
67:55
이것은 흥미로운 일입니다.
67:59
좋아요, 그럼 남은 몇 분 동안 저는 이제 뉴럴 네트워크로 돌아갈 것입니다.
68:04
우리는 지금 어디에 있습니까?
68:06
지금 우리는 스택과
68:10
버퍼 및 품사가 있는 배열을 가지고 있습니다.
68:12
그리고 우리가 어떤 구문을 만들기 시작할 때.
68:19
스택에서 빼서 아크를 만들었습니다.
68:22
우리는 우리가 하는 것처럼 트리 구조를 만들기 시작했다고 생각할 수 있습니다.
68:26
제가 아래 예문에서 지적했듯이.
68:29
고전적인 방법으로 그렇게 할 수 있을 것입니다.
68:33
우리는 모든 자질을 갖고 있습니다.
68:35
스택 맨 꼭대기는 단어는 good일 수도 있고 bad나
68:39
또는 easy일 수도 있습니다.
68:41
스택의 품사에 형용사가 있으면
68:43
스택의 단어는 명사일 것입니다.
68:45
그리고 시작하면.
68:47
여러분은 단어 및 품사의 조합을 가지고
68:52
자질의 수를 매우 빨리 찾을 수 있을 것인데,
68:57
모델에는 천만개 정도의
68:59
매우, 매우 큰 순서가 있을 것입니다.
69:01
여러분은 정확히 어떻게 이런 종류의 구문 분석이
69:06
2000 년대 표준화되었는지를 알고 있을 것입니다.
69:08
따라서 희소 자질을 통해 이렇게 거대한 기계 학습 분류기를 구축하고
69:13
그리고 일반적으로 이것이 연결되는 자질도 있어서
69:16
더 잘 예측할 수 있도록 도와 줄 것입니다.
69:18
스택에 두 번째 단어의 자질이 있습니다.
69:22
그리고 태그는 현재 시제 동사입니다.
69:25
스택의 맨 위 단어는 good입니다.
69:27
이런 것들이 하나의 자질이 될 것입니다.
69:28
그렇게 천만 개 이상의 자질을 쉽게 활용할 수 있습니다.
69:33
이렇게 하는 것은 이미 꽤 잘 작동했습니다.
69:37
그러나 다시 시작되는 출발점이 된 것은
69:41
완벽하게 작동하지 않았고
69:46
그것보다 더 잘하고 싶다는 것입니다.
69:48
그리고 우리는 잠시 후에 그렇게 할 것입니다.
69:52
하지만 그 전에 의존성 분석의 평가를 언급해야만 합니다.
69:56
의존성 구문 분석의 평가는 실제로 매우 쉽습니다.
70:00
왜냐하면 각 단어에 대해 무엇에 의존전인지 말할 수 있기 때문입니다.
70:04
각 단어가 무엇에 의존하는지에 대한 선택을 하면
70:09
그것이 올바른 대답이 됩니다.
70:10
트리 뱅크 라는 금괴에서 가져옵니다.
70:14
우리는 일종의, 본질적으로, 우리가 얼마나 자주 옳은지 계산합니다.
70:18
정확도 측정입니다.
70:20
그래서 일반적으로 행해지는 방법에는 두 가지가 있습니다.
70:24
한 가지 방법은 화살표를 보고 레이블을 무시하는 것입니다.
70:29
그리고 그것은 종종 UAS 측정치, 또는 레이블이 없는 정확성이라고도 합니다.
70:34
또는 레이블에 주의를 기울일 수도 있습니다.
70:37
레이블을 제대로 맞췄다면 여러분은 맞다고 말할 수 있습니다.
70:40
그리고 이것을 LAS라고 지칭합니다.
70:44
네?
70:53
(질문) 그래서 질문은, 뭔가 잘못된 것을 얻으면
70:58
다른 모든 것을 파괴하는 폭포 효과가 생기지 않느냐는 것입니다.
71:02
일부는 그렇게 될 수 있습니다.
71:04
그렇습니다. 한 가지 결정하면 다른 결정을 막겠지만
71:10
일반적으로 그렇게 나쁘지는 않습니다.
71:12
왜냐하면 전치사구 같은 것을 못 맞추더라도.
71:16
그 전치사구 안에 있는
71:19
명사구는 여전히 맞추는 결과를 얻을 수 있습니다.
71:21
그래서 별로 나쁘지 않을 것입니다.
71:23
그리고 실제로 의존성 파싱
71:26
평가는 폭포 효과로 인해 크게 나빠지지 않습니다.
71:30
그 점에서 더 나쁜 CFG 파싱을 하는 것보다.
71:35
그다지 나쁘지 않습니다.
71:38
좋아요, 넘어가야 한다고 생각하는 슬라이드가 하나 있어서
71:49
뉴럴 관련된 것 하나를 넘어가겠습니다.
71:54
사람들은 
72:00
이러한 종류의 범주적 기능에 대한 기계 학습 의존성 파서를 상당히 잘 만들 수 있습니다.
72:05
그럼에도 불구하고, 문제가 하나 있습니다.
72:09
그래서, 문제 # 1은 자질이 매우 희박하다는 것입니다.
72:14
만약 여러분이 보통 약 백만 단어의 트리 뱅크가 있고
72:19
천오백만 가지의 특징을 훈련하려고 한다면,
72:24
이들은 다소 다른 구성의 조합이 될 것입니다.
72:27
다분히 이러한 배열을 여러분도 한두 번은 보았을 것입니다.
72:31
그런데, 여러분은
72:35
다른 배열에 대한 정확한 모델을 가지고 있지 않습니다.
72:36
이러한 약한 자질 가중치를 얻는 것뿐이고
72:40
여러분은 손가락을 교차시키고 최고를 희망하게 될 것입니다.
72:42
그런데 사실 현대 기계 학습에서
72:44
손가락을 교차시키는 것이 꽤 잘 작동합니다.
72:46
그러나 그럼에도 불구하고, 희소성으로 인해 많은 고통을 겪고 있습니다.
72:50
두 번째는 불완전성 문제입니다.
72:54
많은 배열의 실행해볼 시간이 있지만
72:58
배열해 보지 못한 다른 배열이 있을 수 있습니다.
73:02
두 번째 단어가 스택에 절묘하게 있다거나
73:05
스택의 최상위 단어, 대화 등에서
73:10
다양한 종류의 단어들, 저는 단지 그들 중 일부분만을 보았습니다.
73:13
많은 것들을 여러분은 자질로 가져가지 못할 것입니다.
73:16
세 번째 것은 조금 놀랍습니다.
73:18
이 심볼릭 의존성 파서를 보았을 때,
73:23
여러분은 무엇이 느리게 만들었는지 물어 봅니다.
73:25
느려진 이유는 SVM이나
73:29
또는 로지스틱 회귀 분석에서 내적이나, 또는 그런 것들을 실행하지 않았기 때문입니다.
73:34
그 모든 것들은 정말로 빠릅니다.
73:37
이 파서들이 자신들의 시간 중 95 %를 소비하면서
73:42
이 자질을 계산하고
73:46
스택과 버퍼 주변을 왔다 갔다 하면서 정리를 해야 합니다.
73:50
자질 이름과 그리고 큰 해시 테이블에서
73:54
자질 번호와 가중치를 찾아야 합니다.
73:56
늘 그 일이 계속되고 있습니다.
73:59
비록 선형 시간이 있어서 그것이 줄였기는 합니다.
74:04
그래서 2014 년 Danqi와
74:08
저는 그 대안을 개발했습니다.
74:12
“이 모든 것을 뉴럴 네크워크 분류기로 바꾸자.
74:16
그렇게 하면 조밀하고 컴팩트한 피쳐 표현과
74:20
분류가 가능할 것이다.” 라고 생각했습니다.
74:22
따라서 천만 가지의 범주형 자질을 보유하는 대신,
74:26
상대적으로 적은 수의 조밀한 자질을 제공해서
74:31
그것을 사용하여 다음 행동을 결정했습니다.
74:33
그리고 저는 마지막 몇 분을 이것이 어떻게 작동하는지 보여 주는데 쓰겠습니다.
74:37
그리고 이것은 기본적 과제 2번입니다.
74:42
헤드라인을 주면, 정말로 잘 작동합니다.
74:48
이것은 첫 번째 파서인 MaltParser의 결과입니다.
74:51
꽤 좋은 UAS와
74:54
LAS, 그리고 정말로 빠르다는 이점이 있었습니다.
74:58
제가 선호하는 방법이라고 말한 것처럼 저는 여러분에게 회색으로 약간의 대조를 주었습니다.
75:02
두 개의 그래프 기반 파서가 있습니다. 
75:05
그래프 기반 파서는 다소 정확하지만
75:09
규모면에서 느린 두 오더와 비슷했습니다.
75:12
그래서, 많은 것을 파싱하지 않아도 정확하기를 원한다면, 이것을 사용할 것입니다.
75:15
그러나 웹을 파싱하려면 아무도 사용하지 마십시오.
75:19
멋진 일은 이렇게
75:23
뉴럴 네크워크 의존성 파서를 사용하여 훨씬 더 정확한 정확도를 얻을 수 있었다는 것입니다.
75:28
우리는 사실상
75:32
그 당시의 그래프 기반 파서 중 최고 수준의 정확성을 얻을 수 있었습니다.
75:35
그리고 우리는 파서를 크게 만들었고 유의미하게
75:39
MaltParser보다 빠릅니다. 왜냐하면 그것은
75:44
자질 조합을 수행하는데 시간을 쓰지 않았기 때문입니다.
75:47
물론 더 많은 벡터 행렬 곱셈을 해야 하지만.
75:49
그것은 별개의 이야기입니다.
75:52
좋아요, 어떻게 그렇게 했습니까?
75:54
우리가 출발점에서 우리가 가진 도구는 두 가지였죠, 그렇죠?
75:57
분산 표현.
75:59
우리는 분산 단어 표현을 사용할 것입니다.
76:03
우리는 유사한 단어들은 벡터에서도 가깝다는 것을 보았습니다.
76:07
또한 우리는 POS 태그 즉, 품사 태그와
76:12
의존성 레이블을 사용할 것입니다.
76:13
우리는 또한 그것들에 대한 분산 표현을 배웠습니다.
76:17
그리고 그것은 멋진 생각이었습니다.
76:19
왜냐하면 품사가 다른 것보다 더 관련이 있는 경우가 있습니다.
76:23
만약 여러분이 품사를 가지고 있다면,
76:27
복수 명사와 명사에서 품사가 다른 고유 명사 
76:31
단수, 그런 것들이 가까이에 있다고 말할 수 있을 것 같습니다.
76:34
그래서, 우리는 그것들에 대한 분산 표현을 했습니다.
76:39
이제 같은 종류의 배열을 쓸 것입니다.
76:43
똑같은 전환 기반의 의존성 파서를 정확히 실행할 것이기 때문에
76:47
배열은 조금도 다르지 않습니다.
76:51
그러나 시작점에서부터 
76:55
특정 위치를 추출할 것입니다.
76:58
Nivre의 MaltParser와 비슷하지만 우리가 할 일은,
77:02
스택의 상단, 스택의 두 번째 상단, 버퍼 등과 같은 각 위치에서
77:08
우리는 배딩 매트릭스에서
77:12
조밀한 표현을 보게 될 것입니다.
77:14
그러니까 여러분은 단어를 일종의 50 또는
77:17
100 차원 단어 벡터 표현을 보게 될 것입니다.
77:23
그리고 우리는 다른 단어에 대한 표현을 벡터로 얻습니다.
77:27
우리가 할 일은 그것들을 하나의 긴 벡터로 연결하는 것입니다.
77:33
따라서 파서의 모든 배열은
77:37
가장 긴 벡터로 표현되지만
77:38
그렇게 길지는 않을 것입니다.
77:39
우리의 ‘벡터는’ 약 천 정도이고 천만 가지가 아닙니다.
77:51
죄송합니다. ‘의존성은’이네요.
77:53
질문은 입력으로 주는 것에서 의존성이 무엇입니까?
77:57
의존성으로 여기서 임포트하는 것은
77:59
이전에 제 아크 세트에 있는 아크로 만들었습니다.
78:05
어쩌면 이러한 아크를 사용하여 다음 결정을 예측하는 데 도움이 될 것이라고 생각했습니다.
78:10
그래서 아크에 대한 이전 결정을 사용하여 후속 결정을 예측하고 있습니다. 
78:17
좋아요, 이제 어떻게 해야 할까요?
78:19
이것은 본질적으로 이제 여러분들이 쌓을 것입니다.
78:24
저의 배열에서 벗어나서
78:28
거기에 임배딩 표현을 하고
78:33
서로 연결시킬 수 있습니다. 저것이 저의 입력 레이어입니다.
78:38
그런 다음 은닉 레이어를 통해 실행한 뉴럴 네트워크입니다,
78:43
피드 포워드 뉴럴 네트워크로 저는 은닉 레이어에서,
78:48
소프트맥스 레이어를 통해 이를 실행했으며 출력 레이어를 얻었습니다.
78:52
이것은 표준 소프트맥스에서의 다른 확률 분포입니다.
78:59
그리고 물론, 저는 이 숫자들 중 어떤 것이 될지 모릅니다.
79:02
그래서, 제가 할 일은 크로스 엔트로피 에러를 사용하는 것입니다.
79:06
그런 다음 학습을 위해 역전파를 이용했습니다.
79:10
그리고 이것은 전체 모델입니다. 그리고 그것은 아주 잘 학습해서,
79:15
훌륭한 의존성 파서를 만들어졌습니다.
79:20
시간이 조금밖에 없어서
79:23
서두르지 않으면 안 될 것이라고 생각하지만 이것은 말해야 할 것 같습니다.
79:27
비선형성, 우리는 비선형성을 언급했지만
79:32
거의 말하지 않았습니다.
79:36
소프트맥스와 같은
79:40
비선형성에 대해서 조금 더 말하고 싶습니다.
79:41
로지스틱 기능을 사용하면 확률 분포를 얻을 수 있습니다.
79:46
그리고 그것은 일반화된 선형 모델과 통계에서 얻을 수 있는 것과 같다고
79:49
일반적으로 그렇게 말합니다.
79:52
뉴럴 네트워크에서.
79:54
비선형성을 가지려면 다음과 같이 
79:59
일부 비선형성을 갖는 다양한 뉴런에 의한 함수 근사를 하여야 합니다.
80:03
잔 파도같은 작은 조각을 함께 넣어서
80:07
기능적 근사를 할 수 있습니다.
80:09
그리고 주의해야 할 중요한 점은 비선형성을 사용해야 한다는 것입니다.
80:15
깊은 네트워크는 레이어 사이에 무언가를 넣지 않으면 쓸모가 없습니다.
80:20
여러 개의 선형 레이어가 있는 경우 하나의 레이어로 축소 할 수 있습니다.
80:24
선형 층은 선형 변환의 결과이고
80:28
아핀 변환은 단지 아핀 변환일 뿐입니다.
80:32
비선형성이 없는 깊은 네트워크는 아무 것도 하지 않습니다.
80:35
그래서 우리는 로지스틱 비선형성에 관해 이야기했습니다.
80:40
두 번째로 일반적으로 사용되는 비선형성은 tanh 비선형성이며,
80:45
tanh는 일반적으로 조금 다르게 작동됩니다.
80:50
그러나 만약 여러분이 약간만 수학을 한다면, tanh는 정말로
80:55
logistic와 같아서, 조금씩 확장하면서 움직입니다.
81:01
그리고 tanh는 영에서 대칭이라는 장점이 있습니다.
81:07
따라서
81:11
새로운 뉴럴 네크워크의 중간에 배치하는 경우 종종 더 효과적입니다.
81:12
그러나 앞에서 본 예에서,
81:15
여러분들이 의존성 파서를 위해 사용할 것들은,
81:19
첫 번째 레이어에 사용할 수 있는 것은 선형 rectifier 레이어입니다.
81:25
그리고 선형 rectifier 비선형은 이상한 것이고
81:29
흥미로운 곡선이 아닙니다.
81:32
선형 rectifier는 음수인 경우 0으로 매핑하고
81:36
양수이면 선형으로 매핑합니다.
81:39
그리고 이것들이 처음 소개되었을 때, 저는 미쳤다고 생각했습니다.
81:43
정말로 효과가 있다고 믿을 수 없었고, 유용한 어떤 것도 할 수 없었습니다.
81:47
그러나 이제 훌륭하게 성공할 수 있는 것으로 밝혀졌습니다.
81:50
뉴럴 네트워크의 가운데서, 요즘 종종 여러분이 시도하고 있는 것 중에
81:55
가장 잘 작동하는 것은 rectifier linear unit인 ReLU입니다.
82:02
이것이 가지고 있는 효과적인 속성은
82:06
기울기가 단지 1이라는 것입니다. 긍정적인 면으로 보았을 떄
82:10
역전파하는 단계에서 오류를 전송한다는 의미입니다.
82:15
네트워크를 통해 실제로 선형적으로 역으로 전파합니다.
82:19
그리고 음수가 되면 비선형성이 충분해져서
82:23
특정 배열에서 사라집니다.
82:26
그래서 비선형성은 이제 막 매우 성공적인 결과를 내놓았습니다.
82:30
그리고 이것이 우리가 의존 구문 분석기에서 사용하는 것이 좋은 이유입니다.
82:36
이제 끝내야 합니다.
82:39
그러나 이러한 종류의 뉴럴 네트워크를 전환 기반 파서로 만드는 것은
82:44
훌륭한 성공적인 아이디어였습니다.
82:47
Parsey McParseface에 대한 Google의 발표에 대해 알고 계신 분도 있을 것입니다.
82:52
그리고 SyntaxNet은 오픈 소스인 의존성 파서의 일종입니다.
82:57
근본적인 아이디어는 똑같고
83:00
더 큰 규모와 더 잘된 뉴럴 네트워크 최적화로 끝내기 만하면 됩니다.
83:04
감사합니다.