00:00
[음악]
00:04
스탠포드 대학교.
00:08
오늘은 점점 현실로 되어 가는 것들에 대해서
00:10
약간의 오버뷰와
00:15
분류의 배경 지식을 배울 것입니다.
00:20
그 다음에, 지금까지 비감독(unsupervise)방식으로 배운 워드 벡터들을 업데이트하면서
00:24
흥미로운 것들을 할 것입니다.
00:26
감성과 같은 supervision signal과
00:29
다른 것들도 업데이트할 것입니다.
00:30
그런 다음 실제로 유용한 첫 번째 모델을 살펴 볼 텐데
00:33
실습에 적용할 수도 있을 것입니다.
00:35
그리고 워드 벡터로, 윈도우 분류의 한 종류인 downstream 과제를 해 보고
00:39
매우 혼란스러운 부분들을 정리해 보도록 하겠습니다.
00:44
cross entropy error 및 softmax와의 연결 방법을 중심으로
00:48
유명한 신경망(neural network)의 가장 기본적인 레고 블록을 소개할 것입니다.
00:54
그리고 이 수업의 제목에 걸 맞는 딥러닝을 시작할 것입니다.
01:00
자연어처리에서의 딥러닝과
01:01
여러 손실 함수들, 그리고 max margin loss와
01:05
역전파의 첫 걸음을 뗄 것입니다.
01:09
이 클래스를 잘 따라가기 위해서는 먼저, 문제 세트가 많은 도움이 될 것이라고 생각합니다.
01:14
수학이 절실히 필요할 텐데 그래서,
01:17
두 번째, 문제 세트가 필요할 것입니다.
01:18
여러분들에게 도움이 되기를 기대하고 있고 이를 통해서 학기가 끝날 때쯤에, 
01:23
딥러닝의 마술에 관해 더 잘 이해하게 되기를 바랍니다.
01:30
좋아요, 클래스 진행과 관련된 질문이 있나요?
01:34
문제 세트나 프로그래밍 세션에 대해서?
01:40
네 그럼 모두 괜찮죠?
01:41
TA분들, 모든 것을 잘 정리해 주셔서 감사합니다.
01:44
오늘은 표기법에 매우 주의를 기울여야 하는데 왜냐하면,
01:50
더 진도를 나가기 전에  
01:52
매우 복잡한 연쇄 규칙 등을 할 것이기 때문입니다.
01:55
그럼, 처음부터 이야기해 봅시다.
01:58
우리는 보통 입력 X와 출력 Y의 훈련 데이터 세트를 가지고 있게 될 것입니다.
02:02
X는 가장 단순한 경우로, 한 개의 단어일 수 도 있으며 단일 워드 벡터일 수 있습니다.
02:07
실제로 이렇게 하는 경우가 자주 있는 것은 아니지만
02:10
이 방법으로 배우는 것은 좀 더 쉬울 것입니다.
02:13
시작하기 전에 잠깐 컨텍스트 원도우로 이동해 봅시다.
02:15
그 다음에 결국 우리가 소개하는 것과 동일한 기본 빌딩 블록을 사용할 것입니다.
02:20
오늘 문장과 문서는 모두 복잡한 상호 작용이 있지만
02:26
지금 출력은 가장 간단한 하나의 레이블일 뿐입니다.
02:30
그리고 그것은 단지 긍정적이거나 부정적인 문장입니다.
02:33
컨텍스트에서의 특정 단어나 개체명(named entitie)일 수도 있고
02:37
기계 번역에서라면 다른 단어일 수도 있습니다.
02:41
우리는 여기를  yi처럼 다른 단어의 연쇄로 출력하고 싶고,
02:44
2 주 안에 ​​결과를 보고 싶다고 합니다.
02:46
그리고 여러 단어(multiword)의 연쇄로 잠재적인 결과물을 예상하고 ​​있다면
02:50
여기서 여러분의 분류에 대한 직관은 무엇입니까?
02:54
아직 딥러닝의 세계는 아니지만, 표준 기계 학습 사례인
02:58
보통의 단순한 로지스틱 회귀에서
03:02
우리가 기본이라고 말하는 간단한 결정 경계를 정의하여 봅시다.
03:07
이것의 왼쪽의 방향의 모든 것이 하나의 클래스에 속한다고 볼 것이고
03:12
다른 방향은 다른 클래스에 있으므로 속한다고 볼 것입니다.
03:14
그래서 일반적인 기계 학습에서 우리는 입력인
03:19
X는 다소 고정되어 있다고 가정하고
03:23
softmax 가중치인 W 파라미터만 훈련할 것입니다.
03:28
즉, 어떤 종류의 입력 X가 주어지면 Y의 확률을 계산하는 것입니다.
03:34
그리고 여기에 전체 데이터 세트에 아래 첨자 I 가 달려 있습니다.
03:37
여기서 i를 떨어 뜨리고
03:42
x와 y의 예를 볼 것입니다.
03:49
결국, 첨자를 오버로드 해서,
03:52
특정 벡터의 색인을 보겠지만 헷갈리는 사람은
03:56
손을 들어서 질문하세요.
03:57
어느 것이 어느 쪽에 속하는지 분명히 하려고 노력하겠습니다.
04:01
이제 softmax로 들어가 봅시다.
04:04
이전에 언급했지만, 주의 깊게 정의하고 기억해야 합니다.
04:09
여기에서 우리는 모든 파라미터에 도함수를 취하겠습니다.
04:14
x가 주어진 y의 확률을 계산하기 위해 두 단계를 따로 구분할 수 있을 것입니다.
4:21
우선, y 번째 행의 W를 취해 그 행에 x를 곱합니다.
04:25
그리고 여기 또 다시 이 표기법이 쓸 것인데 여기  Wy가 있습니다.
04:31
그 의미는  우리가 이 행렬의 y 번째 행을 취하고 있다는 것입니다.
04:37
그리고 여기에 x를 곱하면 됩니다.
04:42
하저씩 우리 클래스에 있는 모든  c에 이 곱을 계속하면, 
04:48
이렇게 말하자면 1, 2, 3, 4 번째 줄에 각각 곱합니다.
04:53
그러면 여기에 네 개의 숫자가 생길 것입니다.
04:55
정규화 되지 않은 점수입니다.
04:59
그런 다음에, 우리는 기본적으로 이 벡터를 softmax 파이프를 통하면
05:03
1이 되는  확률 분포를 계산할 수 있을 것입니다.
05:08
좋아요 여기서부터 시작하겠습니다.
05:11
질문 있나요?
05:13
없으면 계속 하겠습니다.
05:17
네, 네
05:19
이전의 했던 일종의 설문 조사에 따르면 일반적으로.
05:24
클래스의 15 %가 보통 지루해 한다고 합니다.
05:29
우리가 이 모든 것들을 통과하고 나서, 이 미분들처럼.
05:32
15 %가 힘들어하고 대부분의 사람들은 좋다. 괜찮다.
05:36
적당한 속도이다, 꽤 많이 배웠고, 이해했고, 발전했다고 느낀다고 합니다.
05:39
반면, 유감스럽지만 30 %의 사람들은 너무 느리다고 느끼거나 너무 빠르다고 느낀다고 합니다.
05:44
강의 슬라이드를 보거나
05:47
온라인으로 시청하는 경우 속도를 높일 수 있을 것입니다.
05:50
여러분이 이 꽤 복잡한 미분에 익숙하거나
05:53
조금이라도 힘들다고 느끼면 근무 시간을 이용하여 와 주세요.
05:56
우리를 도울 훌륭한 TA들이  있습니다.
05:59
좋아요, 이제 우리가 예측하기를 원하는
06:04
x와 y의 예를 봅시다.
06:07
일반적으로, 우리는
06:12
클래스의 확률이 극대화하기를 원합니다.
06:12
출력의 argmax를 취함으로써 결국 적절한 클래스가 출력되기를 원하는 것입니다.
06:19
확률을 극대화하는 것은 로그 확률을 최대화하는 것과 동일하고
06:23
로그 확률의 음수를 최소화하는 것과 같은데
06:27
이것이 종종 목적 함수가 됩니다.
06:29
그러면 이것을 왜 크로스 엔트로피 에러 라고 부릅니까?
06:33
크로스 엔트로피를 일반화하고 추상화하면 다음과 같이 정의할 수 있을 것입니다.
06:39
진리처럼 가정하는
06:42
목표 확률 분포에서, 우리가 이 세 용어를 서로 바꾸어 쓸 수 있습니다.
06:47
훈련 데이터 세트에서 이상적인 목표인 y가 있고
06:51
올바른 클래스와 여기저기 0이 있다고 가정해 봅니다.
06:56
예를 들어 여기에 다섯 개의 클래스가 있고 이것이 중심 클래스입니다.
07:01
그것은 세 번째 클래스의 하나가 되고 나머지는 0이 될 것입니다.
07:05
따라서 계산 된 확률에서 이것을 p로 정의하고
07:09
softmax가 q로 출력된다면 모든 클래스에 대한 합으로
07:13
크로스 엔트로피를 정의 할 것입니다
07:17
그리고 이 경우에 p는 ​단지 원 핫 벡터로
07:21
한 위치만 1이고 다른 곳에서는 전부 0입니다.
07:24
다른 용어는 모두 없어지고
07:28
단지 q의 로그로 끝나는데
07:30
이것이 바로 softmax 출력물의 로그입니다.
07:34
이것은 Kullback-Leibler divergence 분포와 연관성이 있습니다.
07:39
이전에는 그것에 대해 이야기하곤 했었지만 오늘 우리에게는 그렇게 많은 시간이 없습니다.
07:42
그래서 통계에 익숙하다면,
07:45
이것은 Kullback-Leibler divergence를 최소화하려는 것으로 볼 수 있습니다.
07:49
둘의 분산은
07:50
실제로 이 클래스의 목적을 위해 알아야 할 모든 것입니다.
07:55
그리고 이것은 훈련 데이터 세트의 한 요소입니다.
07:59
물론 일반적으로 많은 훈련 예시가 있습니다.
08:03
그리고 종종 J로 나타내는 목적 함수와
08:06
파라미터 theta가 있습니다.
08:09
올바른 클래스에 음의 로그 확률을 합산하고
08:14
여기 색인이 있고 하위 색인 yi가 있습니다.
08:18
그리고 이 총계를 최소화하고 싶습니다.
08:23
이것이 우리가 최소화하려는 크로스 엔트로피 오류입니다.
08:25
우리는 앞으로 몇 시간 내에 미분을 많이 다룰 것입니다.
08:32
지금까지 질문 있나요?
08:35
이것은 일반적인 ML을 가정한 것으로 입력 된 내용이 고정되어 있다고 가정합니다. 
08:43
(질문) 예, 단일 번호입니다.
08:51
여기에 벡터를 곱하지 않으므로 p(c)는 해당 클래스의 확률입니다.
08:56
그래서 이것은 하나의 단일 번호입니다.
08:58
좋은 질문입니다.
09:01
크로스 엔트로피는 단일 숫자로 우리의 주된 목적은
09:05
최소화하려는 오차로 나타납니다.
09:09
자, 이 F 첨자 Y를 쓸 때마다,
09:13
우리는 F가 실제로 함수 X, 즉 입력 값이라는 것을 잊지 않아야 합니다.
09:18
그것은 일종의 중간 단계이며, 
09:20
이 표기법을 다룰 때 매우 중요합니다.
09:24
Wy도 다시 쓸 수 있는데, 그 열을
09:29
x로 곱해서, 그 전체 합계를 쓸 수 있습니다.
09:31
그리고 이것은 한 번에 한 요소의 미분으로 
09:35
전체 행 표기법의 더 큰 그림을 보려고 할 때 종종 도움이 될 수 있습니다.
09:43
그래서 자주 이 행렬 표기법으로 f를 써 보겠습니다.
09:46
이것이 f이고 이것이 w, 이것은 x로
09:49
벡터를 이용한 표준 행렬 곱입니다.
09:55
이제 대부분의 시간에 우리는 이 목적 함수의 첫 부분에 대해 이야기 할 것이다.
09:59
그렇지만 조금 단순화 된 것으로
10:02
왜냐하면 모든 실제 응용 프로그램에는 여기 이 정규화 된 용어들도 있기 때문입니다.
10:07
전체 목적 함수의 일부로
10:10
그리고 많은 경우에, 예를 들어 여기의 theta가 있고
10:13
이것이 표준 로지스틱 회귀의 W 행렬이라면,
10:17
목적 함수의 이 부분을 시도 할 것입니다.
10:21
우리는 모델 이 모든 가중치를 가능한 한 작게 유지하여 
10:25
이상적으로는 가능한 한 0에 가깝기를 권장할 것입니다.
10:28
베이지안의 사전확률로,
10:33
Gaussian 분포의 사전확률을 쓰면 이상적으로는 모두 작은 수가 될 것입니다.
10:38
가끔은 정규화 term을 쓰려고 해도
10:40
숫자는 날아가서 점점 더 오버피팅 되기 시작할 것입니다.
10:43
사실, 이런 종류의 줄거리는
10:47
여러분의 프로젝트 및 심지어 문제 세트에서도 자주 보게 될 것입니다.
10:50
첫 번째 통계 학습 클래스를 들었을 때, 교수님께서
10:55
이것은 기억해야 할 첫 번째 플롯이라고 했었습니다.
10:56
그렇게 중요한지는 모르겠지만
10:58
모든 애플리케이션에는 매우 중요합니다.
11:01
그리고 이것은 기본적으로 꽤 추상적인 플롯입니다.
11:03
x 축을 다양한 것으로 생각할 수 있습니다.
11:07
예를 들어 모델이 얼마나 강력한 지,
11:09
몇 개의 깊은 레이어를 갖게 될지, 얼마나 많은 파라미터를 가질 지를 결정할 수 있습니다.
11:12
또는 각 워드 벡터가 갖는 차원의 수나
11:15
또는 모델 훈련 기간처럼
11:18
서로 다른 많은 x 축과 같은 패턴의 패턴을 볼 수 있습니다.
11:22
y 축은 본질적으로 오류거나
11:26
최적화하고 최소화하려는 목적함수일 것입니다.
11:30
자주 관찰할 수 있는 것은, 모델은 더 강력해질수록
11시 33 분
훈련 오류를 낮아지고
11:39
x-i, y-i 쌍에 더 잘 맞을 수 있습니다.
11:42
그러나 어느 시점에서 오버피팅 되기 시작하면 테스트 오류가 발생하거나
11:47
검증 또는 개발 세트 오류가 다시 발생합니다.
11:50
그 모든 것을 피하는 방법과
11:55
이 과정과 프로젝트 조언 등등에 대해 좀 더 자세히 설명하겠습니다.
11:57
하지만 이것은 매우 기본적인 것이고
12:02
구현 및 프로젝트에서는 정규화 파라미터를 원할 것입니다.
12:07
그러나 이것은 거의 모든 목적 함수와 같은 것이어서
12:10
그냥 넘기고 주로 데이터 세트에 초점을 맞출 것입니다.
12:15
좋아요, 정규화에 대한 질문이 있습니까?
12:26
(질문) 그래서 기본적으로, 우리는
12:31
하나의 특정 번호를 신경 쓰며
12:36
모든 파라미터를 조정하면서 정확한 다른 지점으로 이동할 것입니다.
12:42
그리고 그렇게 하지 않으면 조금 더 부드럽게 될 것입니다.
12:47
그러면 그 점에 정확하게 맞출 가능성이 적어지고
12:50
따라서 종종 더 잘 일반화됩니다.
12:53
그리고 곧 이것이 어떻게 생겼는지 몇 가지 예를 들어 살펴보겠습니다.
12:59
언급한 것처럼 기계 학습에서
13:03
Softmax 분류기의 파라미터인 W 만 최적화 할 것입니다.
13:10
따라서  업데이트와 경사도는 매우 작습니다.
13:13
그래서 많은 경우에 우리는 작은 클래스로
13:16
어쩌면  워드 벡터가 100 개 정도로 3 개의 클래스와 100 개의 차원의
13:21
워드 벡터로 분류하려고 하고 있고 300 개의 파라미터만 있다고 생각해 봅시다.
13:27
이제 딥러닝에는 이 놀라운 워드 벡터가 있습니다.
13:31
우리는 실제로 Softmax뿐만 아니라
13:35
워드 벡터를 학습시키려고 하고 있습니다.
13:37
그래서 오늘은 역전파에 대해서 이야기 할 것입니다.
13:41
힌트는 미분을 사용한다는 것입니다.
13:44
문제는 개념적으로 워드 벡터를 업데이트 할 때로
13:47
이것이 매우 크다는 것을 깨닫게 됩니다.
13:51
그리고 갑자기 굉장히 많은 파라미터가 생깁니다. 맞습니까?
13:54
워드 벡터가 300 차원이라고 가정 해 봅시다.
13:57
그리고 10,000 단어가 있습니다.
14:00
갑자기 많은 파라미터 집합이 생깁니다.
14:05
그리고 이런 종류의 플롯은 오버피팅 될 가능성이 높습니다.
14:10
그래서 최적화 작업에 뛰어 들기 전에,
14:14
워드 벡터를 업데이트하는 것이 무엇을 의미하는지에 대한 약간의 직감을 갖기 위해
14:17
아주 간단한 예제를 살펴보겠습니다.
14:20
우리는 각 단어들을 분류하기를 원한다고 할 때,
14:22
다시 말하지만, 자주 있는 일은 아닙니다.
14:25
단일 단어를 긍정 또는 부정으로 분류하려고 한다고 가정 해 봅시다.
14:29
훈련 데이터 세트에 TV와 telly가 있고
14:33
이 영화 리뷰가 TV에 더 적합하다고 말할 수 있습니다.
14:36
그런데 이제 막막 영화관에서 상영하기 시작한 영화에 대해서는
14:40
매우 긍정적인 것은 아닙니다.
14:41
처음에는 telly, TV,
14:44
그리고 television이 사실 벡터 공간에서 모두 가까이 있었습니다.
14:49
우리는 word2vec 또는  glove 벡터로 무언가를 배우고 이 단어를 훈련시킵니다.
14:54
매우 큰 말뭉치에 벡터가 있고 이 세 단어가 모두 나타나기 때문에
14:57
거의 비슷한 컨텍스트에 있기 때문에, 벡터 공간에서 가깝습니다.
15:01
이제 우리는 훈련할 감성 데이터 세트는 매우 작기 때문에
15:07
훈련 세트에는 X-i Y-i가 television이 아닌 TV나 telly만 포함됩니다.
15:14
이제 이 워드 벡터를 훈련 할 때 어떤 일이 발생합니까?
15:17
일단, 움직이기 시작할 것이다.
15:19
우리는 그들에게 감성을 투사 할 것이고, 그래서 우리 이제  British데이터 세트에서
15:24
telly와 TV를 다른 곳으로 이동하여 벡터 공간으로 옮길 수 있습니다.
15:29
그러나 TV는 실제로 처음부터 그 위치에 있었고
15:33
지금 테스트할 때도
15:36
결코 움직이지 않았기 때문에 실제로 이 단어를 잘못 분류 할 것입니다.
15:41
이게 무슨 뜻일까요?
15:42
여기에 있는 메시지는
15:45
아주 작은 훈련 데이터 세트만 있다면.
15:48
이렇게 하면 특히 딥러닝 모델을 사용하면 매우 빠르게 오버피팅 될 수 있습니다.
15:52
우리는 이렇게 워드 벡터를 훈련시키고 싶지 않고
15:55
고정시키고 싶을 것입니다, glove 또는 word2vec 모델처럼
15:59
매우 큰 말뭉치 또는 
16:01
방금 클라우드 웹 사이트에서 다운로드 한 파일로 고정시키고 싶을 것입니다.
16:05
그렇지 않으면 일반화되지 않으니까요.
16:08
그러나 매우 큰 데이터 세트로 훈련시키는 경우는
16:13
다음 슬라이드에서 설명 할 것입니다만,
16:16
예를 들어,
16:18
기계 번역에서는 수백 메가 바이트 또는 기가 바이트의
16:23
훈련 데이터가 있을 것입니다. 그렇지만 무작위로 초기화해서 overall objective로 훈련시키는 것이 아니라면 
16:28
단어 벡터로는 그렇게 많을 필요가 없습니다.
16:30
이해되나요?
16:32
워드 벡터의 일반화에 관해 질문 있습니까?
16:41
(질문) 좋아요, 아직 우리가 이걸 어떻게 훈련하는지 모르겠을 거예요.
16:45
지금 설명하겠습니다.
16:48
우리는 단어 하나로는 거의 분류하지 않고
16:51
정말로 우리가 하고 싶은 것은 컨텍스트 안에서 단어를 분류하는 것입니다.
16:55
재미있고 흥미로운 것들이 많습니다.
16:58
컨텍스트는 언어가 시작되고
17:01
문법과 의미가 연결되는 곳입니다.
17:05
그래서 여기서, 문맥이 정말로 필요한 곳의 몇 가지 재미있는 예가 있습니다.
17:09
그래서 예를 들어, 우리는 실제로 자동 반향 문자라고 하는 단어가 있습니다. 그래서
17:13
각 단어는 그 자신의 반대 의미도 있습니다.
17:14
예를 들어 sanction은 허용하다(permit)와 벌하다(punish)를 의미 할 수 있습니다.
17:18
이 때 그 실제 의미는 이해하려는 맥락에 달려 있습니다.
17:23
seed는 씨앗을 뿌리거나 씨를 제거하는 것을 의미 할 수 있습니다.
17:26
문맥이 없으면, 우리는 이 단어들의 의미를 전혀 이해하지 못할 것입니다.
17:31
개체명 인식이라는 이름의 예제가 많이 보일 것입니다.
17:35
우리가 위치나 사람 이름을 찾고 싶다고 가정 해 봅시다.
17:39
우리는 이것이 위치인지 아닌지 식별하고 싶습니다.
17:41
프랑스 파리, 파리 힐튼과 같은 파리도 있습니다.
17:46
그리고 여러분은 파리를 파리에 머물려면
17:48
뭐가 뭔지 이해하고 싶을 것입니다.
17:51
또는 금융 거래를 위해 딥러닝 쓸 때 Hathaway를 보게 되면,
17:55
긍정적인 영화 평론에서의 앤 해서웨이 (Anne Hathaway) 확인하고 싶을 것입니다.
17:59
버크셔 해서웨이(Berkshire Hathaway)로부터 갑자기 주식을 사지는 않겠죠?
18:03
그래서 즐겁고 재미있는 많은 이슈가 있습니다.
18:05
재미있고 복잡한 것들이 컨텍스트에서 발생합니다.
18:08
이제는 이 유용한 모델을 좀 더 주의 깊게 살펴 보겠습니다.
18:13
윈도우 분류입니다.
18:15
첫 번째 동기 부여 예제를 사용할 것입니다.
18:20
커다란 코퍼스에서 4개의 클래스의 개체명 인식으로 사람이나 위치
18:24
조직 또는 위의 것이 모두 없음을 식별하고 싶습니다.
18:30
다양한 가능성이 존재합니다.
18:33
하지만 기본적으로 다음 모델을 살펴볼 것입니다.
18:35
실제로 꽤 합리적인 모델입니다.
18:37
또한 2008 년에 시작된 것도 있는데,
18:39
Collobert와 Weston의 훌륭한 논문에서 시작된,
18:44
텍스트 분류 및
18:48
단어 분류 컨텍스트입니다.
18:50
우리가 여기서 하고 싶은 것은 softmax 분류기를 훈련시켜서 중심 워드에 레이블링을 하고 18:55
그 단어 주위의 원도우에 있는 모든 단어를 연결하는 것입니다.
19:01
여기에 더 긴 문장에서 생기는 하위 구문을 예로 들어 봅시다.
19:07
여기 있는 파리를 중심 단어로 분류하고 싶습니다.
19:11
이 컨텍스트에서
19:13
원도우 길이를 2로 정의합니다.
19:15
왼쪽으로 2 단어이고
19:17
오른쪽에 2 단어가 우리가 분류하려고 하는 현재 중심 단어 옆에 있습니다.
19:23
자, 이제 우리는 새로운 x를 정의 할 것입니다.
19:27
전체 원도우를 다 연결하면 다섯 개의 워드 벡터로
19:35
일반적으로
19:38
제 모든 강의에서 이 모든 벡터는 열벡터가 될 것입니다.
19:43
아쉽게도 문제 세트 중 두 번째는 행벡터로 되어 있어서
19:46
미안합니다.
19:49
결국, 이 모든 프로그래밍 프레임 워크는 실제로 행 방향 (row-wise)이니까
19:54
행벡터를 사용하는 저수준 최적화가 더 빠를 것입니다.
19:59
계산할 것이 많을 때는 실제로
20:02
열벡터로 생각하는 것이 더 간단하기도 합니다.
20:03
문제의 집합에서는 매우 명확하지만 그 일에 집중하지 않을 것입니다.
20:07
여기에서는 하나의 5D 차원 열벡터로 정의 할 것이고 그래서
20:13
여기 T 차원의 워드 벡터가 있습니다.
20:16
우리는 그것을 하나의 열에 쌓아 두었습니다.
20:20
이제 우리가 생각할 수 있는 가장 간단한 윈도우 분류기는
20:25
이 다섯 개의 워드 벡터를 연결해서 상단에 있는 softmax에 넣는 것입니다.
20:30
이것을 x로 정의 할 것입니다.
20:33
입력은 이 연결에 대한 전체 원도우 x에 불과합니다.
20:37
그 위에 softmax를 가지고 있습니다.
20:39
그리고 이것은 이전에 사용한 표기법과 같습니다.
20:42
슬프게도 
20:47
올바른 현재 클래스에 아래 첨자 y로 y hat을 소개합니다.
20:51
힘들어요, 몇 번 반복했는데[웃음] 
20:54
전체 강좌를 통해 항상 작동하는 표기법을 찾기가 어렵습니다.
20:57
하지만 곧 알 수 있을 것입니다.
21:00
전반적인 objective가 여기 있고, 다시 말하면,
21:04
우리가 가진 확률, 또는 네거티브 로그가 여기 있습니다.
21시 10 분
자 이제 질문은, 어떻게 우리가 이 워드 벡터 x를 어떻게 갱신할 것인가 입니다.
21:13
x는 원도우이고, x는 이제 softmax 내부 깊은 곳에 있습니다.
21:18
짧은 대답은 우리가 미분을 많이 할 것이라는 것이고
21:21
긴 대답은 문제가 많은 부분에서 그렇게 해야 한다는 것입니다.
21:25
어쩌면 중간고사일 수도 있습니다.
21:26
자, 조금 더 도움이 될 수 있도록 몇 가지 단계를 거치고
21:31
나서 힌트를 드리겠습니다.
21:32
그래서 이 중 일부는 실제로 문제 세트에서 해야 할 것입니다.
21:35
저는 모든 세부 사항을 거치지 않을 것입니다.
21:37
하지만 길을 따라 몇 가지 힌트를 줘서
21:42
정답을 맞추고 올바른 길을 가고 있는지 알도록 하겠습니다.
21:46
1 단계는, 늘 변수와
21:50
그들의 차원과 모든 것을 매우 신중하게 정의하라는 것입니다.
21:51
따라서 y hat은 벡터의 softmax 확률로 정의합니다.
21:56
확률의 정규화 된 점수 또는
22:00
우리가 가진 모든 다른 클래스의 확률이 될 것입니다.
22:02
우리의 경우에는 4 가지가 있습니다.
22:05
그 다음에 목표 분포가 있습니다.
22:07
다시 말하지만, 클래스 y의 인덱스에서는 1이니까 제외하고
22:11
원 핫 벡터가 될 것입니다.
22:15
그리고 f는 f(x)로 다시 정의 할 것입니다.
22:18
이것은 이 행렬의 곱셈입니다.
22:20
여기서 대문자 C는 C 차원 벡터로
22:24
우리 클래스의 수입니다.
22:28
여기까지 1 단계였습니다.
22:29
모든 변수를 신중하게 정의하고 차원을 잘 따라가기 바랍니다.
22:32
이것을 구현하고 두 가지를 곱하면 매우 쉽습니다.
22:36
그들은 틀린 차원을 가지고 있고, 실제로 합법적으로 증식시킬 수는 없습니다.
22:40
우리는 버그가 있다는 것도 알고 있습니다.
22:41
그리고 많은 방정식에서도 이것을 할 수 있습니다.
22:43
중간 고시에서 놀라고
22:44
긴장하겠지만
22:46
결국 약간의 시간이 있습니다.
22:48
그리고 첫 번째 단계에서는 혼자서 완전히 점수를 매길 수도 있습니다.
22:51
행렬의 모든 차원과
22:54
벡터 곱셈은 정확한지 확인하기를 바랍니다.
22:58
좋아요, 두 번째 팁은 체인 규칙입니다. 이전에 살펴보았지만
23:01
상담 시간에 아직 좀 혼란스러워 한다고 들었습니다.
23:04
자, 이것을 간단한 예제를 통해 주의 깊게 정의해 봅시다.
23:09
좀 더 복잡한 예를 들어 몇 가지 힌트를 줄 수도 있습니다.
23:12
다시 한 번 말하지만, 매우 간단한 것으로 가령, 함수 y는
23:15
f(u)로 정의할 수 있고 u는 g(x) 정의 할 수 있습니다.
23:20
전체 함수에서 y는 f(g(x)) 표현 될 수 있으며,
23:25
기본적으로 dy의 곱으로 u 곱하기 udx를 곱입니다.
23:30
그리고 매우 구체적으로, 
23:34
체인 규칙을 표시하기 위해 일종의 고등학교 수준으로 정의 한 것이 있습니다.
23:39
여기에서, u를 g(x)로 정의 할 수 있습니다.
23:42
여기 괄호 안에 있는 것입니다.
23:46
y는 f(u)의 함수를 가질 수 있습니다.
23:49
여기서 우리는 5u를 사용합니다. 여기서 내부 정의를 바꿉니다.
23:54
매우 간단합니다. 그냥 교체하는 것입니다.
23:57
이제 우리는 u와 관련하여 미분을 취할 수 있습니다.
24:00
우리는 x(u)에 대해 미분을 취할 수 있습니다.
24:04
그런 다음에 이 두 term을 곱하고 u를 다시 연결합니다.
24:08
이런 의미에서 우리 모두는 이론상으로는 체인 규칙을 알고 있습니다.
24:12
하지만 이제 우리는 softmax를 배울 것이고
24:14
많은 행렬을 알게 될 것입니다.
24:16
따라서 우리는 표기법에  매우 신중해야 합니다.
24:20
또한 우리는 이해하는 것에 대해 신중하게 생각해야 하는데,
24:22
이 안쪽의 파라미터는 다른 상위 수준 요소입니다.
24:28
그래서, f는 예를 들어 x의 함수입니다.
24:30
x에 대해 미분을 취하려고 한다면,
24:34
이 전체적인 softmax 중 x가 나타나는
24:38
다른 클래스 모두를 합쳐야 합니다.
24:41
첫 번째 애플리케이션은 여기에서 볼 수 있습니다.
24:43
이것은 fy일뿐만이 아니라 y 요소의 첨자에 지나지 않습니다.
24:48
이것은 x의 함수이지만, 여기에서는 곱하면 됩니다.
24:54
이것을 쓸 때 도움이 될 수 있는 또 다른 팁은
24:59
미분의 softmax 부분은 실제로 두 가지 경우를 생각할 수 있다는 것입니다.
25:03
하나는 올바른 클래스로  c = y, 
25:05
그리고 다른 하나는 다른 잘못된 모든 클래스입니다.
25:09
그리고 이것을 쓸 때, 여러분은
25:14
이런 것들이 생기는 것을 관찰할 수 있을 것입니다.
25:15
그래서 그냥 문제에 넣어서 쓰지 말고,
25:19
거기에 도착하기까지의 방법에 대한 각 단계를 생각해 보기 바랍니다.
25:21
그렇지만 근본적으로 어떤 시점에서 우리는 이 종류의 패턴을 관찰하게 되는데
25:25
f의 모든 요소와 관련하여 모든 미분을 살펴보게 될 것입니다.
25:30
그리고 지금, 우리는 깨닫습니다.
25:33
올바른 클래스는 실제로 여기에서 하나를 뺍니다.
25:35
그리고 모든 잘못된 클래스들에는 아무것도 하지 않을 것입니다.
25:40
자, 문제는 이것을 구현할 때입니다.
25:42
그것은 마치 if 문장처럼 보인다.
25:44
y가 훈련 세트에서 올바른 클래스와 동일한 경우
25:47
1에서 빼면 되지만 그다지 효율적이지는 않을 것이다.
25:51
그렇다고 실제로 처음
25:54
더 복잡한 뉴럴 네트워크 아키텍처의 방정식을 작성하려고 하면 정신 나간 것입니다.
25:56
대신 우리가 
26:01
구현하려고 하는 것을 벡터화 하려고 할 것입니다.
26:05
이 경우에 이것의 의미는
26:07
실제로 관찰 할 수 있는 것으로 여기 1은 정확하게 1입니다,
26:10
여기서 t는  타겟 분포로 우연히 똑같이 1입니다.
26:15
그래서, 우리가 하고 싶은 것은
26:20
이것을 y (hat) - t라고 기술하면, 이것은 이것과 같은 것입니다.
26:27
중간에 이해하지 못했더라도 걱정하지 마십시오.
26:28
왜냐하면 이것은 문제 세트의 일부로
26:31
어느 시점에서는,
26:32
미분을 하는 동안 이런 방정식을 보게 될 것이기 때문입니다.
26:36
그리고 지금, 역전파를 향한 첫 번째 단계인데, 실제로
26:41
이 term은 더 단순한 단일 변수 델타라고 칭하겠습니다.
26:47
느낌이 좋네요. 델타가
26:50
오류 신호의 좋은 친구가 될 것입니다.
26:51
자, 마지막 몇 가지 팁 중에
26:55
6 번째 팁입니다.
26:56
이 연쇄 규칙으로 시작할 때
27:01
편미분을 살펴보기 전에 명시적 합계를 사용하는 것이 좋습니다.
27:03
몇 번 반복하면 패턴을 볼 수 있습니다.
27:06
그러면 우리는 그 패턴들로부터
27:11
단일 편미분을 벡터 및 행렬 표기법으로 변환해서 외삽을 얻어낼 수 있을 것입니다.
27:16
예를 들어, 여기에 이와 비슷한 내용이 표시됩니다.
27:21
미분은 어느 시점에서는
27:25
훈련 세트 x와 y에서 합한 요소에 대한
27:31
전체적인 목적 함수의 x에 대한 전반적인 미분이 될 것입니다.
27:37
그리고 이것에 대해 잠시 생각해보면,
27:39
여기에 이 행벡터를 가져야 하지만,
27:44
모든 C 에 대해 몇 번 하다 보면 내적이 될 것입니다. 
27:49
전체 벡터를 얻고 싶다고 한 것은 결국
27:53
W 전치(transpose)*the delta를 다시 쓴 것일 뿐입니다.
27:58
그래서, 이것이 softmax에서 얻은 하나의 오류 신호입니다.
28:03
softmax 가중치를 전치한 것을 이 값으로 곱합니다.
28:08
그리고 이들 중 일부가 분명하지 않은 경우에
28:10
여전히 혼란스럽다면, 합하고 나서,
28:11
벡터 표기법으로 이것을 다시 써야 합니다.
28:16
자, 이제 원도우 벡터 경사도의 차원은 무엇입니까?
28:22
이제, 우리 훈련 세트에 있는 x에 관한 한 요소의
28:28
전체 손실되는 값의 미분을 구했습니다.
28:30
x는 원도우입니다.
28:32
좋아요,  다섯 단어로 된 원도우이 있다고 말했었죠?
28:36
그리고 각 단어는 d-차원입니다.
28:39
자, 이 경사도의 미분의 차원은 무엇입니까?
28:55
(질문) 맞습니다. 그것은 5d입니다.
28:58
또 다른 좋은 방법입니다. 우리가 여러분을 처음부터 구현하게 만드는 이유 중 하나입니다.
29:02
만약 어떤 파라미터가 있고, 그 파라미터에 대한 경사도를 구했을 때
29:07
같은 차원이 아니라면 코드나 맵에
29:11
실수가 있거나 버그가 있을 것입니다.
29:15
이럴 때 아주 간단한 디버깅 기술이 있는데
29:19
자신의 방정식을 확인하는 것입니다.
29:22
그래서, 이 원도우에 대한 최종 미분은 이제 이 5 개의 벡터입니다.
29:26
왜냐하면 우리가 연결 한 다섯 개의 d-차원 벡터가 있었기 때문입니다.
29:29
자, 물론 까다로운 점이 있는데
29:32
실제로 전체 벡터가 아닌 워드 벡터를 업데이트하고 싶은 것이고
29:35
원도우는 중간 단계일 뿐입니다.
29:38
그래서 정말로, 우리가 하고 싶은 것은 
29:40
워드 벡터의 각 요소를 미분으로 업데이트하고 싶은 것입니다.
29:45
그것은 아주 간단하게,
29:51
경사도 전반에 걸쳐 발생한 오류를 쪼개서 할 수 있습니다. 전체 원도우는
29:56
기본적으로 모든 다른 워드 벡터의 축소된 연결입니다.
30:02
그리고 전체 시스템을 훈련 할 때 워드 벡터를 업데이트해서 사용할 수 있습니다.
30:06
질문 있나요?
30:19
(질문) 거기에 수학적.... ?
30:19
타겟 벡터 t에 대한 수학적 표기법이 있습니까?
30:24
그냥 변수 t가 아니라.
30:27
저는 괜찮은 표기법처럼 보입니다.
30:31
이것을 확률 분포로 볼 수 있습니다.
30:36
네. >> 그게 다예요. 다른 것은 없습니다.
30:39
한 위치를 제외하고 모두 0 인 단일 벡터.
30:41
>> 그럼 그걸 쓸까요?
30:44
>> 직접 쓸 수도 있어요.
30:45
우리는 언제나 그냥 쓸 수 있으며, 그것이 또한 매우 중요한 것입니다.
30:49
우리는 항상 모든 것을 정의하고 싶어 합니다.
30:54
옳은 것을 생각하고 있다면 미분을 쓰고
30:57
차원을 쓰고, 적절하게 정의하고,
31:00
더 큰 차원의 벡터 인 경우 점, 점, 점을 사용할 수 있습니다.
31:03
t를 타겟 분포로 정의 할 수 있습니다. [안들림]
31:12
>> 질문은
31:13
여전히 각 단어에 대해 두 개의 벡터를 가지고 있습니까?
31:15
좋은 질문입니다. 답은 ‘아니요’ 입니다.
31:16
glove과 word2vec를 했을 때 여기 두 개의 u와 v를 가지고 있었습니다.
31:21
지금부터 이후의 모든 강의에서는 u와
31:25
v 그리고 그것은 각 단어에 대한  단일 벡터 x를 가정합니다.
31:34
질문은 이 경사도는 많은 다른 원도우에서 사라지지 않나요?입니다.
31:36
네 그렇습니다.
31:37
대답은 ‘예’입니다.
31:39
"in"이라는 단어가 있으면
31:44
그 안에 "in"이라는 단어가 있는 모든 원도우에서 벡터와 경사도가 나타납니다.
31:49
박물관과 마찬가지입니다.
31:51
그리고 확률적인 경사 하강(SGD)을 할 때 한 번에 하나의 원도우를 보게 됩니다.
31:54
하나를 업데이트하고 다음 원도우로 가서 다시 업데이트 합니다.
31:57
좋은 질문입니다.
32:01
네. 
32:04
이제, 이렇게 연결된 워드 벡터를 어떻게 업데이트하는지 살펴 보겠습니다.
32:10
훈련 할 때,
32:12
예를 들어 감성 분석이면, 우리는 모든 긍정적인 단어를 한 방향으로 밀어 내고
32:16
다른 방향으로 다른 단어들을 밀어 낼 것입니다.
32:18
우리가 개체명 인식과 같은 것을 훈련시킬 때
32:22
결국  모델은 중심 단어 바로 전의 것을 보고 배우는데
32:26
중심 단어가 위치 정도일 것입니다.
32:32
이제 전체 원도우 모델을 훈련하기 위해 누락 된 부분은 무엇입니까?
32:36
주로 softmax 가중치 W에 대한 J의 경사도입니다.
32:43
기본적으로 비슷한 단계를 밟을 것입니다.
32:45
우리는 Wij와 관련하여
32:48
처음과 나중의 모든 편미분을 적을 것입니다.
32:49
그런 다음에 우리는 이 전체 모델에 대한 완전한 경사도를 알게 될 것입니다.
32:53
다시 말하지만, 이것은 매우 sparse해 집니다.
32:55
이 워드 벡터 업데이트를 구현할 때 좀 더 영리한 방법을 쓰면
33:00
모든 원도우문에서 한 무리의 제로를 보내지 않을 수도 있습니다.
33:05
왜냐하면 각 원도우은 단지 몇 단어일 뿐이기 때문입니다.
33:09
사실, 
33:13
신중한 매트릭스 구현을 통해 문제의 코드를 생각하는 것이 중요합니다.
33:17
이것에 2 ~ 3 개의 슬라이드를 쓸 가치가 있다고 생각합니다.
33:20
softmax에는 기본적인 두 가지 작업이 있는데
33:25
행렬 곱셈과 지수가 그것입니다.
33:28
사실 나중에 이 강의에서 우리는 지수를 다루는 방법을 찾을 것입니다.
33:34
그러나 행렬 곱셈 역시 훨씬 더 효율적으로 구현 될 수 있습니다.
33:40
그래서 처음에
33:43
이 클래스와 이것이 해당 클래스의 확률에 유혹에 빠질 수 있을 것입니다.
33:45
모든 다른 클래스의 for 루프를 구현했습니다.
33:49
한 번에 한 행씩 미분 또는 행렬 곱셈을 취하면
33:53
매우 비효율적일 것입니다.
33:57
그래서 여기 아주 간단한 파이썬 코드를 통해 제가 의미하는 바를 보여 드리겠습니다.
34:02
항상 이러한 워드 벡터를 반복합니다.
34:05
모든 것을 하나의 큰 행렬로 연결하는 대신.
34:09
이들을 곱하면 항상 더 효율적으로 될 것입니다.
34:13
우리가 분류하고 싶은 500 개의 원도우가 있다고 가정합시다.
34:18
그리고 각 원도우의 차원이 300이라고 가정해 봅시다.
34:23
이들은 합리적인 숫자입니다.
34:26
softmax에 5 개의 클래스가 있다고 가정하고
34:30
계산 중 어느 시점에서 두 가지 옵션이 있습니다.
34:34
W는 softmax의 가중치입니다.
34:36
그것은 많은 C 열과 많은 d 칼럼이 있을 것입니다.
34:40
이제는 워드 벡터를 사용하여 각 원도우를 연결할 수 있습니다.
34:44
이렇게 하면 별도의 워드 벡터 묶음 목록을 가질 수 있습니다.
34:48
또는 우리는 dn이 될 하나의 거대한 행렬을 가질 수 있습니다.
34:52
그래서 많은 행의 d 와 n 개의 원도우로
34:55
여기에 500 개의 원도우가 있습니다. 따라서 이 한 개의 행렬에 500 개의 칼럼이 있습니다.
35:01
이제는 기본적으로 W를 각 벡터에 대해 개별적으로 곱할 수 있습니다.
35:07
그런 다음에 이 행렬 곱셈을 마치면,
35:11
문자 그대로 12 배의 속도 차이가 납니다.
35:16
그리고 아쉽게도 이 큰 모델들,
35:19
한 번 도는데 하루가 걸릴지 모르는 복잡한 모델의 대형 데이터 세트에 대한 것입니다.
35:24
12 일인지
35:26
1 일 동안의 반복인지가 마감일과 모든 것들을 결정하게 할 것입니다.
35:31
그래서 매우 중요하고, 지금은 거의 대부분의 사람들은
35:35
무엇을 의미하는지에 매여 있지만
35:39
본질적으로,
35:44
우리가 한 것은 하나의 softmax에서 했던 것과 같은 것으로
35:51
많은 입력 벡터 x를 연결한 것이고
35:55
결국 많은 다른 비정규화 된 점수를 얻을 것입니다.
36:01
그런 다음에 우리는 그들을 다시 떼어 놓을 수 있습니다.
36:04
여기 d 차원 입력에 대한 ct 차원 행렬이 있습니다.
36:10
동일한 표기법을 사용하였나요?,
36:13
각 윈도우의 차원d*n 행렬은 c*n 행렬을 얻습니다.
36:18
그래서 이것들은 여기에 모든 가능성이 있습니다.
36:24
N 개의 많은 훈련 샘플로.
36:30
질문이 있나요?
36:31
(질문)매우 중요합니다. 이렇게 하지 않으면 모든 코드가 너무 느려집니다.
36:38
그리고 이것은 구현을 위한 트릭입니다.
36:42
그리고 대부분의 방정식에서,
36:44
모든 것을 너무 복잡하게 만들게 하지는 않겠지만
36:49
방정식 예제 하나를 살펴 볼 것인데
36:53
결국 모든 코드를 벡터화하고 싶을 것입니다.
37:00
예, 행렬은 친구입니다. 가능한 한 많이 사용하십시오.
37:04
또한 많은 경우, 특히 이 문제에 대해서는
37:09
모델을 훈련하고 최적화하는 데 필요한 기초적인 방법을 이해해야 합니다.
37:13
여러 분은 많은 다른 선택을 할 것입니다.
37:17
마치 제가 이런 방식으로 구현할 수 있을 것 같을 때
37:19
TA에게 가서 이런 방식 또는 그런 식으로 시행할 수 있을까요 라고 물어 볼 수 있습니다.
37:22
하지만 그 시간을 마술 같은 파이썬을 사용하는 데 보낼 수도 있습니다.
37:28
매우 통찰력 있는 결정을 내리고 직감을 얻을 수 있을 것입니다.
37:33
그리고 기본적으로 속도 테스트를 많이 하고
37:37
여러 가지 옵션에 많은 시간을 할애해야 합니다.
37:43
좋아요, 이건 그냥 순수한 softmax 일 뿐이고
37:47
softmax만으로는 그렇게 강력하지 않습니다.
37:52
이 선형 결정 경계는 실제로
37:56
원래의 공간에 있습니다.
37:57
여러분이 쓰는 훈련 데이터가 아주 적다면 괜찮겠지만, 그리고
38:01
추상적인 정규 표현식처럼 강력한 모델을 사용하지는 않을 것이지만
38:06
더 많은 데이터라면 상당히 제한적입니다.
38:08
그래서 우리가 여기에 단어들의 무리를 가지고 있고 우리가 워드 벡터를 업데이트하고 싶지 않다면,
38:13
softmax만으로 선형 결정 경계를 만든다면 상당히 부끄러울 것 같네요.
38:18
이 지점들을 올바르게 분류할 수 있다면
38:21
더 좋을 것입니다.
38:25
근본적으로 이것은 신경망을 쓰는 많은 동기 중 하나입니다.
38:30
왜냐하면 신경망은 훨씬 더 복잡한 의사 결정 경계를 제공 할 것이며
38:35
훨씬 더 복잡한 기능을 훈련 데이터에 적용 할 수 있게 할 것입니다.
38:40
그리고 비판적일 수도 있겠지만
38:42
뉴럴 네트워크라고 명명한 것은 상당히 잘 한 것 같습니다.
38:45
단지 일반적인 함수 접근과
38:46
똑 같다고 할 수는 없지만, 본질적으로 같습니다.
38:53
단순한 로지스틱 회귀부터
38:58
뉴럴 네트워크 및 그 너머, 그리고 딥뉴럴넷까지 정의해 봅시다.
39:01
쉬게 느껴질 수 있도록
39:03
다시 용어의 일부를 정의합니다.
39:05
수학을 통해 더 재미있는 것들을 할 수 있는 1.5시간이 남았습니다.
39:10
기본적으로 모든 레고 블록을 사용할 수 있습니다.
39:12
힘들겠지만 인내심을 가지고
39:15
계속 집중하고 질문도 하세요.
39:19
왜냐하면 우리는 정말 대단한 멋진 모델을 만들 것인데 쓸모가 많을 것이기 때문입니다.
39:25
입력 값이 있고 바이어스 유닛과 활성화 함수 및
39:30
더 커다란 뉴런 네트워크에 있는 각각의 단일 뉴런에 대한 출력이 있을 것입니다.
39:36
그럼 먼저 단일 뉴런을 정의 해 봅시다.
39:39
기본적으로 이진 로지스틱 회귀 유닛을 볼 수 있습니다.
39:44
내부에는
39:47
다시 한 번 우리가 입력 한 결과로 가중치 세트가 있습니다.
39:52
여기에 이 뉴런에 입력 x가 있습니다.
39:55
그리고 결국에는 편향 term을 추가 할 것입니다.
39:56
늘 피처가 있고
39:58
이 뉴런이 시작하는 부분을 정의합니다.
40:02
시작한다는 것은 일에 가까울 확률이 매우 높음을 의미합니다.
40:07
계속해서
40:08
그리고 f는 앞으로 항상 요소별(element wise) 함수가 될 것인데
40:14
이 경우는 이 경우 sigmoid로 여기에 이 합계가 무엇이든 간에
40:20
값에 바이어스를 더한 것을 합쳐 놓으면 기본적으로는 0과 1 사이의 값을 갖습니다.
40:26
좋습니다, 이것이 단일 뉴런의 정의입니다.
40:30
이제 입력 벡터에 각각의 작은 로지스틱
40:34
회귀 함수와 뉴런을 사용하면 출력을 얻을 수 있습니다.
40:38
그리고 이제 softmax로 직접 예측하는 것과의 차이점을 살펴보면
40:43
표준 기계 학습 및
40:45
딥러닝은 실제로 우리가 직접 출력을 내리도록 강요하지 않는다는 것입니다.
40:50
그들은 그들 자신이 또 다른 뉴런에 대한 입력이 될 것입니다.
40:56
그리고 그것은 크로스 엔트로피와 같은 그 뉴런의 꼭대기에 있는 손실 함수 같은 것으로
41:01
이제 이 중간에 숨겨진 뉴런을 지배합니다.
41:06
또는 숨겨진 계층에서 실제로 달성하려고 시도 할 것입니다.
41:09
그리고 모델은 자신이 나타내는 것을 스스로 결정할 수 있습니다.
41:13
이 숨겨진 유닛 내부에서 이 입력을 어떻게 변환해야 할까요?
41:17
최종 산출물에서 더 낮은 오차를 줄 수 있습니다.
41:23
그리고 실제로 이것은 단지 숨겨진 뉴런의 연결일 뿐입니다.
41:27
이 작은 이진 로지스틱 회귀 유닛이
41:30
우리가 매우 깊은 신경망 구조를 구축 할 수 있게 해줄 것입니다.
41:37
다시 한 번, 우리는 행렬 표기법을 사용해야 할 것입니다.
41:43
이 모든 것은 행렬 곱셈으로 매우 간단하게 설명 될 수 있습니다.
41:48
그래서 여기 a1은
41:52
첫 번째 뉴런의 활성화를, 두 번째 뉴런은 a2 로 최종 결정 될 것입니다.
41:56
따라서 여기에 내적을 쓰거나
42:00
내적에 바이어스 term이나 행렬 표기법을 사용하려고 합니다.
42:06
그리고 이 중간 변수에 주의를 기울이는 것이 매우 중요합니다.
42:10
우리는 이것들을
42:12
우리는 체인 규칙을 적용하거나 미분할 때 되풀이해서 볼 것이기 때문에 정의했습니다.
42:17
여기에서는 z를 W*x에 편향의 벡터를 더한 것으로 정의 할 것입니다.
42:24
기본적으로 많은 편향의 조건들이 있습니다. 그리고 이 벡터는
42:28
이 층에 있는 뉴런 수와 동일한 차원입니다.
42:34
그리고 W는 뉴런 수에 대한 행의 수이고
42:39
x는 입력 차원에 대한 열의 수입니다.
42:44
그리고 f(z)를 쓸 때마다,
42:47
여기서 의미하는 것은 실제로 f에 요소별(element wise)로 적용한다는 것입니다.
42:52
따라서 z가 벡터 일 때 f(z)는 f(z1), f(z2) 및 f(z3)이 될 것입니다.
42:59
그리고 이제 여러분은 질문 할 것입니다. 그럼 왜 우리는
43:03
이 시그모이드 함수라고 하는 이 복잡한 것을 여기에 추가해야 합니까?
43:07
나중에 우리는 실제로 다른 종류의 비선형성을 다루어야 할 수도 있습니다.
43:10
이 f 함수를 우리가 알고 있지 않으면
43:13
그 사이의 비선형성과
43:17
이 선형층을 함께 사용하면 매우 다른 기능이 추가되지 않을 것입니다.
43:20
사실 그것은 단지 하나의 선형 함수로 계속 될 것입니다.
43:25
직관적으로 여러분이 더 많은 숨겨진 뉴런을 다룰수록
43:29
점점 더 복잡한 기능에 적합하게 할 수 있을 것입니다.
43:31
이것은 3 차원 공간에서의 결정 경계와 같고
43:34
간단한 회귀에서도 이 term을 생각할 수 있습니다.
43:37
만약 우리에게 단지 하나의 숨겨진 뉴런만 있다면,
43:39
여기 거의 반전된 시그모이드가 가능할 것입니다.
43:42
3 개의 숨겨진 뉴런이 있다면, 더 복잡한 이런 종류가 적합 할 수 있습니다.
43:46
함수와 10 개의 뉴런을 가지고, 각 뉴런은
43:50
근본적으로 오버피팅 되기 시작하고 잘 피팅되는 정확한 한 점에 피팅시킬 수 있을 것입니다.
43:56
이제  단일 원도우 분류기(window classifier)를 다시 살펴 봅시다.
44:00
지금 하고 있는 softmax를 직접 적용하는 대신에
44:05
이제 단어 벡터와 출력 사이에 중간에 은닉층을 갖게 될 것인데
44:10
이때부터는 정확하고 표현력도 뛰어나게 될 것입니다.
44:17
그럼 단일 레이어 신경망을 정의합시다.
44:24
입력 x가 다시 필요하네요.
44:26
원도우는 여러 워드 벡터의 연결이었고
44:31
우리는 z와 a를 정의할 것인데 a에서의 z에 대해 요소별(element wise)로 하게 될 것입니다.
44:37
이제 우리는 이 뉴럴 활성화 벡터 a를
44:43
최종 분류 레이어의 입력으로 사용할 수 있습니다.
44:48
지금까지 우리가 가진 디폴트는 softmax였습니다.
44:51
softmax를 재사용하지 맙시다.
44:53
우리는 여러 번 해본 적이 있고 문제 세트에서 다시해볼 것이니까
44:56
좀 더 단순한 것을 소개하고 이
44:59
분류기를 통해서 모든 세부 사항을 살펴보는 것이 좋겠습니다.
45:03
이것은 간단하고 비표준적인 점수가 될 것입니다.
45:06
이 경우에는
45:12
다양한 간단한 바이너리 분류 문제에 대한 올바른 메커니즘이 될 것입니다.
45:13
이때 확률 z는 0.8이다와 같은 것에 대해서는 신경 쓰지 않아도 되고
45:17
우리가 진짜 신경 써야 할 것은 일인가? 클래스인가 아닌가 입니다.
45:23
이제 이 새로운 출력 레이어의 목적 함수를 잠깐 정의할 것입니다.
45:28
먼저 피드포워드 프로세스를 이해해 보겠습니다.
45:31
그리고 피드포워드 프로세스는 테스트할 때
45:35
미분을 하기 전에 의 각 요소 역시 훈련이 되어 있어야 합니다.
45:38
미분할 때 항상 앞으로 이동 한 다음 뒤로 이동합니다.
45:43
여기서 해야 할 것은,
45:45
예를 들어, 기본적으로 각 원도우를 취한 다음 점수를 매긴 다음
45:50
점수가 높으면 우리는 모델을 훈련 시켜서
45:54
파리, 런던, 독일, 스탠포드와 같은 개체명 위치가 중심 단어인
46:01
위치에 높은 점수를 할당하면 될 것입니다.
46:06
이제 우리는 종종
46:09
이런 종류의 그래프를 많은 논문에서 볼 수 있으니까 익숙해지면 좋겠습니다.
46:14
천천히 전체 강의를 통해서 소개하려고 노력할 것인데 다양한 종류가 있지만
46:17
이것은 가장 일반적인 것입니다.
46:20
아래쪽부터 각각의 레이어는 무엇을 하는지 정의하고
46:26
미분해서 최적화 하는 방법을 배웁니다.
46:31
여기 x 윈도우는 모든 워드 벡터의 연결입니다.
46:35
잘 들으세요. 조금 있다가 질문을 할게요.
46:39
여기서 모든 파라미터의 차원을 알아내려고 노력해 보도록 하겠습니다.
46:42
같이 해봅시다.
46:44
여기에 있는 각 워드 벡터가 4 차원이고
46:49
각 원도우에 5 개의 워드 벡터가 연결되어 있습니다.
46:52
따라서 x는 20 차원 벡터입니다.
46:57
다시 열벡터를 정의하겠습니다.
47:00
여기에 첫 번째 은닉층이 있고
47:03
8 개의 유닛이 있다고 가정 해 봅시다.
47:05
따라서 8 개 유닛의 은닉층을 중간 표현으로 사용하고 싶습니다.
47:11
그렇게 하면  최종 점수는 다시 한 번 말씀드리지만 단순한 단일 숫자가 될 것입니다.
47:15
제가 방금 W의 차원은 얼마라고 했습니까?
47:21
20 차원의 입력과 8 은닉 유닛,
47:31
20 행 8 열이었습니다.
47:34
한 번 더 이동... [웃음] 맞습니다.
47:38
8 행 20 열이 될 것입니다. 그렇죠?
47:41
확실하지 않을 때마다 늘 할 수 있습니다.
47:44
그러면 이것은 n*d와 같이 될 것이고
47:48
그리고 이것을 곱한 다음 이렇게 될 것입니다. 그리고 이것은 항상 d가 될 것입니다,
47:53
그래서 이 둘은 항상 같아야 합니다. 그렇죠?
47:55
이제 이해되겠죠.
48:00
특히 자연어처리의 경우 이 추가 레이어에서 직관적으로
48:03
서로 다른 입력 단어 간의
48:06
비선형적 상호작용(non-linear interactions)을 배울 수 있습니다.
48:08
in이 이 위치에 등장한다면
48:12
다음 단어는 항상 위치일 확률이 높습니다.
48:17
여기서 우리가 배울 수 있는 패턴은 in이 두 번째 위치일 때.
48:23
박물관이 첫 번째 벡터이면 위치의 확률이 증가한다는 것입니다.
48:28
여기서는 서로 다른 입력 간의 상호 작용을 배울 수 있습니다.
48:31
그리고 이제 우리는 결국 우리 모델을 보다 정확하게 만들 것입니다.
48:43
(질문) 좋은 질문입니다.
48:44
질문은 두 번째 W가 있냐는 것이었습니다.
48:45
여기 두 번째 레이어는 점수가 비정규화되어 있으므로 U와
48:50
우리는 하나의 U를 가지고 있기 때문에, 이것은 하나의 열벡터일 뿐입니다.
48:54
내적이 스코어에 대해 하나의 숫자를 얻도록 전치(transpose)
48:59
죄송합니다. 예, 질문은 두 번째 W 벡터가 있냐는 것이었습니다.
49:04
그래서 네, 그것은 어떤 의미에서는 두 번째 행렬이지만
49:07
우리에게는 오직 하나의 은닉 뉴런 층만 있기 때문에, 오직 하나의 벡터만 필요로 합니다.
49:16
좋습니다.
49:17
자 이제 최대 마진 손실을 정의합시다.
49:21
실제로 매우 강력한 손실 함수로
49:26
softmax의 크로스 엔트로피 오류보다 훨씬 강력하고 유용합니다.
49:31
여기서 두 가지 예를 정의 해 봅시다.
49:35
원도우에 높은 점수를 주고 싶고
49:40
중심 단어는 위치는 여기입니다.
49:42
센터 단어가 명명 된 개체명 위치가 아닌
49:46
잘못된 원도우를 줄이기 위해서는 낮은 점수를 주어야 할 것입니다.
49:50
박물관은 기술적으로는 위치 정보이지만 개체명의 위치는 아닙니다.
49:55
그래서
49:56
이 최대 마진의 훈련 목표는 본질적으로
50:01
잘된 원도우를 크게 하고 잘못된 원도우를 작거나 낮게 만들어서
50:07
충분히 좋아지게 만드는 것입니다.
50:08
충분히 좋아지고 나면 하나의 값에 따라 다르게 됩니다.
50:14
여기 마진이 있고
50:16
자주 하이퍼 파라미터로 볼 수 있습니다.
50:18
그것을 m으로 설정하고 다른 것들을 시도해 보면 많은 경우에 잘 동작합니다.
50:22
연속적인 것이고 SGD를 사용할 수 있습니다.
50:25
이제 softmax는 직관적으로 무엇입니까? 죄송합니다. 맥스 마진 손실은 무엇입니까?
50:32
예를 들어 아주 간단한 데이터 세트와
50:35
몇 가지 훈련 샘플이 있다고 해 봅시다.
50:38
여기 다른 클래스 C가 있습니다.
50:43
표준 softmax는 다음과 같은 결정 경계를 제공 할 수 있는데,
50:50
둘 사이를 완벽하게 구분하는 것과 같습니다.
50:52
이것은 아주 간단한 훈련 예입니다.
50:54
대부분의 표준 softmax 분류기는
50:56
이 두 클래스를 완벽하게 분리합니다.
50:59
그리고 다시, 이것은 단지 2 차원으로 그려준 것뿐입니다.
51:01
실제로는 이보다 훨씬 더 높은 차원의 문제들입니다.
51:03
그러나 여기서도 직관적으로 볼 수 있는 게 많습니다.
51:06
이제 결정 경계가 있는데 이것은 softmax입니다.
51:09
문제는 아마도 여러분의 훈련 데이터 세트였을 것입니다.
51:12
하지만 테스트 세트에는 실제로 다른 테스트 세트가 포함될 수 있습니다.
51:17
그것은 우리가 훈련에서 보았던 것들과 꽤 비슷하지만 알다시피 조금 다를 것입니다.
51:22
그리고 이제 이런 종류의 결정 경계는 그리 강건하지 않습니다.
51:27
이와 대조적으로, 최대 마진 손실은
51:32
훈련 데이터 세트의 가장 가까운 지점 사이로
51:37
마진을 높이려고 시도할 것입니다.
51:42
그래서 여기에 몇 가지 점이 있다면 여기에 다른 점이 있습니다.
51:48
우리는 가장 가까운 점으로의 거리를 최대화하려고 노력할 것이기 때문에
51:52
본질적으로보다 강건합니다.
51:56
테스트 할 때 비슷한 점이 몇 가지 있겠지만
52:00
그다지 많지는 않기 때문에 정확하게 분류 할 가능성이 더 큽니다.
52:05
이것은 정말 큰 손실(lost)이거나 또는 목적 함수입니다.
52:10
이제 잘못된 원도우 sc의 경우에
52:15
실제로 대부분의 경우
52:18
이들을 여러 번 합할 것입니다.
52:19
그리고 우리는 이것을 스킵 그램 모델과 유사하게 생각할 수 있습니다. 여기서 우리는 
52:23
잘못된 사례 몇 가지로 무작위 샘플을 만들고
52:25
이렇게 훈련만하면 됩니다.
52:28
이것에 대한 진정한 예는 컨텍스트에서의 위치입니다.
52:32
다른 모든 원도우에는
52:35
여러분의 훈련 자료는 본질적으로 부정적인 클래스의 일부가 있기 때문입니다.
52:40
좋아요, 최대 마진 목적 함수를 둘러싼 질문 있나요?
52:43
우리는 이제 미분을 할 것입니다.
52:57
(질문) 맞아요, 잘못된 원도우은 부정적인 클래스인가요?
53:00
네, 맞습니다.
53:02
따라서 중심 위치가 없는
53:07
다른 클래스로서 원도우를 생각할 수 있습니다.
53:11
좋습니다. 이제 어떻게 최적화할까요?
53:14
우리는 크로스 엔트로피를 사용한 것과 매우 유사한 단계를 밟을 것입니다.
53:19
이제 우리는 실제로 이 은닉 레이어가 있으면, 두 번째 단계는
53:25
다음 강의에서 다룰 전체 역전파 알고리즘입니다.
53:29
여기에서 비용 J가 0보다 큰 것으로 가정합시다.
53:35
그럼 이게 무슨 뜻일까요?
53:36
처음에는 모든 파라미터를 다시 초기화합니다.
53:41
무작위로 또는 합리적으로 워드 벡터를 초기화합니다.
53:44
그러나 이런 컨텍스트의 윈도우를 학습하는 데는 완벽하지 않을 것입니다.
53:48
어떤 것이 위치이고 어떤 것이 아닙니까?
53:50
처음에는 모든 점수가 낮은데 왜냐하면 전부 파라미터가 ​​될 가능성이 높기 때문입니다
53:55
U와 W와 B는 작은 난수로 초기화되었습니다.
54:01
그래서 저는 원도우를 구별하는 데 큰 도움이 되지 않을 것입니다.
54:06
중앙에 정확한 위치 대 부패한 위치.
54:10
그리고 기본적으로, 우리는 이 상태에 있다가
54:15
훈련을 받을수록 결국에는 더 나아질 것입니다.
54:19
그리고 직관적으로 여기에 점수가 있다면
54:21
좋은 원도우의 인스턴스는 5이고 잘못된 것은 단지 두 개뿐입니다.
54:26
1-5+2가 0보다 작은 것을 볼 수 있습니다.
54:30
그러면 기본적으로 그 요소들에 대해 0의 손실을 갖게 됩니다.
54:34
그리고 그것은 이 목적 함수의 또 다른 커다란 특성은
54:39
훈련 세트를 더 많이 무시하고 시작할 수 있는데
54:44
이 예제에서 0의 오류에는 0을 할당하게 되기 때문입니다.
54:48
그 결과 목적 함수에 더 집중할 수 있게 됩니다.
54:54
모델이 여전히 구별하는 데 어려움이 있습니다.
54:59
좋습니다. 처음부터 대부분의 예제에서
55:03
J는 0보다 큰 값이 된다고 가정해 봅시다.
55:06
이제 우리가 해야 할 일은 
55:09
우리 모델의 모든 파라미터를 미분하는 것입니다.
55:12
그럼 다른 것들은 뭐가 있죠?
55:13
U, W, b와  워드 벡터 x입니다.
55:17
항상 처음부터 시작한 다음 내려가야 합니다. 왜냐하면
55:22
다른 요소들을 재사용해서, 미분을 취하고 변수를 재사용하는 단순한 결합은
55:26
전파를 뒤로하게 할 것이기 때문입니다.
55:30
따라서 U와 관련하여 s의 미분입니다.
55:32
s는 무엇입니까?
55:34
s는 단지 a곱의 전치한 것일 뿐입니다.
55:38
그리고 미분한 것이라는 걸 모두 알 것입니다.
55:41
쉽죠? 첫 번째 요소와 일급 미분으로 전진일 뿐입니다.
55:47
다음 미분을 취할 때 중요합니다.
55:51
또한  모든 정의를 알고 있어야 합니다.
55:53
미분을 사용하고 있는 함수를 어떻게 정의 할 것인가?
55:57
그래서 s는 기본적으로 U 전치이고 a는 f (z)이고 z는 단지 Wx + b입니다.
56:04
좋아요, 그냥 계속 추적하는 것이 중요합니다.
56:06
이제 거의 80 % 작업한 것과 같습니다.
56:09
이제, 제가 말했던 것처럼 미분을 합시다.
56:12
직감을 얻기 위해 W의 한 요소의 처음 부분을
56:16
다시 모으고 더 복잡한 행렬 표기법을 사용할 수 있습니다.
56:23
Wij가 실제로 나타날 것임을 관찰 할 것입니다.
56:28
우리 숨겨진 레이어의 i 번째 활성화.
56:32
예를 들어, 우리가 3 차원 x를 가진 매우 간단한 입력을 가지고 있다고 가정 해 봅시다.
56:37
그리고 우리에게는 두 개의 숨겨진 유닛이 있습니다. 그리고 하나의 최종 스코어 U가 있습니다.
56:43
그런 다음 W23과 관련하여 미분을 하면서 이를 관찰 할 것입니다.
56:48
그래서 W의 두 번째 행과 세 번째 열,
56:52
실제로 a2에서만 필요합니다.
56:55
W23을 사용하지 않고 a1을 계산할 수 있습니다.
57:00
그럼 이게 무슨 뜻이죠?
57:01
우리가 Wij 가중치의 미분을 한다면,
57:05
우리는 실제로 벡터 a의 i 번째 요소를 볼 필요가 있습니다.
57:11
따라서 우리는 이 모든 내적을 볼 필요가 없습니다.
57:16
그럼 다음 단계는 무엇입니까?
57:17
우리가 W로 미분을 얻는 것처럼, 우리는 W가 어디서 나타나는지 알고 있어야 합니다.
57:21
다른 모든 파라미터는 본질적으로 일정합니다.
57:25
여기 U는 미분을 없애는 것이 아닙니다.
57:29
그래서 우리가 할 수 있는 것은 단지 하나의 숫자처럼, 꺼내는 것입니다.
57:32
우리는 여기 미분을 넣을 것입니다.
57:36
그리고 이제 우리는  인공 지능을 매우 신중하게 정의 할 필요가 있습니다.
57:41
그래서 아래 첨자 i는 Wij가 나타나는 곳입니다.
57:45
자, ai는 이 함수이고 zi의 f로 정의했습니다.
57:50
우리가 조심스럽게 작성해야 할 것은
57:54
이제 이것은 zi와 관련하여 ai의 미분으로 체인 규칙을 처음 적용하는 것이고
57:59
zi는 Wij에 관한 것입니다.
58:04
따라서 이것은 체인 규칙의 단일 응용 프로그램입니다.
58:14
그리고 그것의 끝은 압도적으로 보입니다. 그러나 각 단계는 매우 분명합니다.
58:18
그리고 각 단계는 간단합니다. 우리는 정말로 모든 세부 사항을 쓰고 있습니다.
58:23
그래서 체인 규칙을 적용하여 이제 우리는 ai를 정의할 것입니다.
58:29
ai는 단지 f의 zi이고, f는 단일 숫자 zi의 요소 y 함수였습니다.
58:36
ai를 z의 f의 정의로 다시 쓸 수 있습니다.
58:40
우린 이걸 하나 그대로 유지할 것입니다.
58:43
그리고 이제는 f의 미분입니다. 우리는 단지 f 프라임을 가정 할 수 있습니다.
58:48
하나의 숫자 만 미분하고
58:50
이것을 f 프라임으로 정의 할 것입니다.
58:52
또한 하나의 숫자이기 때문에 아무런 해도 없습니다.
58:57
우리는 여전히 이 부분에 있는데
58:59
이제 Wij와 관련하여 zi의 미분을 취하고 싶습니다.
59:03
그럼 zi가 무엇인지 정의 해 보겠습니다. zi는 여기에 있습니다.
59:08
i 번째 행의 W와 b곱하기 x 더하기 i 번째 요소 b
59:15
그냥 zi를 정의로 바꿉시다.
59:20
지금까지 질문 있나요?
59:31
괜찮나요,
59:34
좋아요? 아닌가요?
59:38
그래서 f 프라임이 있으니까
59:43
이제 여기 내적 Wij의 미분이 있습니다.
59:48
다시 아주 조심스럽게 다시 쓸 수 있습니다.
59:51
내적은 이 행 곱하기 이 열 벡터에 합계에 불과합니다.
59:56
Wij를 미분할 때
60:02
다른 모든 Ws는 상수입니다.
60:04
그래서 모두 빼고 나면 기본적으로 xk 밖에 없습니다.
60:09
Wij와 합계에 실제로 나타나는 유일한 것은 xj이고
60:15
그래서 이 미분은 단지 Xj에 관한 것일 뿐입니다.
60:22
좋아요, 이 전체는
60:27
모든 용어에 대한 체인 규칙의 곱셈의 정의를 신중하게 정의한 것입니다.
60:33
그리고 이제 기본적으로, 우리가 하고자 하는 것은 이것을 조금 단순화하는 것입니다.
60:38
우리는 다른 부분을 재사용하고 싶을 수도 있습니다.
60:42
우리가 정의한 것처럼, 이 첫 번째 term은 실제로 부지수(subindices) i만을 사용합니다.
60:50
그리고 다른 하위 색인을 사용하지 않습니다.
60:52
단지 모든 다른 델타 I에 대해서
60:57
zi의 Uif 프라임을 정의할 것입니다.
61:00
첫 번째 표기법에서는 xj가 로컬 입력 신호입니다.
61:06
그리고 그 중 하나는 매우 도움이 되는데,
61:09
로지스틱 함수의 미분을 실제로 해 보는 것으로
61:15
원래 값의 terms에서 매우 편리하게 계산 할 수 있습니다.
61:20
여기서 f(z)나
61:22
각 요소의f(zi)는 항상 하나의 숫자로 기억해야 합니다.
61:27
아까 순전파 단계에서 계산했기 때문입니다.
61:30
따라서 이상적인 것은 계산하기 매우 빠른 은닉 활성화 함수를 사용하는 것입니다.
61:38
그리고 여기에서는 다른 지수나 다른 것은 계산할 필요가 없습니다.
61:41
우리는 이미 한 f(zi)를 다시 계산하지 않을 것인데
61:45
순전파 단계에서 계산했기 때문입니다.
61:48
네 그럼
61:49
이제 한 원소의 W와 관련하여 여기에서 편미분을 했습니다.
61:54
물론 전체 행렬에 대해 전체 경사도를 사용하고 싶습니다.
61:58
이제 의문은
62:04
이 행렬의 모든 다른 요소의 i와 xj로
62:08
입력에서 모든 다른 요소를 위한 입력으로 델타 i의 정의입니다.
62:12
전체 행렬 W에 대해 단일 경사도를 얻기 위해서
62:17
이 모든 다른 요소들을 결합할 수 있는 좋은 방법은 무엇일까요? 두 개의 벡터가 있는 경우 
62:29
(질문) 맞습니다.
62:30
그래서 본질적으로 델타 시간 x 전치(transpose)를 사용할 수 있습니다.
62:35
외적을 사용하여 모든 요소 i와 모든 요소 j의 모든 조합을 가져옵니다.
62:41
그리고 이것은 다시 마술처럼 것처럼 보일 수 있습니다.
62:44
하지만 여기에 외적의 정의를 다시 생각해 보고
62:48
그리고 모든 지표의 관점에서 그것을 써내면
62:53
정확히 아주 멋지고 아주 간단한 방정식을 찾을 수 있을 것입니다.
63:00
델타 term을 실제적으로 생각할 수 있습니다.
63:06
이제  전반적인 손실로부터 W의 이 층으로 도착하는 오류 신호를
63:11
마침내 그래프에 흐르게 할 것입니다.
63:15
그리고 그것은 결국
63:19
이 미분을 해야 하는 이 모든 과정을 실제로 거치지 않아도 될 것입니다.
63:20
그리고 그것을 소프트웨어 패키지로 할 수 있습니다.
63:23
하지만 이제 실제로 어떻게 작동하는지에 대해서 알게 되었을 것입니다.
63:37
(질문) 이제 이 외적 i와 j의 모든 요소를 ​​얻었습니다.
63:42
네.
63:43
그래서 델타 시간 x를 전치했을 때
63:47
여기 x는 보통 벡터입니다.
63:53
이제 올바른 표기법을 사용합시다.
63:57
그래서 W와 관련하여 미분을 할 것입니다.
64:03
W는 2x3 차원 행렬, 예를 들어 2x3입니다.
64:19
표기법을 매우 신중하게 쓰겠습니다.
64:24
2x3.
64:25
이제  w와 관련된 j의 미분은
64:31
결국, 2x3 행렬이 되어야 합니다.
64:34
델타*x를 전치하면
64:41
들어오는 치수와 정확히 일치하는 2 차원 델타가 생길 것입니다.
64:45
[안들림] 제가 언급 한 신호
64:49
우리가 가진 숨겨진 유닛의 수입니다.
64:51
이 1 차원, 기본적으로 행 벡터
64:56
xt는 1x3 차원의 벡터이며 이 값은 전치됩니다.
65:02
이게 무슨 뜻이냐 하면
65:05
곱해져서 표준 행렬 곱셈이 된다는 것입니다.
65:10
우리는 그것을 써야 합니다.
65:14
아직 미분을 하지 않은 마지막 term 입니다. [안들림] 
65:20
우리 
bi이고 결국에는 매우 유사해 질 것 입니다.
65:24
우리는 그것을 통과 할 것입니다.
65:26
여기서 Ui를 빼낼 수 있습니다. 우리는 프라임을 취할 것입니다.
65:30
자, 이제 이것은 델타 i입니다.
65:33
매우 비슷한 것을 관찰을 할 수 있을 것입니다.
65:35
이들은 bi와 매우 유사한 단계입니다.
65:37
그러나 결국 우리는 이 term을 끝내고
65:40
다 하나가 될 것입니다.
65:41
그리고 여기, bi 요소의 미분,
65:45
b의 업데이트를 위한 전체 경사도는
65:50
델타 i이고 델타의 모든 요소를 ​​다시 사용할 수 있습니다.
65:58
질문 있나요?
66:04
본질적으로는 이것이 역전파입니다.
66:10
미분해서 체인 규칙을 사용했습니다.
66:13
그리고 첫 번째로, 제가 전체를 알고 나니까
66:15
딥러닝의 마술과 같이 많은 것이 많이 분명 해지고 있습니다.
66:20
우리는 방금 미분을 했고 목적 함수를 가지고 있고
66:24
미분을 기반으로, 이러한 큰 기능의 모든 파라미터를 업데이트했습니다.
66:28
이제 가장 중요한 트릭은 하위 레이어에서 계산한 미분을.
66:32
상위 계층 미분으로 다시 사용하는 것입니다.
66:36
이것은 매우 많은 효율적인 트릭입니다.
66:39
우리는 그것을 사용할 수 없으며 단지 매우 비효율적일 것입니다.
66:43
하지만 여기에 주요 통찰력이 있는 것으로
66:46
왜 우리는 미분을 역전파로 재 명명했는지에 대해 알 수 있습니다.
66:51
그렇다면 우리가 취해야 할 마지막 미분은 무엇입니까?
66:55
이 모델의 경우에도 워드 벡터의 관점에서 볼 수 있습니다.
66:59
그럼 모든 것들을 살펴봅시다.
67:01
기본적으로, 우리는 점수의 미분을
67:06
워드 벡터의 모든 요소.
67:09
다시 한 번 우리는 모든 것을 하나의 원도우로 연결했습니다.
67:14
그리고 지금, 여기의 문제는
67:16
각 워드 벡터는 실제로 이 두 용어 모두에 나타날 수 있다는 것입니다.
67:21
그리고 두 숨겨진 유닛 모두 입력 요소를 모두 사용합니다.
67:27
따라서 우리는 단지 하나의 요소만 볼 수는 없습니다.
67:30
여기에 단순화 된 경우의 활성화 단위 모두를 합산해야 합니다.
67:37
여기에는 두 개의 숨겨진 유닛과 3 차원 입력이 있습니다.
67:41
좀 더 간단하게 유지하고 표기법을 적어 봅시다.
67:44
그럼 여기서부터 시작하겠습니다.
67:47
두 가지 활성화에 대해 미분을 해야 합니다.
67:52
그리고 이제 비슷한 단계를 밟을 것입니다.
67:55
여기 있는 s 를
67:56
u활성화로 정의했습니다.
67:59
그것은 단지 Ui였습니다. 그러면 ai는 f(w)와 같은 것입니다.
68:08
이제 우리는 이렇게 모든 유사한 단계를 다시 거치면서 관찰할 것입니다.
68:12
즉, 이전에 재사용했던 동일한 term을 실제로 볼 수 있습니다.
68:19
Zi (Ui x F프라임)입니다.
68:25
이것은
68:27
우리가 여기서 본 것과 정확히 동일합니다.
68:28
Zi(F 프라임)이죠.
68:32
이것이 의미하는 것은 같은 델타를 재사용 할 수 있다는 것입니다.
68:35
그리고 그것은 정말로 큰 통찰력 중 하나입니다.
68:38
상당히 사소하지만 매우 흥미로우며, 훨씬 더 빠릅니다.
68:42
그러나, 지금도 여전히 다른 점이 있습니다.
68:43
당연히 미분을 합니다.
68:46
이들 각각에, Xj의 내적과
68:50
바이어스 term을 버렸었는데, 미분을 하면, 상수일 뿐이기 때문입니다.
68:55
여기 또한, Xj는 단지 내적일 뿐입니다.
68:59
이 행렬 W의 j 번째 요소는 다음과 같습니다.
69:03
이 내적에 미분을 합니다.
69:07
합을 했고, 이제 이 합의 까다로운 부분을
69:12
단순화하여 매트릭스를 보다 단순하게 만듭시다.
69:17
왜냐하면 다시, 역전파에서
69:22
여기서 이전 오류 신호와 그 미분 요소를 재사용해야 하기 때문입니다.
69:29
지금, 가장 간단한 것은, 우리가 이 합계를 할 때 여기서 관찰 할 첫 번째 것은,
69:33
그 합은 실제로 j 번째 열을 취하는 간단한 내적입니다.
69:39
이  dot은 첫 번째 표기법을 시작할 때의 dot notation이고
69:43
그 다음에 우리는 행을 취하고 여기서 그 열을 취합니다.
69:46
이것이 열벡터입니다.
69:47
물론 그것을 전치합니다, 그래서
69:48
우리에게 하나의 번호를 부여하는 간단한 내적입니다.
69:52
워드 벡터와 워드 원도우 요소의 미분.
69:58
예.
70:13
(질문) 좋은 질문입니다.
70:13
그래서 이 모든 다른 변수에 미분을 할 때
70:16
업데이트하는 순서는 무엇입니까?
70:19
실제로 이것을 모두 병렬로 업데이트하는 순서는 없습니다.
70:22
변수의 모든 요소에서 한 걸음을 내딛거나
70:26
또는 파라미터를 볼 수 있을 뿐입니다.
70:29
그 복잡성은 표준 기계 학습에서 볼 수 있습니다.
70:33
표준 로지스틱 회귀와 같은 많은 모델에서,
70:35
모든 예제에서 W와 같은 모든 파라미터를 볼 수 있습니다.
70:38
그리고  이것은 좀 더 복잡합니다.
70:42
특정 원도우에서 안에서 볼 단어를 업데이트하기 때문입니다.
70:47
그리고 만약 다른 모든 것들을 추측한다면, 우리는 단지 매우 큰, 아주 큰,
70:51
아주 희소 업데이트이고 RAM에서는 효율적이지 않은 좋은 질문입니다.
70:57
여기서 간단한 곱셈을 했고
71:00
합계는 그냥 내적입니다.
71:03
지금까지 간단하게, 
71:06
2 차원 D 차원 벡터를 살펴보았는데
71:07
결국, 두 요소를 합한 것입니다.
71:10
지금까지는 괜찮습니다.
71:11
이제, 정말로, 모든 것에 관해 완전한 경사도를 얻고 싶습니다.
71:18
J에 대한 XJ는 1 ~ 3이고 간단한 경우입니다.
71:23
다섯 단어의 큰 원도우가 있다면 5D입니다.
71:28
그럼 단일 요소를 어떻게 결합해서
71:34
모든 xij에 대해 모든 다른 경사도를 제공하는 벡터로 만들 수 있을까요? 
71:41
그리고 j=1일 때 그럼 누가 이렇게 가깝게 원도우를 구한 사람 있나요?
71:54
(질문) 맞습니다.
71:55
W를 전치한 델타입니다.
71:56
잘 했어요.
71:58
근본적으로 최종 미분과 최종 기울기는 여기에 있습니다.
72:04
점수는 전체 원도우와 관련하여 단지 W를 전치한 것 곱하기 델타입니다.
72:10
매우 간단하고 빠르게 구현할 수 있고, 쉽게 벡터로 만드는 방법입니다.
72:15
여러 원도우에서 여러 델타를 연결하여 다시 이 작업을 수행 할 수 있습니다.
72:19
그리고 그것은 구현되고 미분 된 것처럼 매우 효율적일 수 있습니다.
72:24
좋습니다. 오류 메시지는 이 은닉 레이어에 도착한 델타로
72:27
숨겨진 레이어와 동일한 차원을 가지고 있습니다.
72:31
왜냐하면 모든 원도우문을  업데이트했기 때문입니다.
72:32
그리고 이전 슬라이드에서 윈도우를 업데이트 한다는 것의
72:36
의미는 지금 그 최종 경사도를 줄여서
72:40
해당 원도우에서 각 특정 단어에 대한 다른 청크로 바꾸어서
72:44
첫 번째 대규모 신경망을 업데이트하는 방법에 대한 것이었습니다.
72:48
그럼 이 모든 것을 다시 합시다.
72:52
이 완전한 목적 함수는 max이고
72:58
identity가 있으므로 0보다 큰 것으로 가정할 수 있습니다.
73:02
따라서 이것은 단순한 지시자 함수인 경우가 될 것입니다.
73:05
표시는 true면 1이고 그렇지 않으면 0이 될 것입니다.
73:09
그 다음에 여러분은 참과
73:15
거짓인 원도우 x 및 xc를 무시할 수 있을 것입니다.
73:18
그래서 이 마지막 경사도의
73:21
최대 마진 함수는 본질적으로 이러한 방식으로 구현됩니다.
73:26
이 모든 것들을 매우 효율적으로 곱할 수 있습니다.
73:35
이해되나요.
73:41
이것이 바로 저것이지만 맞지 않습니다.
73:43
이것은 [안들림] 하지만 우리는 여전히 ​​여기에서 미분을 해야만 합니다.
73:46
그러나 기본적으로 이 지시 함수는 우리가 아직 보지 못했던 주요한 진기함입니다.
73:50
이해되나요.
73:54
네.
74:00
>> [안들림]
74:35
>> 네, 긴 질문이네요.
74:36
문제의 요지는 우리가 어떻게 최적 상태인지 확인할 수 있느냐 하는 것이었습니다.
74:42
그리고 일부는 이미 했습니다.
74:46
어쨌든 확률적으로 업데이트를 계속하면 빠져나가지 못하는 것은 어렵습니다.
74:50
사실, 작을수록 더 확률적으로 됩니다.
74:55
더 적은 수의 원도우에서 업데이트를 할 때는
74:59
(지역해에) 빠질 확률이 더 적습니다.
75:00
우리는 모든 원도우를 통해 결과를 얻고 나서 거대한 갱신을 만들려고 노력했다면,
75:04
그래서 실제로는 매우 비효율적이며 (지역해에) 빠질 확률이 훨씬 큽니다.
75:08
그 다음에 천천히 통과한다는 이론이 많은데
75:11
이 클래스에서 거기까지 다룰 수는 없습니다.
75:15
그것은 많은 지역해들이 상당히 좋다고 밝혀져 있습니다.
75:19
많은 경우에,
75:20
심지어는 전역해( global optima)가 되는 것도 생각과 거리가 있을 수 있습니다.
75:24
앞으로 여기에, 많은 시간을 보내면서 여러 차례 살펴볼 것입니다.
75:29
그러나 실제로 프로젝트 조언은 받으면 완전한 fit일 수도 있습니다.
75:32
우리는 충분히 강력한 신경망 모델을 가지고 있습니다.
75:34
입력 데이터와 학습 데이터 세트를 완벽하게 맞출 수 있습니다.
75:39
실제로 결국 대부분의 시간을 
75:42
모델을 더 자주 그리고 자주, 적어도 더 많은 확률로 정규화 하는데 보내게 될 텐데,
75:47
그 중 일부는 통과하는 것이 나을 것입니다.
75:49
(질문) 좋은 질문입니다.
75:50
그래, 결국, 한꺼번에 업데이트가 하면 되니까 매우 간단합니다.
75:55
좋아요, 그럼 요약해 봅시다.
75:56
이것은 꽤 서사적인 강의였습니다.
75:58
잘 따라와 준 것을
76:01
다시 한 번 축하드리며, 이것은  매우 유용한 기본 구성 요소 강의였습니다.
76:06
그리고 이제 이 원도우 모델은
76:09
관찰하고 연습할 수도 있고 실제로 구현하려고 할 수도 있습니다.
76:12
실제 생활 환경에서 가장 중요한 원도우 모델입니다.
76:13
요약하면,
76:14
우리는 워드 벡터 훈련을 배웠고 Windows를 결합하는 방법을 배웠습니다.
76:18
우리는 softmax와 cross entropy error를 가지고 있습니다.
76:21
우리는 그곳의 세부 사항 중 일부를 살펴보았습니다.
76:23
점수와  max margin loss 그리고 신경망을 배웠고
76:26
이 두 단계는 문제 해결을 위해 다르게 결합해야 합니다.
76:31
일번과 특히 이번 
76:33
힘든 수학 수업 하나를 더 했고
76:36
이제 재미있게 모든 것을 결합해 보도록 합시다.
76:39
감사합니다.