네 여러분
00:11
준비되었나요?
00:12
CS224N에 오신 것을 환영합니다.
00:20
놀랍네요.
00:22
여기서 참여하고 있는 모든 사람들, 그리고 여기 있지 않은 분들께도 감사드립니다.
00:27
이 곳과 SCPD에서 온라인으로 보고 있는 여러분.
00:32
이 수업을 듣기 위해 등록한 사람들의 숫자를 보고 놀랐습니다.
00:37
그래서 어떤 의미에서는 특별히 알려　드릴 필요가 없는 것처럼 보이네요.
00:42
왜 자연 언어 처리 과정과
00:45
딥러닝의 조합이 좋은지 말입니다.
00:48
그렇지만, 오늘은 수업 소개를 하는 날이죠?.
00:53
일단, 제 이름은 크리스토퍼 매닝입니다.
00:57
이제 앞으로 무엇을 할 것인지를 알려　드리는　것으로　시작하겠습니다.
01:02
자연어 처리가 무엇인지, 그리고 딥러닝이 무엇인지에 관한 것들과,
01:07
이 코스의 진행 계획을 알려 주는데 몇 분을 쓰도록　하겠습니다.
01:12
그리고 공동 강사 리차드로부터도 몇 마디를 듣고.
01:16
그런 다음 왜 딥러닝 자연어 처리가 어려운지를 알아보고
01:20
그리고 나서 딥러닝 자연어처리에 대해 소개하도록 하겠습니다.
01:26
오늘 시작하기 전에 좀 힘들었는데요.
01:29
화재 경보 때문에 약 10 분 정도 늦게 시작한 것 같네요.
01:34
다행히도 이 첫 번째 강의에서는 어려운 내용은 별로 없습니다.
01:39
이 첫 번째 강의는 자연어처리 클래스가 무엇인지 설명하는 것이고 또
01:44
어떻게 그리고 왜 딥러닝이 세상을 변화시키고 있는지에 대한 동기 부여적 내용들입니다.
01:48
목요일 강의부터 바로 변경되는 것들이 있는데.
01:53
목요일 강의를 일종의 벡터로부터 시작한다는 것입니다.
01:57
그리고 거기서 파생된 연쇄 규칙(chain rule)과 같은 모든 것들을 다룰 것입니다.
02:00
그래서 여러분들이
02:02
두 강의 사이의 수준의 변화에 대해서　일종의　마음의 준비를 하시기를 바랍니다.
02:06
자, 자연어처리란 무엇입니까?
02:10
자연어처리는 일련의 컴퓨터 과학자들을 지칭합니다.
02:15
이 필드에서는 말이죠.
02:16
그리고 본질적으로는 전산 언어학과 동의어입니다.
02:19
이 필드는 언어학의 일종이고.
02:21
컴퓨터 과학과 언어학과
02:26
인공 지능의 교차점에 있습니다.
02:27
우리가 하려는 일은 컴퓨터를 영리하게 하는 것으로
02:31
인간의 언어를 이해하고
02:36
인간이 하는 방식으로 인간 언어로 표현하도록 하는 것입니다.
02:41
따라서 자연 언어 처리는 인공 지능의 일부로 간주됩니다.
02:46
그리고 인공 지능의 다른 중요한 부분이 분명히 있습니다.
02:50
컴퓨터 비전 수행 및 로봇 공학,
02:52
지식 표현, 추론 등이 그것인데.
02:55
그러나 언어 역시 인공 지능의 아주 특별한 부분을 차지하고 있습니다.
03:02
그 이유는 언어가 매우 독특한 특성이 있기 때문입니다.
03:07
우리는 언어를 통해서 생각하고 세상에 관해서도 많이 생각합니다.
03:12
우리 행성의 수많은 생물들은 꽤 훌륭한 비전 시스템을 가지고 있지만.
03:17
언어를 사용하는 것은 사람뿐입니다.
03:20
그리고 우리가 우리의 생각을 표현하거나 어떻게 할 것인지에 대해 표현할 때,
03:24
언어는 사고에 도구이자 의사소통의 도구가 됩니다.
03:28
따라서 인공지능의 핵심 기술 중 하나로 생각하게 되었고.
03:31
우리는 지금 이런 관점에서 공부할 것입니다.
03:35
또한　이 수업의 목표는 컴퓨터가 （인간의　언어를）처리 할 수 있도록 하는 방법이 될 것입니다.
03:39
과제를 수행하기 위해서는 인간 언어를 이해하는 것은 유용한 작업일 수 있습니다.
03:44
그것은 약속을 하거나 물건을 사거나 하는 일일 수도 있습니다.
03:49
또는 세계의 상태를 이해하는 것과 같은 더 높은 목표가 될 수도 있습니다.
03:54
그리고 여기는 거대한 규모의 상업이 시작되는 공간이기도 합니다.
04:00
약속을 잡는 것과 같은 다양한 방향으로의 활동,
04:05
많은 부분은 묻고 대답하는 것과 관련되어 있기도 합니다.
04:07
다행히도, 언어를 사용하는 사람들에게 모바일은 이제 막 굉장해 지고 있습니다.
04:14
언어의 가치는 매우 친근한 것이 되었고 매우 중요해 졌습니다.
04:20
그리고 이제는 모든 거대한 기술 회사가 Siri이든,
04:23
Google Assistant, Facebook 및 Cortana 등이
04:26
맹렬히 하고 있는 일은
04:30
자연어를 사용하는 제품으로 사용자와 소통하는 것입니다.
04:35
그리고 이것은 매우 강력한 일입니다.
04:38
휴대 전화는 작기 때문에 매우 매력적입니다.
04:42
하지만, 작은 키보드는 입력하기가 정말로 어렵습니다.
04:46
그리고 어떤 사람들은 문자 메시지를 할 때 매우 빠릅니다.
04:51
그러나 다른 많은 사람들은 그렇지 못합니다. 
04:55
한자를 입력하는 것은 훨씬 어렵습니다.
04:58
영어로 타이핑해야 한다면 더 그렇겠죠. 
04:59
노인이라면 훨씬 어려울 것입니다.
05:02
문자 이해력이 낮아도 힘들 것입니다.
05:05
그러나 이제 새로운 시각이 열리고 있습니다.
05:09
아마존은 Alexa와 함께 놀라운 성공을 거두었습니다.
05:13
우리의 주변 환경에 있는 유틸리티 장치에
05:18
이야기를 거는 것으로 의사소통을 할 수 있습니다.
05:21
애플에 간단한 외치면 되죠! 나는 정말로,
05:24
애플의 시리 출시에 감사해야 한다고 생각합니다.
05:27
근본적으로 애플이 배팅한 것은
05:32
인간의 언어를 소비자 기술로 바꾸는 것입니다.
05:35
다른 회사들도 이 군비 경주에 진입해 있습니다.
05:40
저는 느슨하게 의미라고 말했지만
05:44
우리가 더 많이 이야기 할 것 중 하나인 의미는 복합체의 일종이고,
05:51
힘든 일이고 완전히 의미를 이해하는 것이 무엇을 의미하는지는 알기가 어렵습니다.
05:56
어쨌든 사람들이 완벽한 인공지능(AI-complete)을 언급하는 것은 확실히 어려운 목표입니다.
06:01
그것은 세계에 대한 모든 형태의 이해를 포함합니다.
06:04
그래서 우리가 의미를 이해하는데 많은 시간을 보내게 됩니다.
06:07
우리가 반쯤이라도 의미를 이해한다면 행복할 것이기 때문에.
06:10
우리는 그것을 할 수 있는 다른 방법에 대해서도 이야기 할 것입니다.
06:15
좋아요, 여러 분들이 이 수업에서 들을 수 있는 것은
06:20
많은 것들이 있겠지만 인간 언어에 대한 감사와
06:25
언어에서 레벨이 의미하는 것은 무엇이며 어떻게 처리 되는지 등입니다.
06:29
당연히 엄청난 양의 것들을 여기서 다 다룰 수는 없고 좀 더 배우기를 원한다면.
06:32
많은 것을 배우기 위해서
06:33
언어학과에서 수강 할 수 있는 수업이 많이 있습니다.
06:36
거기서 더 많이 배울 수 있습니다.
06:37
우리는 적어도 할 수 있는 한 높은 수준의 도달할 수 있기를 진심으로 바랍니다.
06:40
자연어 이해에 대해서는 말입니다.
06:41
사람들이 전통적으로 그려온 큰 그림에서 보면.
06:45
언어의 레벨에서
06:47
처음에 입력이 있을 것입니다.
06:49
그리고 그 입력은 일반적으로 대화일 것입니다.
06:52
그런 다음에 음성을 다루고
06:53
그 음성을 이해하기 위해서 음운론적 분석을 할 것입니다.
06:57
비록, 그것이 일반적으로 텍스트라고 하더라도
06:59
수행해야 할 몇 가지 처리가 있습니다.
07:02
언어학적인 견지에서 주변부에 있기는 하지만, OCR이나,
07:06
단어의 토큰화 작업과 같은
07:08
일련의 처리 단계를 거쳐야 합니다.
07:12
우리가 이해할 수 없는 어떤 복합 단어의 의미를 알아야 한다고 생각해 봅시다,
07:16
그 단어의 끝에는  -ible이 붙어 있습니다.
07:19
이 경우는 단어에 대한 일종의 형태소 분석이 시도 될 것입니다.,
07:22
우리는 이러한 시도를 해 보고나서
07:23
그 다음에 구문 분석이라고 불리는 것을 통해 문장의 구조를 이해하려고 할 것입니다.
07:26
그래서 'I sat on the bench'와 같은 문장이 있다면,
07:31
'I'는 'sat' 라는 동사의 주어이고 'on the bench'는 그 위치를 나타내게 된다는 것을 알게 될 것입니다.
07:37
그 다음에는 의미론적 이해를 시도할 것입니다.
07:40
의미론적 해석은 문장의 의미를 알아내는 것입니다.
07:45
그러나 단순히 문장의 의미를 아는 것만은 아닙니다.
07:49
실제로 인간 언어를 충분히 이해하는 것입니다.
07:53
언어가 사용되는 맥락에 따라 많은 정보가 전달됩니다.
07:58
그래서 화용론과 담화 처리와 같은 영역으로 이끌 것입니다.
08:03
결국, 이 수업의 대부분의 시간을 우리는,
08:08
구문 분석과 의미 해석의 중간쯤에서 쓰게 될 것입니다.
08:12
자연어 처리 수업이기 때문에.
08:16
진심으로 토론하게 될 부분 증 하나는
08:21
음성 신호 분석이 될 것입니다.
08:23
흥미롭게도, 실제로 이 부분이 딥러닝이 이루어지는 첫 번째 장소였습니다.
08:28
그리고 정말로 인간의 언어와 관련된 작업에 매우 유용하다는 것이 증명 되었습니다.
08:34
그래서 자연 언어 처리의 응용 프로그램은.
08:39
정말 빠르게 넓게 퍼져 나가고 있습니다.
08:42
우리는 매일 매일 여러 가지 응용 프로그램을 사용하고 있는데,
08:45
그것은 자연 언어 처리를 이용한 것들입니다.
08:47
그리고 그 스펙트럼이 매우 다양해서
08:49
매우 단순한 것에서 훨씬 복잡한 것까지 있습니다.
08:54
낮은 수준에서는 맞춤법 검사와 같은 것이 있고
08:58
휴대 전화에서 일종의 자동 완성 기능을 하는 것과 같은 것도 있습니다.
09:01
이것은 일종의 원시적인 언어 이해 작업의 결과입니다.
09:05
여러분이 다양하게, 웹 검색을 할 때도,
09:08
검색 엔진은 동의어와 같은 것을 고려하여 처리하고 있습니다.
09:12
그리고 이것은 언어 이해의 과제이기도 합니다.
09:17
하지만 우리가 더 관심을 가지고 있는 것 때문에
09:20
우리를 언어 이해의 계산을 보다 복잡한 작업으로 밀어 넣고 있습니다.
09:25
이를 통해 우리가 실제로 하고 싶어 하는 것은 다음 수준의 일들이 될 것입니다.
09:29
컴퓨터는 웹 사이트에 있는 신문 등의 텍스트 정보 같은 것을 통해
09:34
그리고 실제적인 텍스트를 충분히 이해할 수 있도록 정보를 얻고 싶을 것입니다.
09:37
사람들이 이야기 하는 것을 어느 정도 이해하고 싶어 하는 것일 수도 있고,
09:41
거기서 특정 정보를 기대하는 것과 같을 것일 수도 있습니다.
09:45
제품과 가격 또는 사람들과 그들의 직업 같은 것들이나.
09:51
또는 문서를 이해하는 것과 관련된 다른 작업을 수행하고 싶어 할 수도 있습니다.
09:55
독서 수준과 관련된 것이거나 목표로 하는 청중과 관련된 작업을 하는 것,
09:59
가령, 어떤 트윗이 긍정적인 것을 말하고 있는지 또는
10:02
어떤 사람, 회사, 밴드 또는 여러 가지 것들에 대해 부정적인지와 같은 것들입니다.
10:06
그리고 그보다 더 높은 수준으로 나아가자면, 우리는 컴퓨터가
10:11
전체 레벨의 언어 이해 작업을 완료하는 것을 원할 것입니다.
10:17
그리고 우리가 이야기할 탁월한 과제 중에는
10:21
한 언어에서 다른 언어로의 기계 번역이나
10:25
대화형 대화 시스템을 구축하여 컴퓨터와 채팅할 때
10:29
인간과 마찬가지로 자연스러운 대화를 나누기를 원할 것입니다.
10:33
또는 실제로 세계에 대한 지식을 활용할 수 있는 컴퓨터가 사용한다면,
10:38
위키피디아 및 기타 여러 곳에서 사용 가능할 것입니다.
10:42
그래서 실제로 지능을 요하는 질문에 대답할 수 있을 것입니다.
10:46
이제 우리는 인간이 할 수 있는 모든 것을 아는 것과 마찬가지로
10:49
좋아요, 실제로 사용되는 많은 것들이 보이기 시작할 것입니다.
10:54
산업에서는 정기적으로.
10:56
따라서 작게 본다면, 검색을 할 때마다,
11시
자연 언어 처리 및 자연 언어 이해가 발생한다고 할 수 있습니다.
11:04
그래서 만약 여러분이 단어의 어미인 부분까지 입력하고 있다면,
11:07
검색 엔진은 거기서 벗어나 써칭하는 것을 고려하고 있을 것입니다.
11시 11 분
만약 맞춤법 오류가 있다면 수정할 것입니다.
11:13 
동의어와 같은 것들도 고려되고 있습니다.
11:15
이것은 광고를 검색 할 때도 마찬가지입니다.
11:19
그러나 정말 흥미로운 점은 이제 우리가
11:24
상업적으로 성공한 자연 언어 처리의 더 큰 응용을 하고 있다는 것입니다.
11시 29 분
지난 몇 년 동안 놀라운 일이 있었습니다.
11:31
나중에 다시 살펴 보겠지만 기계 번역의 놀라운 발전이 있었습니다.
11:36
또 음성 인식에 있어서도 놀라운 진보가 있어서 우리는 지금
11:41
휴대 전화에서도 음성 인식 성능이 크게 향상 되어 있습니다.
11:46
감정 분석과 같은 제품은 상업적으로 크게 성장해서
11:49
점점 더 중요해지고 있습니다.
11:51
좋아하는 산업에 따라 다르지만 월스트리트 저널이 있다면,
11:55
매 시간마다 뉴스 기사를 찾아 검색하는 회사는
11:59
구매 및 판매 결정을 내리는 회사에 대한 정서를 살피고 있을 것입니다.
12:02
그리고 최근에는 지난 12 개월 동안
12:05
채팅 로봇을 만드는 방법에 관심이 커지면서
12:10
모든 종류의 인터페이스 작업을 위한 대화형 에이전트가
12:13
거대한 새로운 산업으로 성장하고 있는 것처럼 보입니다.
12:17
네, 우리는 벌써 뒤쳐져 있네요.
12:21
그래서 몇 분 후에는,
12:24
딥러닝과 같은 것들을 이야기할 것입니다.
12:28
그러나 그것에 들어가기 전에,
12:30
인간 언어에 특별한 점에 대해 잠깐 말하겠습니다.
12:35
우리는 이것으로부터 되돌아올 것이지만,
12:37
처음에 올바른 감각을 갖는 것이 흥미로울 것이라고 생각합니다.
12:41
다른 것들과 언어 사이에는 중요한 차이가 있습니다.
12:46
사람들이 신호 처리를 할 때 생각하는 대부분의 다른 종류의 것들이나
12:51
또는 데이터 마이닝과 이러한 종류의 모든 것들을 포함하여 보았을 때,
12:56
대부분 세계 어느 쪽인가에 데이터가 있습니다.
13:03
그리고 어떤 종류의 것이든 그것은 시각적 시스템을 가지고 있습니다.
13:08
누군가가 지역 Safeway에서 제품을 구매할 때도 마찬가지입니다.
13:13
어떤 사람들이 영업 로그를 집어 들고,
13:16
분석 결과 무엇을 찾았는지를 볼 것입니다, 맞나요?
13:18
그래서 이 모든 무작위 데이터를
13:21
누군가는 이해하려고 노력할 것입니다.
13:23
근본적으로 인간 언어는 그런 것이 아닙니다.
13:27
인간 언어는 여러분이 시도하고 있는 것과 같은 엄청난 양의 데이터 배출이 아닙니다.
13:30
프로세스가 유용할 수도 있는데,
13:32
인간 언어는 거의 항상
13:35
사람이 실제로 의사소통하기를 원하는 정보가 있고,
13:40
그것을 전하기 위해 메시지를 만들었습니다.
13:43
그리고 그 정보를 다른 사람에게 제공합니다.
13:45
따라서 실제로는 특정 유형의 
13:50
메시지를 전송하는 의도적인 형식입니다.
13:52
인간 언어에 관한 놀라운 사실은 이것이 매우 복잡한
13:57
시스템이라는 것입니다. 두
13:59
세 살이나 네 살짜리 아이들이 놀랍게도 몇 마디 정도는 사용할 수 있습니다.
14:04
거기에는 뭔가 좋은 일이 벌어지고 있네요.
14:06
언어의 또 다른 흥미로운 특성은 실제로 언어가
14:11
이산적, 상징적 또는 범주적 신호 시스템이라고 부를 수 있는 것이나
14:16
로켓이나 바이올린과 같은 개념적 단어를 포함하고 있다는 사실입니다.
14:21
기본적으로 우리는 상징을 통해 다른 사람들과 소통하고 있습니다.
14:25
표현된 신호에는 약간의 예외가 있습니다.
14:29
우리가 I love it과 I LOVE it을 구별 할 수 있는 것은
14:33
더 강하게 들리기 때문입니다.
14:34
하지만 의미를 전달하기 위해 99 %는 기호를 사용합니다.
14:40
그래서 일종의 EE 정보 이론에서 의미가 도출될 수 있습니다.
14:45
기호를 통해서,
14:47
어느 정도 거리가 있어도 안정적으로 신호를 보낼 수 있다는 것은
14:52
매우 신뢰할 수 있는 장치입니다. 우리가 알아야 할 중요한 것은
14:54
언어는 기호라는 것입니다.
14:55
따라서 기호가 단순한 논리 또는 고전적인 AI의 발명이 아니라,
15:01
그 이상으로 나아가고자 한다면,
15:03
실제로 재미있는 일이 일어나게 됩니다.
15:05
그래서 우리가 언어로 의사소통 할 때
15:09
소통하고자 하는 내용은 상징으로 표현이 됩니다.
15:13
그들이 상징을 전달하는 방식은 연속적인 기판을 사용하는 것과 같습니다.
15:19
그리고 언어에 대한 정말 흥미로운 점은 바로 사람입니다.
15:22
사람은 서로 다른 연속 기질을 사용하여 정확히 동일한 메시지를 전달할 수 있습니다.
15:27
일반적으로 음성을 사용하기 때문에 오디오 웨이브가 생깁니다.
15:31
종이 조각 위에 적으면 비전 문제와 같은 것이 생기고
15:36
수화로 의사소통을 할 수도 있는데,
15:38
그것은 다른 종류의 연속적인 기판이 될 것입니다.
15:41
이들 모두를 사용할 수도 있습니다.
15:43
그러나 이러한 다양한 인코딩에 모두 상징체계가 존재합니다.
15:48
결국, 통신 매체를 통해 연속적으로 이루어진
15:55
상징체계가 인간의 언어입니다.
15:59
그리고 더 흥미로운 부분은 그 이후에 일어나는 일들입니다.
16:03
철학과 역사의 대부분의 지배적인 생각이나
16:08
과학과 인공 지능은 일종의
16:13
언어의 상징체계를 우리의 뇌에 투영하는 것이었습니다.
16:17
하지만 상징적인 프로세스로서의 사고는 
16:20
실제로는 두뇌에 기초한 것처럼 보이지 않습니다.
16:24
우리가 두뇌에 대해 알고 있는 모든 것은 그들이 완전히
16:27
연속 시스템이라는 것입니다.
16:30
그래서 이 일에서 딥러닝을 통해 나온 흥미로운 생각은
16:35
우리의 뇌 역시
16:39
지속적인 활성화 패턴을 가지고 있다고 생각된다는 것입니다.
16:43
그리고 우리가 가지고 있는 그림은 우리가 언어를 사용할 때마다 계속해서
16:48
상징적으로 변한다는 것입니다
16:51
이것은 흥미로운 점입니다.
16:52
또한 언어 이해의 문제 중 하나를 지적되는 것으로,
16:57
많은 시간에 걸쳐
16:59
언어에는 거대한 어휘가 생성되었습니다.
17:03
따라서 언어에는 수만 단어가 있습니다.
17:07
그리고 실제로, 거대한 과학 어휘가 있는 영어와 같은 언어는,
17:11
거기에 수십만 단어가 있습니다.
17:14
물론, 세는 방법에 따라 달라질 수 있습니다.
17:15
형태론적으로 세기 시작하면,
17:18
언어에는 생산적 형태를 가진 무한에 가까운 단어가 있다고 주장할 수 있을 것입니다.
17:22
이것을 계산하게 되면 우리는 바로 희소성(sparsity)의 문제에 부딪히게 될 것입니다.
17:26
이것은 우리가 처리해야 할 큰 문제 중 하나입니다.
17:30
자, 이제 기어를 바꿔서 딥러닝에 대한 약간의 소개를 해 보겠습니다.
17:36
딥러닝은 지난 10 년 동안 이 분야에서 크게 각광 받기 시작했고.
17:43
엄청나게 커졌습니다.
17:45
그동안 딥러닝이 얼마나 성공적이었으며, 어떻게 확대되어 왔는지 매우 흥미롭게 보아 왔습니다.
17:50
정말, 지금은 테크 뉴스에서 매월 볼 수 있습니다.
17:55
놀랄만한 새로운 성능 개선이 딥러닝 분야에서 나오고 있습니다.
18:00
지난달에는 슈퍼 휴먼 컴퓨터 비전 시스템이,
18:04
다음 달에는 기계 번역이 크게 향상되었습니다.
18:07
또 한 달이 지나고 나면 컴퓨터가
18:11
믿을 수 없을 만큼 현실적으로 자신만의 예술을 만들어내고 있습니다.
18:14
그 다음 달에는
18:15
사람들은 놀랄 정도로 생생하게 들리는 새로운 텍스트 음성 변환 시스템을 제작되고 있습니다.
18:20
이 모든 엄청난 진보의 원동력은 무엇입니까?
18:23
이 모든 것을 근간으로 하는 것은 또 무엇입니까?
18:26
그 출발점에는, 기계 학습의 일부로서의 딥러닝이 있습니다.
18:31
그래서 일반적으로, 컴퓨터가 자동으로 무언가를 배우도록 하는 방법으로
18:36
이전에는 말로 하거나 손으로 코딩하는 것과 같은
18:41
전통적인 컴퓨터 프로그램으로 우리가 해왔던 것들입니다.
18:47
그러나 딥러닝은 또한
18:52
80 년대, 90 년대, 00 년대의 기계 학습에서 일어난 일과 다릅니다.
18:56
그리고 그 핵심적인 차이점은 전통적인 기계 학습의 대부분이
19:01
제가 부른다면.
19:02
의사 결정 나무, 로지스틱 회귀,
19:07
나이브 베이즈, 서포트 벡터 머신등이 있고, 그리고 이러한 것들로
19:11
본질적으로 우리가 일을 해 왔던 방법은,
19:15
특정한 문제에 처해 있는 사람을 신중하게 바라보고.
19:19
그 문제에 주요한 것들을 해결하는 것이었습니다.
19:23
그런 다음 유용한 자질들이 될 수 있는 것을 디자인했습니다.
19:29
즉, 손으로 인코딩 할 문제를 처리해 왔습니다.
19:34
보통 파이선 코드를 작성하거나
19:36
그러한 특징을 인식 할 수 있는 그런 것.
19:39
오른 쪽에 있는 작게 써 져 있어서 읽을 수 있는 것들처럼
19:44
개체명 인식 시스템을 위한 몇 가지 기능과 같은 것이 될 것입니다.
19:47
텍스트에서 사람 이름, 회사 이름 등을 찾는 것으로
19:51
이것은 제가 작성한 시스템의 일종입니다.
19:54
어떤 단어가 회사 이름인지 여부를 알고 싶다면 보기를 원합니다.
19:57
대문자로 표시했는지의 여부와 상관없이 해당 기능이 작동합니다.
20:01
왼쪽에 있는 단어를 보고
20:03
오른쪽에 그 기능을 보여주게 되어 있는데, 유용할 것입니다.
20:06
단어의 하위 문자열을 보는 것이
20:09
유용했기 때문에
20:12
사람 이름이나 회사 이름을 나타내는 문자 시퀀스에
20:15
하위 문자열을 위한 기능을 추가했습니다.
20:19
하이픈과 함께 보이는 것들이 있다면, 그것은 어떤 것들의 지표일 것입니다.
20:22
특징을 넣고 나서
20:24
기능을 넣고 계속해서 이런 종류의 시스템을 끝날 때까지
20:28
수백만 가지의 손으로 디자인 한 기능을 제공해 왔습니다.
20:30
그리고 이것이 본질적으로 구글 검색이 2015 년경까지 이루어진 방식일 것입니다.
20:37
그들은 특징이라기보다는 신호라는 단어를 좋아했습니다.
20:40
구글 검색을 개선하는 방법은 매달
20:44
많은 엔지니어들이 새로운 신호를 내놓으면
20:47
실험을 통해 이러한 기능을 추가하게 되고
20:51
그리고 구글 검색이 조금 개선되었습니다.
20:53
그리고 학위가 수여되고
20:56
조금씩 검색은 개선될 것이었습니다.
20:57
여기서 생각할 것은, 이것은 기계 학습으로 알려져 있지만,
21:03
기계가 실제로 무엇을 배우고 있습니까?
21:06
기계는 거의 아무것도 배우지 않고 있는 것으로 밝혀졌습니다.
21시 10 분
오히려 인간은 문제에 대해 많은 것을 배우고 있었습니다. 맞나요?
21:13
그들은 문제를 열심히 보고, 많은 데이터 분석을 수행하고,
21:18
이론을 배우고 이 속성에 중요한 요소에 대해 많이 배웠습니다.
21:22
기계는 무엇을 하고 있었습니까?
21:24
기계가 하는 유일한 일은 알려진 대로
21:27
숫자 최적화였습니다.
21:29
일단 모든 신호를 받으면
21:32
선형 분류자로 만듭니다.
21:35
이것은 각 기능 앞에 매개 변수 가중치를 넣었다는 의미입니다.
21:39
그리고 기계 학습 시스템의 임무는 이러한 숫자를 조정하는 것이었습니다.
21:44
이를 통해 성능 최적화를 할 수 있습니다.
21:46
그리고 이것은 실제로 컴퓨터가 잘하는 것입니다.
21:49
컴퓨터는 수치 최적화를 잘 수행하고 있습니다.
21:52
그것은 사람이 실제로 덜 잘하는 것입니다.
21:55
왜냐하면 여기에 100 가지 자질(features)이 있고,
21:57
성능을 극대화를 위해 각각 앞에 실수를 붙였다고 해봅시다.
22:01
음, 처음엔 모호한 아이디어였지만
22:03
확실히 컴퓨터만큼 잘 할 수 없는 것이었습니다.
22:06
그래서 유용하지만 수치 최적화(numeric optimization)를 한 것입니다.
22:11
기계 학습이란 무엇입니까?
22:13
그것이 할 것이라고 기대한 것을 한 것처럼 보이지 않습니다.
22:15
네. 그래서 우리가 실제로 기계 학습에서 발견한 것의 90 %는
22:21
인간이 데이터를 기술하고 중요한 자질을 탐구한 것입니다.
22:27
그리고 이 학습을 실행하는 컴퓨터는 약 10 % 만
22:30
수치 최적화 알고리즘을 하고 있었습니다.
22:34
그렇다면 딥러닝은 어떻게 다른가요?
22:38
딥러닝으로 풀 수 있는 것은.
22:40
표상 학습(representation learning)이라고 하는 부분입니다.
22:43
그리고 표상 학습의 아이디어는 우리가 컴퓨터에 피드를 제공하는 것들로
22:50
가공하지 않은 신호 즉, 시각 신호 또는 언어 신호인지 여부에 관계없이 입력하면
22:55
컴퓨터가 그 자체 그대로
23:00
좋은 중간 표상(intermediate representations)을 사용하여 작업을 잘 수행 할 수 있다는 것입니다.
23:05
그래서 어떤 면에서는 자기 자신이 자질을 발견하게 되는 것입니다.
23:09
과거에는 인간이 그 특징을 발견했던 것과 같은 방식으로 말입니다.
23:14
그래서 정확하게,
23:18
딥러닝이라는 단어의 진정한 의미로 우리가 주장할 수 있는 것은,
23:23
다중 레이어를 통해 배운 표상들에 있습니다.
23:29
그리고 이 방법으로 보다 뛰어난 성능을 낼 수 있습니다.
23:34
표상을 배울 수 있는 다중 레이어 때문에
23:39
딥러닝이라는 용어가 나오기 시작했습니다만.
23:43
요즘은, 딥러닝이라는 용어에서 절반은 뉴럴 네트워크를 사용하고 있다는 의미로 쓰이고 있고
23:49
그리고 나머지 절반은 기술 전문 기자들이 쓴 이야기 속에 있는 것으로
23:53
그것은 지능형 컴퓨터와 거의 관련이 없으며
23:56
다른 시도들도 의미가 없어 보입니다.
23:57
[웃음] 네.
24:01
그래서 딥러닝과 부합하는
24:07
뉴럴 네트워크의 일부분에 참여하려고 합니다.
24:10
그래서 우리가 이 수업에서 집중적으로 다루는 것은 다른 종류의 신경망이 될 것입니다.
24:15
현재 그것은 분명히 우세한 부류에 속해 있고.
24:19
일부의 사람들이 딥러닝을 ​​통해 성공적인 결과에 도달하고 있습니다.
24:23
하지만 이것이 우리가 시도할 수 있는 유일한 방법은 아닙니다.
24:27
다양한 다른 종류의 확률론적 모델을 사용하려고 노력하거나
24:31
딥아키텍쳐 등이
24:33
나중에는 더 많은 도움이 될 것이라고 생각합니다.
24:38
우리가 말하는 신경망이란 무엇입니까?
24:41
목요일과
24:45
다음 주에 이것에 대해 더 이야기할 것이지만
24:46
좀 더 익숙한 신경 과학에서의 정의나
24:51
또는 통계 배경에서
24:55
말하는 뉴럴 네트워크를 의미하는 것입니다.
24:58
그것은 단지 로지스틱 회귀에서 좀 더 나아간 의미로,
25:02
또는 일반적으로 좀 더 스택이 있는 일반화된 선형 모델인 것이
25:05
어떤 면에서는 사실입니다.
25:08
어떤 경우에는 신경 과학과 관련이 있다고 했지만
25:12
이 수업은 전혀 거기에 초점을 맞추고 있지는 않습니다.
25:15
또한 신경과학과는 질적으로 다른 무언가가 있습니다.
25:21
사람들이 현재 구축하고 있는 아키텍처의 종류나
25:26
신경 단위 아키텍처의 복잡한 스태킹 방식 등에서
25:30
우리의 행동, 사고방식, 그리고 일을 하는 방식 등에서
25:35
이전 통계에서 나온 것과는 큰 차이가 있습니다.
25:40
그러나 우리는 역사적인 접근 방식을 취하지 않을 것입니다.
25:42
다만 지금 당장 잘 작동하는 방법에 집중할 것입니다.
25:47
딥러닝의 오랜 역사를 읽고 싶다면,
25:51
비록 제가 꽤 건조하고 지루한 역사라는 것을 경고 하지만,
25:54
Jürgen Schmidhuber의 매우 긴 arxiv 논문이 있습니다.
25:59
그렇다면 왜 딥러닝이 흥미로운가를 살펴볼까요?
26:04
일반적으로 우리가 수동 설계한 자질들은 지나치게 명세되어 있고
26:08
불완전하고, 설계하고 검증하는데 오랜 시간이 걸립니다.
26:12
그리고 하루가 끝났을 때 일정 수준의 성과만 얻을 수 있습니다.
26:16
학습된 기능은 적응하기 쉽고, 훈련이 빠르며,
26:20
그들은 더 나은 수준으로 나아갈 수 있도록 계속 학습 할 수 있습니다.
26:24
그리고 이전보다 더 우수한 성능을 제공합니다.
26:28
그래서, 딥러닝은 매우 유연하고 거의 보편적인 것을 제공하게 됩니다.
26:33
모든 종류의 정보를 나타내기 위한 훌륭한 프레임워크입니다.
26:37
언어 정보뿐만 아니라 세계의 정보 또는 시각적인 정보나
26:42
지도 및 비지도의 모든 방향에서
26:48
딥러닝이 대부분의 사람들에게 흥미로운 이유는 무엇입니까?
26:52
그것은 효과가 있기 때문입니다.
26:55
약 2010년부터 성공적 이었고,
27:00
딥러닝 방식이 전통적인 기계학습보다 훨씬 잘 작동하는 것으로 나타났습니다.
27:04
지난 30 년간 사용된 학습 방법 중
27:09
그 이상으로 능가하는
27:10
충격적인 것은 불과 지난 6 ~ 7 년 동안에 일어났습니다.
27:15
딥러닝 방법으로　진입할 수　있는　길은
27:19
점점 개선되고 놀라운 속도로 좋아지고 있습니다.
27:24
실제로, 아마 제가 편향되어 있을지도 모르지만
27:28
제 평생을 돌아보았을 때 전례가 없는 일이라고 말할 수 있습니다.
27:34
매우 빠르게 진행되고 있는 분야를 보는 관점에서
27:39
한 달에 한 번씩 더 나은 방법으로 일을 할 수 있는 능력이 쏟아져 나오고 있는　것　같습니다．
27:45
그래서 여러분은 이 거대한 산업이 새로운 제품에　매우 흥분해 있는 것처럼 보일 것입니다．
27:49
그리고 오늘 여기에 와　있습니다．
27:52
딥러닝은 왜 이렇게 훌륭하게 성공했을까요？
27:57
실제로 좀　더 미묘한　부분으로
28:02
어떤　면에서는 이야기를 너무 고양시키지 않아도　되는　것이
28:06
우리가 사용하는 많은 핵심 기술을 볼 때
28:10
딥러닝은 실제로 80 년대 또는 90 년대에 발명되었기　때문입니다.
28:15
완전히　새로운 것은 아닙니다.
28:16
우리는 80 년대와 90 년대에 많은 일을　했습니다.
28:20
그리고 여하튼, 그 때의　것이 완전히　사라진　것은　아닙니다.
28:26
그럼　차이점은 무엇입니까?
28:28
글쎄요. 실제로 차이의 일부가 밝혀졌고
28:31
실제로 많은　차이가　있을　수도 있습니다.
28:35
이 모든 것을 가능하게　하는 기술 발전이 일어났기　때문입니다.
28:41
우리는 현재 우리가 사용하고　있는
28:45
거의 모든 것이 데이터로 제공되는 온라인 사회이고
28:49
그래서　방대한 양의 데이터를 가지고 있으며 딥러닝 모델이 선호되고　있습니다.
28:54
80 년대와 90 년대에는,
28:55
딥러닝을 잘 수행 할 수　있는 충분한 계산 능력을　보유하고　있지　않았습니다.
29:00
하지만　수십 년에 걸쳐 컴퓨팅 성능이 보완　되고 있습니다.
29:04
그래서　이제는 잘　작동하는 시스템을 구축 할 수　있게　되었습니다.
29:07
제　말은　특히 놀라운 합류가 있었다는　것입니다.
29:10
딥러닝은 잘 맞는 것으로 판명된　것으로
29:16
GPU로 아주 적은 돈으로도 병렬　벡터 프로세싱을 이용할 수　있습니다．
29:21
그래서 이런 종류의 결혼으로 딥러닝과
29:24
GPU를　통해 많은 일들이 가능해　졌습니다.
29:28
이것은　실제로 진행되고　있는 일입니다.
29:30
그러나 그것은 진행되고　있는 유일한 것도 아니고
29:34
매달 더 좋아지고 있기　때문에　늘　선도적인 것도 아닙니다.
29:37
제 말은, 사람들이 또한
29:40
중간 표상을 배우는 더 나은 방법처럼
29:44
엔드 - 투 - 엔드 결합 시스템 학습을 수행하는 훨씬 더 좋은 방법을 제시했습니다.
29:49
그들은 훨씬 더 나은 방법을 생각해 냈는데
29:52
사물과　컨텍스트와　도메인　사이에　정보를　교환하는
29:56
새로운 알고리즘처럼 알고리즘의 진보도 많이 있었습니다.
30:01
어떤 의미에서　여기에 우리가 집중해야 할 흥미진진한 것들이　있고
0:04
더 많은 시간을　보내야 할　것입니다．
30:07
좋아요.
30:08
실제로 딥러닝에서 첫 번째 큰 돌파구는 음성 인식에　있었습니다.
30:14
딥러닝에서 두 번째 큰 돌파구는　아직 널리 알려지지 않았습니다.
30:19
그러나 이미 시작된 큰 것이 있습니다.
30:23
George Dahl은 토론토 대학에서 Geoff Hinton과 함께 작업한
30:27
작은 데이터 세트를 보여주기 시작했습니다.
30:31
그들은 음성 인식을　위한 딥뉴럴 네트워크로 흥미로운 것을　시도했습니다.
30:36
George Dahl은 마이크로 소프트에 간 직후에,
30:44
토론토 출신의 다른 학생이 Google에 가면서
30:48
딥러닝 네트워크를 사용하는 대형 음성 인식 시스템 구축하기　시작했습니다．
30:53
그리고 음성 인식을　위해
30:56
수백 명이 수십 년을 보냈고
30:58
대기업도　함께　하고　있고
30:59
표준화 된 기술도 있습니다.
31:03
음향 해석을　위한 가우시안 혼합 모델 사용이라거나
31:06
은닉 마르코프 모델 등등으로　불리는　것들을
31:09
수십 년에　걸쳐서 매년 몇 퍼센트씩 향상시키려고 노력하고 있습니다.
31:14
그　결과　그들이 보여줄 수 있었던 것은
31:17
음성 인식을　하기　위한 딥러닝 모델 사용에서
３１:22
즉시　단어 오류율이 엄청나게 감소해서
31:27
약 30 %가 감소했습니다.
31:30
딥러닝의 성공에 대한 두 번째 큰 사례는,
31:35
그것을 알아차리는 모든　면에서 훨씬 더 큰 것입니다.
31:40
ImageNet 컴퓨터 비전 경쟁에 참여했습니다.
31:44
2012 년 토론토의 Geoff Hinton 학생들은 컴퓨터 제작에 착수했습니다.
31:51
객체를 카테고리로 분류하는 ImageNet 태스크를 수행하는 비전 시스템은.
31:57
몇 년 동안 실행 된 작업이었지만
32:01
기존의 컴퓨터 비전 방법으로는 성능이 상당히 느슨해 보였습니다.
32:06
그런데　GPU에서 뉴럴 네트워크를 실행하여
32:11
1/3 오류 감소를 보였습니다.
32:14
그 진전은 수년 동안 계속되고 있지만
32:19
우리는 여기서 그것에 대해 하나하나　말하지는 않을 것입니다.
32:22
좋아요　그게 공정하겠죠?
32:25
잠시 멈추고 로지스틱을 하고
32:28
딥러닝과 자연어처리에 대해 더 자세히 말하겠습니다.
32:30
이 수업에는 두 명의 강사가　있을　것입니다.
32:34
저는 크리스 매닝이고 스탠포드　대학의 교수입니다. 그리고 다른 한　명은　리처드입니다.
32:37
faith of Salesforce의 수석 과학자로 있습니다.
32:40
잠시　인사를　나누도록　하죠．
32:43
안녕하세요？　만나서　반갑습니다．
32:44
저에 대해 잠깐　이야기하면
32:47
저는　2014 년에 졸업하고 Chris와　Enring과 함께
32:52
자연어처리　딥러닝으로　박사 학위를 받았습니다．
32:54
그리고 거의 교수가　되었을　때 작은 회사를 시작했습니다.
32:58
광고 플랫폼을 구축하고 조사했습니다.
33:01
그리고 작년 초,
33:03
우리는 Salesforce에 인수되었습니다.
33:06
저는 지난 2 년간 CS224D를 가르치고 있습니다.
33:09
두　클래스가　합쳐진　것을　매우　즐겁게　생각합니다．
33:13
네
33:16
저는　다음 주에 두 강의를 할 것이므로 자주 보게 될 것이라고 생각합니다.
33:20
[웃음] 저는 지루한 모든 방정식을 담당할 것입니다.
33:23
[웃음] 좋아요, 그리고 TA들, 이 수업에는　정말 멋지고,
33:28
유능하고 훌륭한 TA가 있습니다.
33:32
네, 일반적으로 저는　TA를　통해서　모두를　살펴　보겠지만,
33:35
사람들이　많다　보니까 모두에게　신경 쓰지　못할　수도　있습니다．
33:39
교실에　있는　TA분들은　잠깐　일어나　주시겠어요？
33:44
모두 구석에　뭉쳐　앉았네요.
33:47
좋아요, 그럼　이 시점에서,
33:54
방 크기에 대해 사과　드립니다.
34:00
문제는　이 수업이 비디오 및 방송을　하고
34:05
그것은 기록하는 가장 큰 SCPD 교실입니다.
34:09
그래서, 다른 선택권이 없었습니다.
34:11
여기서 221과 229가　열리는　것도　같은 이유입니다.
34:16
그러나 모든 사람들에게 충분한 자리가 없다는 것은　매우 죄송합니다.
34:23
각 수업은 끝난 직후에 비디오로 제공 될 예정이고
34:28
다른 일반적인　정보는 웹 사이트를 참조하십시오.
34:33
제가　하고 싶은 일, 선행 조건과　해야 할 일.
34:37
그리고, 수업에 관해서,
34:39
여러분이 꼭 알아야 할 것들이　있습니다.
34:42
여러분들이　알　것이라고 생각하지만　만약 모른다면
34:47
모르는　것에　대해　무엇을　해야　하는지를 빨리 알아야　합니다.
34:51
그래서 첫 번째 과제는 파이선으로 과제를 수행하는 것입니다.
34:55
파이선에 능숙해질　수　있도록 웹 사이트에 튜토리얼이　있습니다,
34:59
조금　더　해보면　배우기 어렵지 않습니다.
35:02
파이썬은　딥러닝　툴킷　때문에　이제　공용어　같은　것이　되어서
35:05
많이　사용하는 것 같습니다.
35:09
우리는 미적분과 벡터로 많은 것을 할 것입니다.
35:13
행렬,　다변수 미적분학, 선형 대수학이
35:17
목요일과 다음 주에 시작될 것입니다.
35:22
기본적인 확률과 통계의 종류들이고
35:26
martingales이라든가　하는　팬시한　것들은　필요하지 않습니다.
35:29
그러나 부분적으로　알아야　할　것들이　있는데　가령，
35:33
이　수업은　기계 학습의 기본 사항을 알고 있다고 가정합니다.
35:36
221이나 229를　배웠다면 따라갈　수　있을　것입니다.
35:39
다시 말하지만, 모든 내용을 다　알 필요는 없지만
35:43
손실 함수나
35:47
경사하강법（gradient descent）과　같은 것들로 최적화　하는 방법은 안다고 가정하고　진행합니다．
35:52
자, 우리가 가르치려는 것은, 우선　딥러닝을 위한
35:58
효과적인 현대식 방법을 사용할 수 있는 능력과 이해이고　이를　위한
36:01
모든 기본 사항을 다룰 것입니다.
36:03
특히 자연어처리에서 사용되는 주요 방법에 중점을　두고,
36:07
recurrent networks,  attention, 같은 것들을　다루려고　합니다．
36:11
인간 언어를 이해하는 큰 그림과
36:13
언어를 이해하고 만들어 내는 어려움
36:16
그리고 세 번째 것은 본질적으로는　이 두 가지의 교차점에　대한　것입니다．
36:20
중요한　자연어처리 문제를　해결하기　위한 시스템을 구축하는　것입니다.
36:25
그리고 여러분은 다양한 과제를 통해 몇 가지를 만들 것입니다.
36:29
따라서　해야 할 일은.
36:32
그래서 세　번의 과제가　있을　것입니다.
36:34
그리고　중간고사가　있을　것이고.
36:36
그리고 마지막에, 좀 더 큰 것으로
36:41
스스로　흥미 있다고　생각하는　것을　선택해서
36:45
최종 프로젝트를　제안하고　제출하세요．
36:48
마지막　프로젝트에 멘토가 있는지 확인해야　합니다. 멘토는 Richard
36:53
저, 그리고　TA 중 한　명, 또는 딥러닝에 관한 지식을 가진 다른 사람일　것입니다.
36:57
아니면 우리는 여러분에게　흥미로운 프로젝트를 줄 수도 있습니다.
37:02
일종의 기본 최종 프로젝트가　있을 것입니다.
37:05
과제 4번으로도　알고　있을　것입니다.
37:11
최종 포스터 세션도　있을　것입니다.
37:14
따라서 최종 프로젝트의 모든 팀은 다음과 같은 이유로 최대 3 개의 팀에　속하게　됩니다.
37:18
최종 프로젝트는 최종 포스터 세션이　있어야　합니다.
37:22
이제 우리는 공식 시험 슬롯에서　이 문제를 논의했는데
37:25
그것은 금요일 오후는　선호하지　않을　것　같다고　결정해서
37:30
화요일 오후 일찍부터 세션을 가질 예정입니다.
37:35
언어 수업 시험이 끝난　직후입니다．
37:39
언어에 대한 반감이　아니라
37:40
우리는 여러분 중 누구도 1 학년이 집중 언어 수업을　들을　것이라고는 가정하지　않았습니다.
37:46
아니면 적어도, 그렇지 않은 팀원을 찾는 것이 좋을　것입니다.
37:48
>> [웃음] >> 좋아요, 네, 그래서.
37:52
늦어도
37:55
각 과제는 3 일 이내에 제출 되어야만　합니다.
38:05
그래요, 알았어요．핸드아웃은　목요일에 나눠 드리겠습니다.
38:13
과제는 파이선만으로　작성하는데　물론
38:17
넘파이 라이브러리를 사용할　수　있습니다. 이 라이브러리는 기본 벡터 및 행렬 연산　라이브러리입니다.
38:22
일을　밑바닥부터　하는　것이　좋다고　생각합니다. 왜냐하면 
38:27
실제로　교육에서　중요한　기술은　여러분이　뭔가를　실제로　했다는　것이고
38:31
그것을 처음부터 작동하도록　했다는　것입니다.
38:33
자신이　만들었기　때문에　잘 알고 있을　것이고
38:34
스스로 계산했기 때문에 더　파생되어　나올　수　있는　것이 무엇인지 직접 확인할　수　있습니다.
38:38
그리고 구현한　사람만이 계산할 수 있게　되고
38:41
파생되어　나온　것들과 구현된　것, 그런　것들을　통해　실제로 배우면서　작업할　수　있게　됩니다.
38:45
여러분들이 이런　경험이　없다면,
38:47
모든 것이 마술처럼 보일　것입니다.
38:50
그러므로 실제로 직접 작업하는 것이 중요합니다.
38:53
그럼에도 불구하고 딥러닝을 변화시키고　있는 것으로
38:57
아시다시피　아주 좋은 소프트웨어 패키지가 있습니다.
39:00
그레서　실제로 딥러닝 모델을 만드는 것은 정말 어렵지 않습니다.
39:04
말 그대로 이러한 라이브러리 중 하나를 사용하고 60 줄　정도만　쓰면
39:09
파이선으로 최첨단 딥러닝 시스템　학습을　진행할　수 있습니다.
39:13
잘 훈련 될 데이터가 있다면 더 잘 작동할 것입니다.
39:18
이것은 지난　１，２년　사이에　일어난
39:21
놀라운 발전이었습니다.
39:22
과제 2와 과제 3에 대해서도 그렇게 할　것입니다．
39:26
특히 구글의 TensorFlow를 사용할 예정인데요．
39:29
딥러닝 라이브러리로, 일종의, 글쎄요, 우리에게 매우 친숙하고
39:34
매우 잘 설계된
39:36
현재 가장 많이 사용되는 라이브러리입니다.
39:39
그러나 다른　딥러닝　라이브러리들도 있습니다.
39:42
그리고 나는 그들 중 일부를 （핸드아웃의）아래　부분에서 언급했습니다.
39:45
수업 준비에 대해 질문이 있나요?
39:51
아니면 지금까지　한　것　중에서　질문　있나요？ 아니면 그냥 전원을 켤까요?
39:56
>> [잘　안 들림] >> 네, 좋아요.
40:02
나는 모든 질문을 듣고 반복해서 말하는 것으로 비디오 작업을 할 것입니다.
40:07
질문은 어떻게 과제를 제출할 수 있느냐　하는 것입니다.
40:10
온라인으로 제출하면　될　것입니다．
40:13
지침은 첫 번째 과제에 관한 것입니다.
40:16
네，　모든 것이 전자식이어야　합니다. 우리는 등급을　매길　때　분류　판정을　합니다.
40:21
직접 쓴　것이 있다면 스캔해서　제출해야　합니다.
40:24
온라인으로 제출하세요.
40:27
다른 질문?
40:28
>> [안들림] >> 네
40:33
그래서 질문은 웹 사이트의 슬라이드가　있냐는　것이죠?
40:35
예, 있습니다.
40:36
수업이 시작되기 전에 슬라이드가 웹 사이트에 올라갈　것이고
40:41
이번　학기　내내
40:43
cs224n.stanford.edu.에서　찾을　수　있을　것입니다．
40:47
다른 질문은 없나요?
40:56
그래, 네가 과제 4를 수행 중이라면 그건 logistic입니다.
41:00
부분적으로 조금　다르고 부분적으로 동일하기 때문에 기본 값으로 사용할　수　있습니다．
41:04
과제 4는 2 주 안에 최종 프로젝트에 관해 모두 이야기하도록　하겠습니다.
41:09
최종 프로젝트 제안서를 작성하거나 멘토와　상의할 필요는 없습니다.
41:14
우리는 여러분이　프로젝트를 시작하는　것을　도와주는 프로젝트를 설계했습니다.
41:21
다른　것들은　모두 동일합니다.
41:24
결국　오픈 엔드 프로젝트의　형태가　될 것입니다.
41:27
시스템을 더 잘 만들 수　있는 많은 것들이 있습니다.
41:31
우리는 여러분들이 다른　여러 가지　것들을
41:35
시도하면서, 시스템을 더 좋게　만들었거나　그렇기　못했더라도　흥미진진한 경험들을　나누기를　바랍니다．
41:38
그래서 우리는 4 번 과제를 수행하는 사람들이
41:44
무엇을　했는지 포스터에　쓰기를　바라고　있습니다.
41:48
다른 질문　있나요?
41:50
네, 우리가  Piazza를 사용하고 있는지에 대한 질문이었습니다.
41:56
예, 우리는 의사소통을 위해  Piazza를 사용하고 있습니다.
41:59
그래서, 우리는 이미  Piazza를 설치했고, 우리는 등록 된 모든 학생을 등록하려고 시도했습니다.
42:04
참여한 학생이라면 어딘가에
42:09
가령，정크 메일 박스나 그런　곳　중 하나에  Piazza 발표 사본이　있을　것입니다．
42:14
다른 질문?
42:20
좋아요, 20 분 남았네요.
42:22
진도를　조금만　더　나가보죠．
42:24
아주 빨리, 자연어처리가 왜 힘든지　생각해 봅시다.
42:28
나는 대부분의 사람들, 특히 컴퓨터 과학자도,
42:33
자연어처리가 왜 어려운지 이해하지 못한다고　생각합니다.
42:37
그것은 단지 단어의　연속　정도로 프로그래밍 언어에서 다뤄 왔습니다.
42:41
그리고 말　그대로　단어의　연속일　뿐입니다.
42:45
그럼　왜 이렇게 어려운걸까요?
42:47
여러 가지 이유 때문에 어렵다는 것이 밝혀졌습니다.
42:50
왜냐하면 인간 언어는 프로그래밍 언어와 같지 않기 때문입니다.
42:53
인간 언어는 모호합니다.
42:59
프로그래밍 언어는 모호하지 않게 구성되며,
43:02
우리가　쓸　수　있는 규칙들을 갖고　있습니다.
43:05
가령，　'if'라든가
43:07
파이선에서 들여 쓰기를　해야　한다는　규칙　같은　것입니다.
43:10
인간 언어는 그렇지　않습니다. 인간 언어는
43:16
청자가　'ｉｆ'에서　마음에　들더라도　' ｅｌｓｅ '가 있는　것으로　해석합니다．
43:22
프로그래밍 언어에서는 참조 할 때,
43:26
x와 y와 같은 것을　변수 이름으로　씁니다．
43:31
반면, 인간의 언어로는, 이것， 저것, 그녀와 같이　말합니다.
43:36
우리가 이야기하는 상대방의 상황을 파악할 수　있는 것입니다.
43:41
이러한　것들이 큰 문제입니다만 아마도 가장 큰 문제는 아닐　수도　있습니다.
43:46
가장 큰 문제는 사람은
43:49
효과적인 커뮤니케이션 시스템으로서 언어를 사용한다는　것입니다．
43:53
그래서　우리는　이야기할　때　거의　모든 것을 말하지는　않습니다, 그렇죠?
43:57
우리가　프로그램을 작성할 때는 프로그램을 실행하는 데 필요한 모든 것을 말해야　합니다.
44:02
사람의 언어에서는 대부분의 프로그램이 생략되어도　됩니다.
44:07
청취자가 반드시　알아야　할　코드를　잘　찾아낸　다음.
44:12
StackOverflow　같은　곳에　코드　조각들이　있어서
44:15
청자는 나머지 프로그램을 잘　채우는　것　같습니다.
44:19
그래서 인간 언어는 매우　효율적으로
44:22
빠르게 실제의　의사소통하는 것 같습니다. 맞습니까?
44:26
우리가 말하는 속도는
44:28
5G와　같은 통신 속도가 아니죠?
44:33
느린 통신 채널입니다.
44:35
그러나 효율적으로 작동하는 이유는 최소한의 메시지만 말해도　되기 때문입니다.
44:39
그렇게　해도 청자는 나머지 모든 것을 그들의 세계 지식으로 채웁니다.
44:43
상식적 지식이나 상황에 대한 문맥 지식　같은　것들로　말입니다．
44:47
이것이 자연 언어가 어려운 가장 큰 이유입니다.
44:51
자연 언어가 왜 어려운지에 대한 심오한 버전의 일종으로, 저는
44:56
정말 이 XKCD 만화를 좋아하지만 여러분은　분명히 읽을 수　없을　것이라고　생각되는　게，
45:01
저　역시 컴퓨터　앞에서 간신히 읽을 수　있는　것이　있었는데요.
45:04
>> [웃음] >>　이것을　생각해 보면,
45:06
왜 자연 언어 이해가 어려운지에 관해서 많은　것을　말해　줍니다.
45:12
거기서　두 여자가 서로 이야기합니다.
45:16
하나는 '어쨌든, 나는 덜 신경 써도　돼'라고 말했더니, 다른 한　명이 말합니다.
45:20
'네가 덜　신경써도 된다고　말하는　것은　신경을　더　써도　된다고 말하는 것 같아.
45:25
그러니까 어느 정도는 계속　신경을 쓰는 것을　의미하는　거지. 그러자 다른　한　명이,
45:29
'몰라.'라고　말하고 이야기가　계속됩니다.
45:36
우리는　이 믿을 수 없을 정도로 복잡한 것들로　빈　의미　공간을　채워　나가는　거야.
45:40
비어　있는　것에서　서로를　연결하기　위해
45:43
맹목적으로 어둠 속으로　단어들을 내 보내지.
45:46
어구의 모든 흔적과 철자법과 어조와
45:48
수많은 신호와 컨텍스트 및 하위 텍스트 등을 전달하지.
45:53
그리고 모든 청자는 이러한 신호를 자신의 방식으로 해석해
45:57
언어는 공식적인 체계가 아니야. 영광스러운 혼란일　뿐이지.
46:01
어떤 단어가 누구에게　무엇을 의미하는지 확실히 알 수 없어.
46:05
우리가 할 수　있는  할 수 있는 것은 단지　우리가　한 말이 사람들에게 어떻게 영향을 미치는지 추측하는 것뿐이야.
46:09
그래서, 우리가　무엇을　만들었는지　알　수　있을　때는
46:12
우리가 느끼기를 원했던　것을　듣는　사람이　느끼는　때이고
46:15
대부분의　다른　말들은 무의미하지.
46:18
네가 단어를 해석하는 방법에 대한 조언을 해주고　있는 것 같은데.
46:21
왜냐하면 너는 내가 덜 혼자라고 느끼기를 원하기 때문이겠지.
46:23
만약 그렇다면 고마워.
46:27
너는 방금 내　문장　사이의　（의미）공간을　달려서　너의　정신 검진표까지　잘　도달했네.　그리고
46:30
네가　얼마나 잘 이해했는지도 보여　주었고！
46:34
>> [웃음] >> 이 XKCD에 대해 생각한다면
46:39
만화지만, 거기에는 실제로 인간에 대한 심오한 내용이 많이 있습니다.
46:44
언어 이해와　같은　것들이　왜 어려운지에　대해서　말입니다.
46:49
하지만 아마 자세한　내용을　알기는 어려웠을　것입니다.
46:52
몇 분만 간단한 예제를 보여 드리도록　하겠습니다.
46:56
자연 언어는　재미있는 모호성을 포함하여 모호한 점이 많습니다.
47:02
여기에 몇 가지를　보여　드리겠습니다.
47:04
TIME 잡지에서 최근에 나온 것　중　재미있는　것입니다.
47:10
The Pope's baby steps on gays 아니, 그렇게 해석하라는 의미가 아니에요.
47:15
이건 The Pope's ／／baby steps on gays으로 해석해야　합니다.
47:21
>> [읏음] >> 좋아요.
47:33
여기서 질문, 왜 여러분들은　두　가지 해석을 하게　되었습니까?
47:39
인간의 언어와 영어는 무엇입니까?
47:43
왜 영어는　이 두 가지 해석이　가능할까요?
47:48
무슨 일이 진행되고 있나　봅시다．
47:51
누군가 우리에게　설명할　수　있나요？
48:18
네, 맞아요.
48:22
설명을 좀　더　되풀이해서　말하면
48:24
관용적이라고 말한 것은, 어떤 의미에서,
48:28
첫걸음을 일종의 은유로　본　것입니다.
48:32
baby steps이 의미하는 관용구는 아기처럼 작은 단계일　것입니다,
48:37
하지만 여러분이 결론에 도달하기 전에, 많은 부분을 생각할 수 있습니다.
48:42
이것은 구문의 모호함이 나머지 부분을 지배하기　때문인데,
48:46
먼저， 선택할　수　있는　것은  Pope's baby를 명사구로　보는　것이겠죠？
48:51
즉　진짜 아기로 해석한　것입니다．
48:54
그런 다음 단계로 동사를 해석합니다.
48:58
우리는 영어를 포함한 많은 언어에서
49:01
같은　단어가　근본적으로 다른 역할을 하는　것을　발견할　수　있습니다．
49:05
그와 동사의　언어적　해석　단계에서　 steps를 동사로 사용했다고　본　것입니다.
49:11
하지만 다른 해석은 명사 합성이라고　한　것과　같이
49:14
영어에서　명사　합성은　자주　일어나는　것이므로　두　단어를　모두　명사로　보는　것입니다．
49:19
사람들은 항상 그렇게　하죠?
49:22
디스크 드라이브 인클로저나 네트워크 인터페이스 허브와 같은 것을 얻자마자,
49:27
같은　방식으로, 두　명사를　하나로 못 박는 것입니다.
49:32
그래서  baby　steps을 두 명사로 합치고　나니
49:36
baby steps를 명사구로 사용하는　것이　가능해졌습니다.
49:39
그리고 이제　Pope's baby steps를 더 큰 명사구로 만들 수 있습니다.
49:43
그리고　그　결과 아주 다른 해석을 얻게　되었습니다.
49:45
또한　동시에 우리는 아기의 의미를 바꾸었습니다.
49:50
그래서 한 경우 아기는　은유적 의미의　아기였고 다른 한　편에서는
49:54
아마도　반대로 문자 그대로의 아기로　해석되었습니다.
50:01
하나　정도　더　해보겠습니다．
50:04
여기 또 다른 좋은 재미있는 예가 있습니다.
50:07
paralyzed after tumor fights back to gain black belt
50:10
>> [웃음] >> 다시,
50:13
여러분이 그것을 읽으려는 의도와　다릅니다.
50:15
여러분은 그것을 소년으로 읽으려고　하고　있습니다
50:19
paralyzed after tumor// fights back to gain black belt.
50:23
그렇다면 여기서는　어떻게 모호함의 특징을 설명할 수 있을까요?
50:31
누군가 구두점이　없다는 것을 제안했는데,
50:37
어느 정도는　맞습니다.
50:42
그리고 어느 정도까지는 쉼표를 사용하여
50:45
해석을 명확하게　할　수　있습니다.
50:48
그러나 언어에는 여전히　모호한 부분이 많이 있습니다.
50:53
구두점을 넣고 모호하지 않게　하는 것이 일반적인 표준은 아닙니다.
50:59
그리고 실제로, 만약 우리가 컴퓨터 과학자로서
51:03
불분명한 언어의 주위에 괄호를 쳐서.
51:08
해석이 훨씬 명확해지게　해야겠다고　생각한다면 더 이상 전형적인 언어 사용자가 아니게　될　것입니다.
51:13
[웃음] >>좋아요, 이제　다른　부분으로　접근해보죠.
51:17
네?
51:28
예, 그렇습니다. 이것은 구문의 모호함 때문입니다.
51:35
그래서, 여러분이 'paralyzed'를　문장의
51:38
주동사로　해석했을　것입니다. 그랬더니
51:40
한　소년이 마비됐고, 그런　다음　종양이 검은 색이 되기 위해 다시 싸운다　라는　문장은
51:46
그런 일이 언제 일어났는지 말하기　위한 종속절이　되었습니다.
51:51
이렇게　되면 '종양'은 ＇다시　싸우는＇ 주체로　해석됩니다．
51:57
선택　가능한　다른　해석은 ＇마비 된＇을　다르게　해석하는　것입니다．
52:03
수동 분사라고 불리는 것이　있을 수 있는데.
52:07
이렇게　하면　'종양 후 마비'라는　분사구를　이끌게　됩니다．
52:13
그래서 형용사처럼 소년의 수식어가 될 수 있습니다.
52:18
어린 소년 블랙 벨트를 얻기 위해 다시 싸운다는　의미로
52:21
종양　이후에　마비된　소년，　검은　벨트를　얻기　위해　다시　싸운다는　문장이　될 수 있습니다.
52:25
이렇게　하면 싸움의 주체는 소년이　됩니다.
52:29
좋아요. 이 슬라이드에 몇 가지 예시가 더 있지만 끝까지　자세히　설명하지는 않겠습니다.
52:33
왜냐하면 진도가　살짝　늦어지고　있기　때문입니다.
52:40
좋아요, 그럼 제가 조금이라도 들어가기를 원했던 부분은
52:46
제 시간이 다 끝나기　전에
52:49
딥러닝과 자연어처리의 아이디어를 소개하는 것입니다.
52:54
본질적인 것은, 이것이
52:57
우리가 지금까지 이야기 해 왔던 두 가지는,　딥러닝과 자연어처리.
53:03
그리고, 딥러닝과　신경망의 결합입니다.
53:06
우리는 표상 학습을
53:09
자연 언어 처리에서의　언어　이해 문제에　적용할　것입니다．
53:14
그리고 지난 몇 년 동안,
53:16
이제 막 날기 시작한 일종의 부문에　대해서
53:21
오늘 수업의 남은　시간에 조금 말해　보겠습니다.
53:25
매우 높은 수준에서　일어나는 일들에 대해
53:30
목요일에 대비해서　구체적인 세부 사항들에　들어가　보려고　합니다．
53:35
그래서, 그렇게,
53:40
우리가 볼 수　있는 것과는　매우 다른 종류의 분류가 있습니다.
53:45
그리고　다른 한편으로는 딥러닝이 여러 다양한 수준
53:49
즉，단어, 구문, 의미 등에 적용되고 있습니다.
53:53
그것은 다양한 종류의 도구와 우리가 사용하는
53:57
자연 언어 처리　알고리즘에 적용되었습니다.
53:59
가령， 품사　레이블링이나 사람　이름,
54:04
조직 이름, 또는 문장의 통사 구조와　같은　것들입니다.
54:08
그런 다음 많은
54:11
언어 응용 프로그램 등에　함께　적용되어　들어갔습니다.
54:14
제가 앞에　이야기했던　기계 번역, 감성 분석,
54:18
대화 에이전트　같은　것들이　될　것입니다．
54:19
정말 흥미로운 점 중 하나는 딥러닝 모델이
54:24
동일한 도구를 사용하는 매우 통일 된 방법을 제공하고
54:30
기술적인 문제들을 이해하고　있습니다.
54:34
그래서 여러 문제에 대한 구체적인 내용을　제공해　주고　있는데요.
54:37
딥러닝의 발달이 아주 놀라운 것은
54:41
실제로 핵심 기술의 아주 작은 툴박스지만,
54:47
그것은 방대하게 적용 가능하고
54:51
많은　문제들을　매우 정확하게　해결한　것으로 밝혀졌습니다.
54:55
실제로 많은 언어 문제뿐만 아니라,
55:00
나머지 부분 대부분도 딥러닝을　통해서 일어납니다.
55:03
비전분야에서　딥러닝을　통해서
55:08
다른 종류의 신호 분석, 지식 표현 또는
55:12
다양한 문제를 해결하는 데 사용한 몇 가지 중요한 툴들이 있습니다.
55:17
그리고 인류에게는　다소　부끄러울　수도　있는　것으로,
55:22
일을　너무　잘해서
55:25
인류가 이전에 수십 년간 성장해 온　것들，
5:30
수십 년 동안의 개발이나, 다른 작업에 대해　개인화하지　않고도　인간의　기술보다　더　좋다는　것입니다．　
55:35
좋아요, 딥러닝과 언어는 모두 단어 의미로부터 시작합니다.
55:42
이것은 두 번째 수업부터 진행할　때의 아주 핵심적인 아이디어입니다.
55:49
여기서, 우리가 단어라고　하는　것은　표현된　단어를　의미하는 것입니다．
55:55
특히 우리는 그 단어의 의미를
55:58
숫자　벡터로　나타낼 것입니다.
56:00
그래서 여기에 기대 단어에 대한 벡터가 있습니다.
56:04
저는 그것을 8 차원 벡터로 만들었습니다.
56:07
슬라이드에 적기　좋기　때문에　이렇게　만들었지만
56:10
그러나 실제로, 우리는 이렇게 작은 벡터들을 많이 사용하지는 않습니다.
56:12
최소한, 우리는 25 차원 벡터와 같은 것을 사용할 것이고
56:17
보다　일반적으로는 300 차원 벡터와 같은 것을 사용할 것입니다.
56:21
우리가 정말로 실제적인　것을　다룬다면
56:24
최고의 시스템으로 무언가를　하고 싶어 하기 때문에,
56:27
1000 차원 벡터나 그와 비슷한 것을 사용하게　될　것입니다.
56:31
단어에 대한 벡터가　있다는　것은,
56:34
우리가 고차원의 벡터 공간에 단어를 배치한다는 것을 의미합니다.
56:38
우리가 알아내는 것은, 우리가　이 방법을 사용할 때로
56:43
딥러닝은　여기서 단어 벡터와　단어의　위치를 배우게　되면
56:47
고차원 벡터 공간은　훌륭한 의미 공간으로 작용하게　될　것입니다．
56:52
비슷한 의미의 단어가 벡터 공간에 함께 모이게　됩니다.
56:57
사실 그 이상으로.
56:58
벡터 공간에 방향이 있다는 것을 알 수 있습니다.
57:01
그러면　실제로 구성 요소와 의미에 대해 알려　주게　됩니다.
57:04
그래서 우리 인간의 문제 중 하나는 
57:08
고차원 공간을 아주 잘 볼 수는　없다는　것입니다.
57:11
그래서 인간을 위해서 항상 이차원　또는
57:14
3 차원으로　투영해야　합니다．
57:15
그리고 배경에서, 약간의 워드　클라우드와　같은　것을 볼 수 있는데　그것은
57:20
전혀 읽을 수　없는 단어 벡터 공간을 2D 투영법을 설명해서　가능해집니다.
57:25
그러나 우리는 일종의 줌인을 통해서
57:29
그러면 읽을 수　있는 무언가를 얻을 수도 있습니다.
57:32
공간의 일부　중 여기는　국가라는 단어가 클러스터 되는 곳입니다.
57:38
그리고 공간의 다른 부분에서 우리는　동사 클러스터링을　볼　수 있습니다.
57:42
유사도가 가장　비슷한 동사들이 그룹 짓는 것을　보고 있습니다.
57:47
따라서  'come'과 'go'는 매우 유사하며 'say' 와 'think' 는 유사하며 'think'와
57:51
'expect'는 유사합니다.
57:53
'Expecting'과 'thinking는 실제로 'seeing things'와 비슷합니다.
57:57
왜냐하면 사람들은 흔히 생각을 비유로 사용하기 때문입니다.
58:00
맞나요?
58:11
좋아요. 그렇다면　이 벡터 공간의 축은 무엇을 의미합니까?
58:15
어떤　면에서 의미　없는　대답이　될　것입니다．
58:21
우리가　이 벡터 공간을 배울 때, 사실 우리는　300D 벡터를 가지고 있었습니다.
58:26
그리고 그 벡터들에 해당하는 축들이 있습니다.
58:29
그리고 종종 실제로, 우리는 이러한 요소 중 일부를　볼　수　있는데
58:34
축을 따라 가면서 해석하면　좀　더　쉽게　볼　수　있습니다.
58:38
그러나 정말로, 요소와
58:43
의미가　벡터　선을　따라 가야 할　특별한　이유는　없습니다．
58:45
벡터 공간에서 다른 각도일 수도 있습니다.
58:48
그것이　꼭　어떤　것을　의미하는　것은　아닙니다.
58:50
이와 같이 2D 투영을 원할 때 우리가 사용하는 것은 무엇입니까?　어떤　방법을　사용할　때
58:55
성실하게 주요한 의미를 파악하려고 노력하고 있는데　이것이
59:00
고 차원 벡터 공간의 주된 의미에서　온　것으로 나중에　볼　수　있을 것입니다
59:04
그래서　이것은 여러분 중 많은 사람들이 다른 곳에서　이미 보았을 가장 간단한 방법입니다.
59:09
PCA를　통해　주성분 분석을　하고 있습니다.
59:13
우리가 t-SNE라고 부르는 또 다른 방법도 있습니다.
59:16
일반적으로 사용되는 비선형 차원 감소는
59:20
단지 사람들에게 무슨 일이 일어나고 있는지에 대해 어떤 감각을 주려고 시도되는 것입니다.
59:25
그리고 이러한 저 차원 투영 중 어떤 것이든
59:30
극도로 오해의 소지가　있을 수 있다는　것을　아는　것이　중요합니다.
59:33
왜냐하면 막대한 양의 정보는 남겨두고 있어서
59:37
그것은 실제로 벡터 공간에 있기　때문입니다.
59:40
여기, 저는 개구리와　가장　가까운 단어를　보고 있습니다.
59:45
스탠포드에서 한 GLOVE 임베딩을 사용했는데요.
59:48
다음 강의에서　좀　더　이야기하겠지만
59:50
개구리는 두꺼비가 가장 가까운 단어로　보기에　좋네요．
59:55
그러나 우리가 이해하지 못하는　이 다른 말들도,
59:59
다른 예쁜 개구리들의 이름이기도　합니다.
60:04
그래서　이 단어 의미 벡터는 뭔가를 시작하기에 좋은 기초가　될　것입니다.
60:10
하지만 여기서　감각을　더　키워　주고　싶은데.
60:11
지난 몇 분, 우리는 그 이상으로 많은 것을 할 수 있다는　것을　알았을　것입니다.
60:15
놀랍게도 우리는　이 벡터들을 계속 사용할 것입니다.
60:19
전통적으로, 만약 우리가　uninterested　같은　복합어를　보고 있다면, 우리는
60:24
더 작은  형태소로 구성된　것으로　생각할　것입니다．
60:30
우리가 하려는 것은, 아니라고 말하는 것이다.
60:33
물론　우리는 단어의 일부를 생각할 수도 있습니다.
60:35
단어의 해당 부분의 의미를 나타내는 벡터로　말입니다.
60:40
그리고 우리가　하고자　하는 것은 신경 네트워크를 구성하는 것입니다.
60:45
이 작은 조각들에서 더 큰 단위의 의미를　찾는　것으로
60:50
그것은 Minh-Thang Luong과 Richard가 몇 년 전에 스탠포드에서　했던 작업입니다.
60:56
이제　그것을　넘어서, 문장의 구조를 이해하고 싶습니다.
61:01
그리고 우리가 딥러닝으로 할 수　있는 또 다른 것은
61:06
문장의 구조에서 문법적인 휴지를　찾는　것으로.
61:11
저기에　있는 Danqi Chen은 수업을　위한 TA 중 한　명으로
61:16
2 년 전부터 여기에　관심을　기울이고　있습니다.
61:22
의존성 분석을　위한 네트워크 방법도
61:25
대단히 성공적이었습니다.
61:27
근본적으로 최근 Google 공지 중 일부를 본 적이 있다면
61:31
Parsey McParseface 및 신텍스넷을 사용한다는　것을　알　것입니다.
61:34
근본적으로 더 연마된　방법으로　사용할　수　있고
61:37
Danqi가 소개한　 기술의 좀　더 큰 버전입니다．
61:42
우리가 문장 구조의 일부를　이해했다면，
61:46
그　다음은　문장의 의미를 이해하고자 할 것입니다.
61:51
그리고 사람들은 수십 년 동안 문장의 의미에 대해 연구해 왔습니다.
61:55
그리고　저는 다른 방식으로
62:00
문장의 의미를　탐구하고　있지는　않습니다． 
62:02
그러나 딥러닝　자연어처리를　하는 측면에서,
62:05
이 수업에서 저는 우리가 어떻게 일을 다르게 할 것인지에 대해서도 알려　드리겠습니다．
62:11
전통적인 방법은 일반적으로 람다 계산법입니다.
62:16
미적분학 기반의 의미 이론이죠．
62:19
그것은 개별 단어의 의미 함수를 손으로 계산합니다.
62:24
그리고 다음에 신중하고 논리적인 대수를　통해
62:28
단어의 의미를 결합하는 의미　결합법이　있습니다．
62:36
또한 프로그래밍 언어도 때때로 사용되었습니다.
62:40
프로그래밍 언어의 표기　의미론은
62:43
하지만 여기서 우리가 다루려고　하는　것은　아닙니다．
62:45
우리가 할 말은, 글쎄요，　우리가　단어　의미로서의
62:50
벡터로부터　시작하면, 벡터인 구의 의미도　만들　수　있을 것입니다.
62:55
그리고 더 큰 구절과
62:57
문장도 의미 벡터가　될　수　있을　것입니다.
63:00
우리가 알고　싶어 하는　것은　일종의　관계들인데　먼저，
63:04
문장과 문장 또는　시각　장면　같은　문장과 세계 사이의
63:09
의미를　찾는　방식으로 신경 네트워크를 배우고
63:14
결정을　할　것입니다．
63:19
자　봅시다.
63:20
그래서 우리는　이　방법을　모든 종류의 의미에 사용할 수 있습니다.
63:25
이것은 사실 리처드가 일하는 동안　했던 것 중 하나입니다.
63:28
박사 과정 학생으로 감성 분석을　하고　있었습니다．
63:33
그래서 이것을 훨씬 더 잘하기 위해　노력했었는데
63:36
신중하게 실제적인 의미 표현에서
63:40
문장의 긍정적인　것과 부정적인 정서에 대한 이해를　하고
63:44
문장의 어느 부분이 다른 의미를 갖는지 실제로 조사함으로써
63:49
그래서 이 영화의 문장에서는 영리함이나 재치,
63:55
또는 어떤 다른 종류의 지능형 유머　등에　대해서 신경 쓰지 않고 있지만, 이　시스템은
63:59
실제로 매우 정확하게 작동했고, 여기　긍정적인 것들에는
64:04
영리함, 지혜, 지적인 유머가 있는　것을　알　수　있습니다.
64:07
그것은 모두 매우 긍정적이며, 전통적으로 감성의 한 종류입니다.
64:11
분석 시스템이 이들을　분리하여 긍정적인 문장이라고 말하는 것입니다.
64:16
우리의 뉴럴 네트워크 시스템은
64:19
처음에는 이　영화에　관심이 없다가
64:21
전체　문장에 대해서 부정적인 감성으로 정확하게 결정하고　있습니다.
64:27
네, 시간이 없으니 몇 가지를 건너뛰고
64:32
매우 흥미로웠던 두 가지 다른 것들을 언급하겠습니다.
64:36
그래서 지금 채팅 봇을 만들려고　시도하는　사람들이　많습니다.
64:42
대화형 에이전트로
64:44
음성 및 언어 이해의 인터페이스에서
64:49
우리는 모바일 컴퓨터와 상호 작용할 수 있습니다.
64:52
가령，　알렉사와　같은 것들이 있습니다.
64:56
제　생각을　솔직히　말씀드리면　기술의　상태가　한　순간에
65:01
음성 인식　부분에서 엄청난 발전을 이룩한 것입니까?
65:05
지난　수십 년 동안 음성 인식이 진행되어　왔고
65:10
언어 기술에　속해　있는　일원으로서 제가
65:16
1990 년대부터　주장한　것은．．． 아니, 음성 인식은 정말 좋습니다.
65:18
우리는 정말 좋은 음성 인식 시스템을 개발했습니다.
65:21
그러나 문제는 그들이 아주 좋지만 실제 인간만큼은　아니었습니다．
65:27
정확도가 너무 낮기 때문에 선택의 여지가 있었다면 사용하지 않았을 것입니다.
65:32
반면, 지난 몇 년 동안 신경 네트워크 기반의
65:36
딥러닝 음성 인식 시스템은 놀라울 정도로 훌륭해졌습니다.
65:41
제 생각엔　이 방에　있는 젊은　사람들은 그렇게　생각하지　않을　수도 있습니다.
65:45
저는　제외하고．
65:46
그러나 저는 많은 사람들이 얼마나 좋아졌는지 실제로 깨닫지 못하고　있다고 생각합니다.
65:51
2012 년에 사람들이 많은　것을　시도했고
65:56
꽤 합리적이지만 훌륭하지 않다고　결정했습니다.　그리고
65:59
그 이후로는　사용하지　않고　있었습니다.
66:02
혹시　음성 인식을 평상시에　사용하지 않는다면
66:06
집에 가서 전화에 몇 가지 말을 해보십시오.
66:09
그러면　음성 인식이 얼마나 잘 작동하는지 놀라게　될　것입니다.
66:14
그러나 문제가 있습니다.
66:16
음성 인식이 완벽하게 작동합니다.
66:19
그럼에도 불구하고 전화는 우리가 하는 말을 몰라서.
66:23
구글로　검색해　드릴까요？ 라고　합니다．
66:25
여기 이　큰　문제가　바로
66:28
이 수업에서 우리가 작업하고　있는 핵심　과제입니다.
66:32
우리는 어떻게 자연 언어 이해를 실제로 똑같이 좋게 만들 수 있을까요?
66:36
그래서 우리는　여기에 집중해서　작업할　필요가　있어　보입니다.
66:40
실제적인　한　곳으로,
66:42
휴대 전화로 구글의 Inbox 프로그램을 사용해 본 사람 있습니까?
66:48
시도해 봤어요?
66:49
몇 사람 있네요.
66:52
여기　아주 단순하지만　멋지게 전개된 예가 있습니다.
66:57
딥러닝 다이얼로그　에이전트는 구글 Inbox의 추천 답변입니다.
67:03
순환 신경망을 이용한　메시지로
67:08
3 개의 응답을　답변으로 제안합니다.
67:14
그리고 이런 종류의 프로그램에서　신경 써야　하는
67:19
개인 정보 보호 및 다른 것들도 포함하고 있으며, 사려　깊게　처리하고　있습니다.
67:22
실제로 종종 응답은 상당히 좋습니다.
67:26
이메일　로드를 줄이려고 구글 Inbox에 시도해　보고　나면
67:31
꽤　많은　이메일에　답장할 수 있을　것입니다.
67:36
좋아요, 끝내기 전에 언급하고 싶은 또 하나의 예는
67:41
기계 번역입니다.
67:43
기계　번역은 자연 언어 처리와　함께　시작되었습니다.
67:48
실제로는 일반적으로 언어 이해로 시작하지 않았습니다.
67:51
자연어 처리는 냉전의 시작과　함께　시작했습니다.
67:57
미국인과 러시아인은
68:01
놀랄 만큼 서로의 말을 이해하지 못했습니다.
68:04
그래서　제2차 세계 대전에서 암호 해독의 성공의　결과로
68:08
사람들은　컴퓨터가 언어도 번역 할 수 있다고 생각했습니다.
68:14
그러나　초기에는 정말　끔찍하게 작동했고
68:17
2000 년대에 상황이 조금 더 나아지기 시작했습니다.
68:22
고전적 구글 번역의 일종으로　절반　정도는　작동했습니다.
68:26
무슨 뜻인지 요점을 알 수는 있겠지만 여전히 끔찍한　정도의　성능이었습니다.
68:31
지난 2 년 동안 실제로는 2014 년에 시작되고　나서,
68:36
딥러닝　시스템의　end-to-end 학습을 ​​시작했습니다.
68:41
neural machine translation　방법입니다.
68:48
이것으로　기계학습의 모든 문제가 해결 된 것은 확실히　아닙니다.
68:52
여전히 기계 번역을 향상시키기 위해　해야 ​​할 일이 많이 있습니다.
68:56
그러나 다시 한 번, 이것은 단지
69:00
구글 번역에서 200 명의　근무년수를　하루　아침에　대체하는
69:05
새로운 딥러닝 기반의 기계 번역 시스템이 하루　아침에
69:11
번역 품질을 크게 향상시켰습니다.
69:14
그리고 그것에 관한 긴 기사가　있었습니다.
69:17
몇 주 전에 뉴욕 타임스지에서　본　사람도　있을　것입니다 .
69:22
기존의 번역 접근 방식이　아니라
69:26
단지 크고　깊은　순환 신경망으로　운영됩니다.
69:31
소스가　되는 문장에서 생성된 벡터를 통해 읽기 시작하는데，
69:36
문장을 나타내는 벡터 내부 표현이　있는　곳입니다．
69:40
그리고 문장이 끝나면
69:44
번역 단어를 생성하기 시작합니다.
69:48
번역에서 차례대로 단어를 생성해　내는　것입니다．
69:51
neural language model이라고　불리는 것은 무엇입니까?
69:54
이것은 우리가　하는 많은 일에서 사용되는 핵심 기술이기도　합니다.
69:58
이것이 바로 구글 Inbox의 종류에 사용되는 순환 신경망
70:03
그리고 neural machine translatioｎ 시스템의 생성　측면에서　작동합니다．
70:09
좋아요, 이제 우린 한　일분　정도　남았네요．
70:15
우리가 늦게 시작 했음에도 불구하고 그렇게　늦지는　않았네요．
70:18
제가  마지막으로　강조하고　싶은 것은
70:24
여기서 일어나는 놀라운 일은 실제로는　모두 벡터라는　것입니다．
70:29
우리는 모든 언어의　표상들,
70:33
가령, 소리나 단어의 부분, 단어, 문장과
70:38
대화가 모두 실제　값의　벡터로 바꾸었습니다.
70:44
그런 다음 훨씬 더 많이 이야기 할　수　있을 것입니다.
70:46
목요일에 단어 벡터에 대해 이야기하고
70:50
Richard는 다음에 단어　벡터에 대해 더 많이 이야기 할 것입니다.
70:55
제 말은 많은 사람들의 눈길을　끌고　있지만
70:59
실제로 많은 사람들이 깨달은 것보다 훨씬 더 미묘한 것임을 알게　될　것입니다.
71:04
크고　긴 벡터에는 구조가 없다고 생각할 수도 있습니다.
71:08
그러나 똑같이 말하면, 그 벡터를 재구성 할 수도　있고，
71:12
텐서라고 불리는　것으로 행렬이나 또는 더 높은 고차원 배열로 변환할 수 있습니다.
71:16
또는　다른　부분이긴　하지만
71:18
그 방향이 다른 종류의 정보를 나타내는
71:21
실제로　매우 유연한 데이터 구조입니다.
71:24
거대한 표현력이
71:26
딥러닝　시스템이 실제로　모든 것을 활용할　수　있게　된　원동력이　된　것입니다．
71:33
네　감사합니다．
71:35
>> [박수]