00:00
[음악]
00:04
스탠포드 대학교.
00:10
>> 좋아요!
00:11
안녕하세요?
00:13
세 번째 강의에 오신 것을 환영합니다.
00:14
저는 리차드입니다, 그리고 오늘 우리는 단어 벡터에 대해 조금 더 이야기 할 것입니다.
00:18
하지만 그 전에 세 가지 수업 운영 관련 공지가 있습니다.
00:23
먼저, 이번 주에 첫 번째 코딩 세션을 갖는데
00:27
문제 세트 중 하나는 여러분들의 프로그래밍을 위한 것입니다.
00:31
처음부터 모든 것을 혼자 해야 하니까
00:36
일찍부터 시작하기를 바랍니다.
00:39
코딩 세션은 주로 다른 사람들과 함께 채팅하면서 서로 도우면서.
00:42
버그와 같은 것들을 해결하는 것입니다.
00:44
설정이 모두 올바르게 되었는지, 실습 환경은 어떤지에 대한
00:47
모든 것으로 여러분들이 딥러닝으로 신나게 들어갈 수 있게 도와 줄 것입니다.
00:53
그리고 컴퓨터 과학자 포럼인 취업 박람회가 있습니다.
00:56
일자리를 찾거나 경력 등에 대해 이야기를 나누는 등 도움을 받을 수 있을 것입니다.
01:04
그리고 오늘은 제 첫 번째 프로젝트 상담 시간입니다.
01:08
저녁 식사를 대충 하고 난 후 여기 Huang 지하층에서 다시 이야기를 나룰 생각입니다.
01:12	
주로 프로젝트에 관해서 이야기할 것이므로 미리부터 프로젝트를 생각해 볼 것을 권합니다.
01:16
오늘부터 시작하겠습니다.
01:19
초보자를 위한 아이디어를 얻고 싶다면 이야기 나누는 것이 재미있을 것입니다.
01:22
기대되네요.
01:25
운영에 관한 질문이 있나요?
01:29
네. 세상 어디나 마찬가지겠지만 시간을 놓치면 안 됩니다.
01:33
이 수업도 마찬가지죠.
01:38
기타 운영에 대한 질문 있나요?
01:46
네. 저 사람이 상담 시간도 잡아 줄 것입니다.
01:47
웹 사이트의
01:50
달력에서 모든 상담 시간을 확인할 수 있습니다.
01:57
네. 고칠 수 있을 거예요 .
01:58
상담 시간을 담당할 사람의 이름을 추가하면 됩니다. 특히
02:02
크리스와 제 이름을 넣으면 될 것입니다.
02:08
오늘 word2vec를 마칠 것입니다.
02:10
정말로 재미있는 부분은,
02:12
실제로 word2vec이 무엇을 포착하는지 질문할 것입니다.
02:16
목적 함수가 있고 최적화를 하기 위해서
02:18
잠시 살펴본 후에 무엇이 진행되고 있는지를 분석할 것입니다.
02:21
그 다음에 조금 더 효과적으로 word2vec의 본질을
02:25
실제로 포착하려고 노력할 것입니다.
02:27
그 다음에 첫 번째 분석으로, 단어 벡터에 대한  내적, 
02:30
외적 평가를 볼 것입니다.
02:34
정말 재미있을 것입니다.
02:36
여러분은 실제 단어 벡터를 평가하는 방법에 대한 좋은 감각뿐만 아니라
02:40
최소한 두 가지의 훈련 방법도 배울 것입니다.
02:45
Word2vec에 대해 간단히 살펴보겠습니다.
02:47
이 방정식으로 끝낼 것인데 여기서 우리는 기본적으로
02:53
중심 단어의 외부 벡터에 대해
02:57
무슨 뜻인지 정말 빨리 요약 할 수 있습니다.
03:01
저에게 말뭉치의 시작 부분이 있다고 가정 해 봅시다.
03:06
I like deep learning.
03:12
이나 그냥 NLP 같은 문장이 될 것입니다.
03:17
이제 우리가 할 일은 기본적으로 확률을 계산하는 것입니다.
03:21
예를 들어, 단어 벡터로 시작해서, 첫 번째 중심 단어는,
03:26
deep이 되겠네요.
03:28
먼저
03:33
deep 이라는 단어가 주어졌을 때, 첫 번째 외부 단어는 I로
03:37
그것은 UO의 지수와 같을 것입니다.
03:43
U 벡터는 외부 단어이므로
03:48
이 경우에, deep에 전치(transpose)될 것입니다.
03:52
그런 다음에 우리는 이 큰 합은 여기에 두고 합계는 특정 VC가 주어지면
03:57
언제나 동일합니다.
03:59
이것이 중심 단어입니다.
04:01
이제 어떻게 이 V와 U를  구할 수 있습니까?
04:03
기본적으로 여기에
04:08
모든 다른 단어들과 벡터와 함께 큰 매트릭스가 있습니다.
04:11
aardvark에 대한 벡터로 시작해서.
04:16
zebra에 대한 벡터에 이르기까지가 있습니다.
04:21
그리고 기본적으로 모든 중심 단어 v가 여기에 있고
04:24
하나의 거대한 행렬과 모든 벡터가 있습니다.
04:30
A와 aardvark 등으로 시작하여 zebra까지가 있습니다.
04:38
첫 번째 윈도우가 이 말뭉치를 통과할 때, 기본적으로,
04:44
이 벡터 deep을 위한  V 플러그를 여기에 넣고
04:48
이 확률을 극대화하고 싶을 것입니다.
04:54
그리고 지금, U와 같은 이 모든 다른 단어들,
04:59
I, like, learning, 등등에 대한 벡터를 취할 것입니다.
05:01
그래서 다음 할 일은 like를 위한 I나 또는 주어진 deep의 확률 같은 것입니다.
05:08
그리고 그것은 v의 전치처럼 U의 지수가 될 것입니다.
05:15
그리고 다시, 우리는 이것으로 나누어야만 합니다.
05:18
전체 어휘에 대해서 하면 꽤 큰 합이 되겠네요.
05:20
근본적으로 거의 모두가 분류 문제입니다.
05:23
이것이 이 말뭉치의 첫 번째 윈도우입니다.
05:27
이제 다음 윈도우로 이동할 때 기본적으로 한 윈도우를 넘깁니다.
05:33
그리고 지금 중심 단어를 통해서 이 주변 단어를 예측하고 싶을 것입니다.
05:41
이제 우리는 다음, 두 번째 윈도우를 위해 여기를 취할 것입니다.
05:46
이것은 첫 번째 윈도우, 두 번째 윈도우이었습니다.
05:49
우리는 이제 학습을 위해 벡터 V를 취하고 like, deep 과 NLP를 위한 U 벡터를 취할 것입니다.
05:57
그래서 이것이 지난 강의에서 이야기했던 skip gram 모델로
06:01
같은 기호로 다시 설명하는 것뿐입니다.
06:05
기본적으로 한 번에 하나의 윈도우를 가져와 다음 윈도우로 이동하는 방식으로
06:08
중심 단어 옆에 있는 외부 단어를 예측하려고
06:11
계속 노력할 것입니다.
06:14
이것에 대해서 질문이 있습니까?
06:15
왜냐하면 이제 다음으로 넘어갈 것입니다.
06:23
좋은 질문입니다. 어떻게 실제로 개발할 수 있냐는 질문이었습니다.
06:26
숫자로 시작할 때 이 모든 벡터는 무작위입니다.
06:30
일부의 작은 난수를 일으키면, 대체로 두 개의 작은 숫자들 사이에서 균등하게 샘플링 될 것입니다.
06:36
그 다음에, 여러분은 이 벡터들의 미분을 취하여
06:40
이 확률을 높일 수 있을 것입니다.
06:44
SGD를 사용하여 이러한 각 윈도우에 경사도를 구할 수 있을 것입니다.
06:50
그래서 지난번 라틴 강의에서 배운 도함수를 구하면,
06:55
이 모든 다른 벡터들에 관해서, 같은 결과를 얻을 수 있을 것입니다.
06:59
매우 큰 희소행렬(sparse)의 업데이트가 가능하겠네요.
07:02
왜냐하면 모든 매개 변수는 본질적으로 모두 단어 벡터입니다.
07:06
그리고 기본적으로 이 두 개의 행렬은 모든 다른 칼럼 벡터를 사용합니다.
07:10
그리고 여러분이 100 차원 벡터를 가지고 있다고 가정 해 봅시다.
07:14
우리는 2 만 단어 정도의 어휘가 있습니다.
07:18
그래서 우리가 최적화해야 하는 숫자가 많이 있습니다.
07:22
따라서 이러한 업데이트는 매우 범위가 크고
07:25
각 윈도우는 아주 희소(sparse)한 형태로 있습니다.
07:27
윈도우 크기가 2 인 경우 일반적으로 5 단어만 표시됩니다.
07:33
네? >> [안들림]
07:36
>> 좋은 질문입니다.
07:37
우리가 이러한 단어 벡터의 평가를 살펴보면 그것을 알 수 있을 것입니다.
07:44
이 비용 함수는 볼록(convex)하지 않고, 이와 상관없이
07:50
아 죄송합니다, 모든 질문을 반복해야 하는데.. 동영상으로 보는 분들께 죄송합니다.
07:52
첫 번째 질문은 우리가 어떻게 차원을 선택하는가 하는 것이었습니다.
07:55
우리는 곧 그것을 배울 것입니다.
07:58
그리고 다음 질문은,
08:00
“어떻게 시작하나요?” 하는 것과
08:01
“얼마나 중요하게 다루어야 합니까?” 였습니다.
08:03
이 수업에 나오는 대부분의 목적함수는
08:08
볼록하지 않으므로 초기화가 중요합니다.
08:13
또 몇가지 팁과
08:14
아주 나쁜 지역 최적(local optima) 상태에 갇히지 않는 방법에 대한 트릭으로 해결하려고 할 것입니다.
08:20
하지만 단어 벡터와 같은 작은 랜덤한 숫자로 작게 초기화하는 한
08:23
특히 이러한 단어 벡터와 같은 경우에는 거의 문제가  되지 않습니다.
08:30
좋습니다. 여기까지가 지난 강의를 요약 한 것입니다. 이제 기본적으로 SGD를 실행할 것입니다.
08:34
SGD를 실행하면 각 비용 함수가 여기에서 업데이트되고
08:39
말뭉치가 윈도우를 통과하게 되겠죠?
08:43
따라서 이러한 업데이트와 이를 구현하는 방법에 대해 생각할 때,
08:47
우리는 곧 문제 세트 1에 대해 알게 될 것이기는 하지만,
08:50
만약 전체 매트릭스가 있다면, 여기 전체 벡터가 있습니다.
08:55
모든 다른 수의 벡터는
08:57
사실 외현적으로도 0을 유지하고 있을 것입니다.
08:59
매우 큰 업데이트가 있게 되니까 메모리가 너무 빨리 소모될 것입니다.
09:04
어떻게 하면 될까요?
09:06
특정 칼럼만 업데이트하는 희소 행렬 연산을 통해
09:11
이 두 번째 윈도우에서는 
09:16
deep과 NLP 내부 벡터 같은 경우의 외부 벡터를 업데이트하기만 하면 됩니다.
09:20
또는 키와 값이 있는 해시로
09:26
구현할 수도 있습니다.
09:26
이때 값은 벡터이고, 키는 단어 문자열이 될 것입니다.
09:33
좋습니다, 지금, 제가 “이것이 skip-gram 모델입니다” 라고 말했을 때,
09:37
나는 실제로 한 번에 한 단계 씩 가르쳐주기 위해 약간 거짓말을 했는데
09:43
여기서 계산을 할 때는,
09:48
위 부분이 꽤 간단합니다, 맞죠?
09:50
이것은 단지 백 - 차원 벡터이고,
09:52
그것을 다른 100 차원 벡터로 곱하면 되는 것입니다.
09:54
그래서 꽤 빠릅니다.
09:56
하지만 각 창에서 다시 전체 말뭉치를 살펴보면,
10:00
한 번에 한 단계씩하고 또 한 번에 한 단어씩 합니다.
10:03
그리고 각 윈도우마다 이 계산을 합니다.
10:05
그리고 우리는 또한 이 거대한 합계를 합니다.
10:07
그리고 이 합계는 전체 어휘에 적용됩니다.
10:09
다시 말하지만 여러분의 전체 말뭉치에서는 잠재적으로 이만 개의 단어가
10:14
될 수도 있습니다.
10:16
그러면 각 윈도우에서,
10:18
이 내적을 이만 번 만들어야 하는데
10:23
그다지 효율적이지 않아 보입니다.
10:25
그리고 모델이 그렇게 많이 가르쳐주지는 않습니다.
10:28
각 윈도우에서 deep learning이나 learning은 zebra와 동시에 등장하지 않고
10:35
aardvark와도 동시에 등장하지 않습니다.
10:36
20,000 개의 다른 단어와도 동시에 등장하지 않습니다.
10:39
반복적으로 그럴 것입니다.
10:40
왜냐하면 대부분의 단어는 실제로 대부분의 다른 단어와 함께 표시되지 않을 뿐만 아니라 꽤 희소합니다.
10:44
그리고 skip-gram의 주요 아이디어는 매우 정교한 트릭이고
10:50
우리는 진정한 쌍을 찾아서 이진 로지스틱 회귀를 돌리고 싶습니다.
10:53
이 아이디어를 최적화하고 싶다면
10:57
중심 단어와 주변 단어의 내적을 극대화하여야 합니다.
11:01
그러나 모든 것을 거치지 않고,
11:03
우리는 실제로 무작위로 몇 가지 단어를 선택한 다음에
11:05
어떻게 이 임의의 단어가 말뭉치의 다른 단어와 공기하지 않지? 라고 말할 것입니다. 
11: 11
우리가 종종 소프트웨어 패키지 중에서 Word2vec이라 불리는 
11:15
skip-gram 모델의 원래 목적 함수를 돌이켜 봅시다.
11:19
원래의 논문 제목은 단어와 구의 분산 표현과
11:23
그 합성성이었습니다.
11:25
그리고 전체적인 목적 함수는 다음과 같습니다.
11:27
천천히 들여다봅시다.
11:30
기본적으로, 각 윈도우를 다시 보게 됩니다.
11: 33
여기 T는 우리가 말뭉치를 통과 할 때 각 윈도우에 해당합니다.
11:38
그 다음 여기에 두 가지 용어가 있습니다.
11:39
첫 번째 것은 본질적으로 이 두 중심 단어와
11:44
공기하는 주변 단어의 로그 확률이며
11:47
여기서 시그모이드는 간단한 요소별 함수(element wise function)로
11:50
좋은 친구처럼 될 것입니다.
11:51
이 수업에서 시그모이드 함수를 많이 사용하기 때문에
11:53
그 미분을 잘 할 수 있어야 합니다.
11:57
그러나 본질적으로는 단지 모든 실수로
12:00
0과 1 사이를 채우는 것에 해당합니다.
12:02
딥러닝을 배우는 사람들에게는 확률이라고 부르기에 충분한 것으로
12:06
통계를 배웠다면 적절한 측정을 계속하고 싶을 것입니다.
12:09
그래서 그렇게까지는 아니지만 0과 1 사이의 숫자들로 이루어진 이것을
12:12
확률이라고 부를 것입니다.
12:13
그리고 우리가 이 용어를 통해서 기본적으로 여기서 하고 싶은 것은
12:19
이 두 단어가 동시에 발생하는 로그 확률을 최대화하는 것입니다.
12:25
첫 번째 term에 대해 궁금한 점이 있나요?
12:30
그럼, 이것과 매우 비슷한 두 번째 term를 살펴 보겠습니다.
12:33
기댓값이라고 설명했던 것인데
12:37
실제로 좀 더 명확한 표기법으로 이해하기 위해서
12:41
말뭉치에서 몇 개의 단어를 무작위로 추출해 봅시다.
12:45
그리고 이들 각각에 대해,
12:47
동시 발생 확률을 최소화하려고 노력할 것입니다.
12:52
중간고사를 준비하는
12:56
좋은 연습의 하나로
13:00
다음의
13:05
(- x)의 시그모이드는  1-시그모이드(x)이다.
13:11
를 증명하지 않으면 어떻게 되는지를 보는 것도 짧고 간결한 증명으로 좋은 연습이 될 것입니다.
13:14
그리고 기본적으로 이것은 1에서 저 확률 값을 뺀 것입니다.
13:18
그래서 임의의 단어 몇 개를 포함한 서브샘플을 시도해서 
13:22
aardvark이 나타나지 않는다거나
13:24
얼룩말은 학습에서 나타나지 않는 것을 해결하려고 할 것입니다.
13:26
우리는 5개나, 10개를 샘플링해서 그 확률을 최소화합니다.
13:33
그리고 보통
13:34
이것은 다시 하이퍼 파라미터이며,
13:39
중요도에 따라서 평가된 k 개의 negative samples을 통해서
13:41
여기가 각 윈도우에 대한 목적 함수로 두 번째 부분입니다.
13:45
그 다음 이 임의의 단어가 
13:49
중심 단어 주위에 나타날 확률을 최소화할 것입니다.
13:51
그런 다음에 단순한 유니폼이나
13:55
유니그램의 분포로부터 실제 샘플링을 할 것입니다.
13:56
기본적으로 얼마나 단어가 얼마나 자주 나타나는지 보고
14:00
그것을 바탕으로 샘플을 만들 것입니다.
14:01
그러나 우리는 또한 3/4 제곱을 취하게 될텐데
14:03
이것은 일종의 hacky term으로
14:04
이 모델로 충분히 오래 놀면,
14:08
어쩌면 이 저빈도 단어의 일부를 더 자주 샘플링 하거나. 그렇지 않으면,
14:12
더 자주 샘플링되는 The와 A와 다른 불용어(stop words) 때문에
14:16
우리 말뭉치에서 aardvark와 zebra을 샘플링할 일이 한 번도 없을지도 모릅니다.
14:20
우리가 3/4의 제곱을 취하면
14:25
이 기능을 구현할 필요가 없게 됩니다.
14:27
우리가 이것을 다룬 이유는 통계적 계산을 할 때
14:31
각 단어의 말뭉치 빈도와 관련된 것을 알려준 것입니다.
14:33
그러나 문제 세트에는 이것을 있을 것입니다.
14:37
Skip-gram 모델에 대한 질문이 있나요?
14:40
네?
14:46
맞습니다. 질문은 w의 p를 정의하는 방식을 선택할 수 있습니까? 입니다.
14:53
선택할 수 있습니다, 우리는 많은 다른 것들도 할 수 있습니다.
14:57
그러나 실제로 유니그램 분포를 사용하는 것과 같은 매우 간단한 작업으로도
15:02
단어 빈도는 잘 작동하게 할 수 있습니다.
15:05
그래서 사람들은 그보다 복잡한 버전을 실제로 탐구하지 않았습니다.
15:19
좋은 질문입니다.
15:20
여기 있는 무작위 샘플은
15:24
정확히 이 단어와 같지 않습니까?라는 것입니다.
15:28
맞습니다. 하지만 매우 큰 말뭉치에서 그 확률은
15:32
아주 적으므로 무의미하다고 볼 수 있습니다.
15:36
왜냐하면 무작위로 하위 샘플 하는 정도에서는 변화가 없습니다.
15:48
어느 부분이죠?
15:51
K? 10 개입니다.
15:53
상대적으로 작고 흥미로운 트레이드 오프입니다.
15:56
우리는 실제로 몇 가지 딥러닝 모델을 관찰 할 것입니다.
15:59
흔히 말뭉치가 통과 할 때, 각 윈도우마다 업데이트를 수행 할 수 있습니다.
16:04
하지만 5 개의 윈도우를 통해서 보면서 업데이트를 수집하고
16:08
한 단계씩
16:10
확률적 경사 하강법(stochastic gradient descent )의 미니 배치 등의
16:12
클래스에서 옵션을 여러 개 볼 것입니다.
16:17
skip-gram에 대한 마지막 질문으로 Jt (theta)는 무엇을 나타내나요?
16:25
좋은 질문입니다.
16:26
theta는 종종 우리 모델의 모든 변수에 사용되는 매개 변수입니다.
16:33
따라서 skip-gram 모델의 경우,
16:35
그것은 본질적으로 모든 U 벡터와 모든 V 벡터입니다.
16:39
나중에 우리가 theta를 불러들일 때
16:42
그것은 뉴럴 네트워크, 레이어 등의 다른 매개 변수를 가질 것입니다.
16:46
그리고 J는 우리의 비용 함수이고 T는 T 번째 타임스텝이거나
16:50
말뭉치를 통과하는 T 번째 윈도우가 될 것입니다.
16:53
결국 우리가 실제로 최적화하는 우리의 전반적인 목적 함수는
16:56
이들 모두의 합계가 될 것입니다.
16:57
그러나 다시 말하지만, 우리는 전체 말뭉치에 대해 한 번에 큰 업데이트를 하지는 않을 것입니다.
17:02
또한 모든 윈도우를 통해 모든 업데이트를 수집하는 것도 좋지 않을 것입니다.
17:06
왜냐하면 이렇게 거대한 스텝은 잘 작동하지 않기 때문입니다.
17:12
좋은 질문입니다, 제 생각에는 지난 강의에 최소화(minimization)에 대해 여러 번 이야기했었습니다.
17:16
오늘은 논문에 나온 로그 확률과 이를 최대한 활용하고자 하는 것이
17:24
더 자주 직관적일 수 있을 것입니다.
17:25
확률을 얻은 후에는 일반적으로
17:29
우리의 말뭉치에서 실제로 볼 수 있는 확률을 최대화하고 싶을 것입니다.
17:32
그리고 다른 때에는,
17:33
비용 함수를 최소화하고 싶을 것입니다.
17:39
좋습니다. word2vector의 또 다른 모델은
17:43
여러분이 포인트를 얻고 싶지 않으면 구현할 필요가 없습니다.
17:46
하지만 미분을 이해하는데
17:49
아주 간단한 개념 수준에서 이해하는 것은 매우 좋습니다.
17:53
그런데
17:57
기본적으로,
18:00
주변 단어의 합으로부터 중심 단어를 예측한다는 점에서 skip-gram 모델과 매우 유사합니다.
18:04
여기에서 매우 간단히 말하자면, 우리는 NLP의 벡터와 deep의 벡터와
18:08
like의 벡터를 합칠 것입니다.
18:10
그러면 내부에 벡터가 있는 내적을 얻게 될 것인데
18:14
이것을 continuous bag of words 모델이라고 부를 것입니다.
18:18
이 문제 세트에 있는 모든 세부 정보와 정의에 대해 배우게 되겠지만
18:23
우리가 이 단어 벡터를 훈련 할 때 실제로 무슨 일이 일어나는 것일까요?
18:27
이 목적 함수를 최적화하고 경사도를 구하면
18:31
잠시 후에, 어떤 종류의 마술이 단어 벡터에 발생합니다.
18:37
실제로 비슷한 의미를 중심으로 클러스터가 되기 시작하는 것입니다.
18:43
때로는 비슷한 종류의 통사론적 기능도 포함할 수 있습니다.
18:47
그래서 확대해서 보면, 다시 말하지만, 보통 이 벡터들은 25나 심지어
18:53
500 또는 1000 차원의 벡터의 PCA로 시각화한 것입니다.
18:58
그리고 우리가 관찰 할 수 있는 것은 화요일과 목요일의
19:03
클러스터가 함께 있는 것과, 숫자 용어 클러스터 함께 있는 것,
19:08
성씨들의 클러스터 등을 볼 수 있습니다.
19:12
기본적으로 비슷한 맥락에서 나타나는 단어들은
19:16
지난 시간에 이야기 한 것과 같이 비슷한 의미를 갖고
19:19
본질적으로 비슷한 벡터로 나타납니다.
19:24
우리는 충분한 수의 세트로 이 모델을 훈련시켰습니다.
19:30
좋아요, 이제 word2vec을 요약해 봅시다.
19:33
기본적으로 우리는 말뭉치의 각 단어를 통해서
19:36
윈도우 안의 주변 단어를 보고 나서
19:38
주변 단어를 예측했습니다.
19:40
자, 우리가 근본적으로 하고 있는 것은
19:43
단어의 동시 발생을 포착하려 합니다.
19:46
어떤 단어는 얼마나 자주 다른 단어와 함께 나타납니까?
19:49
이것을 한 번에 하나씩 계산했는데
19:51
그것은 deep과 learning을 통해 살펴보았습니다.
19:56
그런 다음에 이 두 벡터를 모두 업데이트합니다.
19:58
그리고 말뭉치를 살펴보았습니다.
20:02
동시 발생(=공기 관계)을 학습하고 다시 별도의 업데이트 단계를 확인하였습니다.
20:06
다시 생각해보면 매우 비효율적이지는 않습니까?
20:08
왜 전체 말뭉치를 한 번에 하면 안됩니까? 얼마나 자주 deep과 learning의
20:12
두 단어가 동시발생 하는지 학습하고, 이것을 통해 한 번 업데이트 
20:18
할 때 한 번에 하나의 샘플을 하는 대신, 전체 카운트를 업데이트하는 단계를
20:25
만들 수도 있습니다.
20:27
그리고 이것은 사실 역사적으로 word2vec 이전에 사용 된 방법입니다.
20:33
그리고 우리가 이것을 할 수 있는 방법에는 여러 가지 옵션이 있습니다.
20:36
가장 간단한 것 또는
20:37
word2vec과 가장 유사한 것은 우리가 다시 각 단어의 윈도우를 사용하여
20:41
전체 말뭉치를 통과시키는 방법입니다.
20:45
우리는 아무것도 업데이트하지 않으며 SGD를 수행하지 않습니다.
20:47
먼저, 카운트만 수집합니다.
20:49
그런 다음에 카운트를 얻으면, 매트릭스 계산을 합니다.
20:53
이 예제에서와 같이 각 단어의 크기에 따라 길이 2의 윈도우를 보았거나 
20:56
또는 5를 보았다면
21:01
우리가 할 일은 의미뿐만 아니라
21:05
또한 각 단어의 통사 정보의 일부까지 수집하는 것입니다.
21:07
즉, 어떤 종류의 품사표를 붙일 것인가가 될 것입니다.
21시 10 분
그래서 동사는 서로 가깝게 있게 될 것이고
21:13
그 다음 동사에서 명사로 가는 것과 같은 예가 될 것입니다.
21:18
반면에, 우리는 단지 동시 발생의 카운트로
21:21
윈도우 주변만이 아니라 전체 문서를 보는 것입니다. 그래서 각 윈도우가 아니라
21:25
제 말은 모든 위키피디아 전체나
20:30
또는 예를 들어, Word 문서 전체의  빈도를 보겠다는 것입니다.
21:33
그러면 실제로 더 많은 주제를 수집할 수 있는데
21:39
잠시 동안 크게 인기 있었던 모델인 잠재 의미 분석(Latent Semantic Analysis)라고 불렀던 것입니다.
21:44
기본적으로 우리가 얻을 수 있는 것은, 
21:48
어떤 종류의 통사 정보도 무시하고 단지,
21:53
수영과 보트와 물과 날씨와 태양,
21:56
그런 것들이 모두 함께 이 주제나 이 문서에 함께 등장한다는 것을 알면 됩니다.
22:01
세부 사항까지 자세히 들어가지 않아도 되는데 왜냐하면
22:05
기계 번역과 같은 많은 다른 다운 스트림 작업에
22:08
이 윈도우를 정말 사용하고 싶지만 그러러면 좋은 지식이 있어야 합니다.
22:12
그래서 지금은 아주 작으면서 우리가 할 일의 간단한 예만 살펴보겠습니다.
22:18
작은 말뭉치를 윈도우로 수집하고 그로부터 단어 벡터를 계산할 것입니다.
22:44
(질문) 기술적으로 길이에 따라 정규화하지 않았기 때문에 코사인이 아니고,
22:48
확률의 내적으로 최적화하지 않았습니다.
22:51
(질문) 계속하세요.
23:00
맞습니다. 그래서 질문은
23:01
여기 모든 시각화에서 유클리드 거리를 살펴보았는데,
23:05
그런데 사실 
23:10
일종의 유사성 측정으로 내적을 사용하고 있는 것이냐 하는 것이었습니다.
23:12
예, 어떤 경우에는 유클리드 거리가 합리적으로 잘 작동함에도 불구하고
23:17
사용하지 않는 것은 우리는 전적으로 하나의 평가만 한 경우와
23:22
부분적으로 유클리드 거리, 부분적으로 내적을 기반으로 한 것을 모두 실험해본 결과
23:25
각각의 객관적 기능에도 불구하고 이 두 가지가 모두 잘 작동한다는 것을 알 수 있습니다.
23:30
그리고 훨씬 더 놀랍게도
23:33
이런 종류의 목적 함수로 시작하더라도 아주 잘 작동하는 많은 것들이 있습니다.
23:41
우리는 종종 예, 그렇기 때문에 내적 최적화만 잘 수행하더라도,
23:45
유클리드 거리의 관점에서도 매우 잘 할 것이라고 추측할 수 있습니다.
23:50
네.
23:54
글쎄요, 복잡하지만 몇 가지 흥미로운 관계가 있습니다.
23:59
동시 발생을 카운트한 비율은 시간이 충분하지 않기 때문에
24:04
세부 사항에 관심이 있다면 논문을 알려 주도록 하겠습니다.
24:07
슬라이드 5나 또는 10에서 이 논문의 제목을 언급했습니다.
24:12
조금 더 잘 이해하고 좀 더 직관력을 얻을 수 있을 것입니다.
24:15
좋아요, 그럼, 윈도우 기반의 공기 관계 매트릭스를 봅시다.
24:20
여기에 말뭉치가 있습니다.
24:22
윈도우 길이를 1로 간단히 나타냅니다.
24:26
일반적으로, 5에서 ~ 10 의 윈도우를 더 일반적으로 사용합니다.
24:30
우리는 대칭 윈도우를 가지고 있다고 가정합니다.
24:34
우리는 단어가 우리 센터 단어의 왼쪽이나 오른쪽에 있는지 상관하지 않습니다.
24:38
이 말뭉치를 가지고
24:39
윈도우 기반 공기 관계 매트릭스로 만든 것입니다.
24:44
이 아주, 아주 간단한 말뭉치에서
24:46
I 라는 단어를 보고, 그 다음에는 I 옆에 어떤 단어가 나타나는지 살펴봅니다.
24:51
like가 두 번 나타났습니다, 우리는 여기에 2라고 씁니다.
24:56
그 다음에, enjoy를 한 번 보았고 역시 카운트를 셉니다.
25:00
그리고 like가 있고
25:02
I와 두 번 동시 발생했다는 것도 알 수 있습니다.
25:07
deep이 한 번 NLP도 한 번이네요
25:09
매우 큰 말뭉치도 같은 방식으로 모든 단어로 이용해서
25:14
카운트 센 것을 매우 간단하게 계산할 수 있습니다.
25:17
이제, 이미 벡터가 되었다고 말할 수 있겠네요.
25:20
여기에 숫자 목록이 있고 그 숫자 목록이 이제 그 단어를 나타냅니다.
25:24
그리고 우리는 이미 어떤 것을 포착합니다, 음, like와 enjoy가 약간 겹치기 때문에,
25:29
아마 더 유사한
25:30
단어 벡터를 가지게 되었습니다, 그렇죠?
25:32
그러나 지금은 몇 가지 이유로 아주 이상적인 단어 벡터가 아닙니다.
25:36
첫 번째로 어휘집에 새로운 단어가 있다면,
25:39
그 단어 벡터가 바뀔 것입니다.
25:41
그래서, 만약 여러분이 downstream 기계 학습 모델을 가지고 있다면
25:44
벡터의 입력을 항상 변경해야 합니다. 그리고 일부 매개 변수가 빠져 있습니다.
25:48
또한, 이 벡터는 매우 고차원이 될 것입니다.
25:51
물론 이 말뭉치는 작지만 일반적으로,
25:54
수만 단어로 이루어져 있는
25:56
큰 고차원적인 벡터가 될 것입니다.
25:58
따라서 기계 학습 모델을 교육하려고 하면 희소성 문제가 발생합니다.
26:01
그 다음에는 덜 강건한 downstream 모델로 나아가게 됩니다.
26:07
그래서 해결책으로 word2vec와 비슷한 아이디어를 다시 생각하게 되었습니다.
26:12
모든 동시 발생 횟수나, 모든 단일 숫자를 저장하지는 않고 
26:16
중요한 정보의 대부분을 저장한
26:19
word2vec과 비슷하게 고정된 작은 수의 차원으로
26:22
약 25 ~ 1,000 차원의 어딘가에 있을 것입니다.
26:26
그 다음 드는 의문은 이제 어떻게 차원을 감소시킬 수 있을까 일 것입니다.
26:29
여기에 매우 큰 공기 관계 행렬들(co-occurrence matrices)이 있습니다.
26:32
현실적인 상황에서는 20,000 x 20,000 또는 심지어 백만 x 
26:36
백만 개의 매우 큰 희소행렬이 될 텐데 그 때는 어떻게 차원을 줄일 수 있겠습니까?
26:41
아주 간단한 SVD를 사용하는 것입니다.
26:44
누군가 특이값 분해에 익숙한가요?
26:49
좋아요, 좋습니다, 대다수의 사람들이 익숙한데, 만약 그렇지 못하다면
26:52
선형 대수학 수업에서 공부할 것을 강력하게 권합니다.
26:59
기본적으로, 여기에 이 X hat 매트릭스가 있습니다.
27:04
원래의 공기 관계 매트릭스 X에 대한 최상위 rank k에 근사하게 될 것입니다.
27:10
그리고 직교 칼럼이 있는 이 세 가지 간단한 행렬이 생길 것입니다.
27:18
U 종종 left-singular 벡터라고 하는 것이 있고 S 대각 행렬도 있습니다.
27:23
일반적으로 가장 큰 값에서 가장 작은 값까지의 모든 특이값(singular values)을 포함하는 행렬입니다.
27:28
여기에 행렬 V가 있고 정규직교 열(orthonormal row)입니다.
27:32
코드에서 구현하는 것은 매우 간단합니다.
27:36
우리는 문자 그대로 몇 줄이면 구현할 수 있습니다.
27:41
이 부분이 말뭉치이고, 이 부분이 공기 관계 매트릭스 X를 나타냅니다.
27:46
그 다음 한 줄의 파이썬 코드로 SVD를 실행하면
27:51
행렬 U를 얻을 수 있습니다.
27:54
그리고 이제, 우리는 U의 처음 두 칼럼을 가져 와서 그것들을 시각화 할 수 있습니다, 맞습니까?
28:02
여기 첫 2 차원으로 만들면 제가 보여 줬던 것과 같이 실제로
28:07
다른 모든 것들과 함께 시각화 할 수 있습니다.
28:12
이것은 몇 줄의 파이썬 코드로 그와 같은 단어 벡터를 만든 것입니다.
28:18
지금은, tea leaves을 읽는 것 같아서 그 어떤 차원도
28:23
명사의 차원이라거나, 동사성 단어라고 말할 수 없습니다.
28:29
그러나 이것들이 충분히 길면,
28:32
확실히 어떤 종류의 패턴을 관찰 할 수 있을 것입니다.
28:35
예를 들어, I와 like는 이 말뭉치에서 고빈도 단어이기 때문에
28:40
좀 더 왼쪽에 있습니다. 이것이 하나입니다.
28:42
이 공간에 Like와 enjoy는 최근접이웃(nearest neighbors)입니다.
28:46
또 다르게 관찰해 보면, 둘 다 동사입니다.
28:50
그리고  liked, flying
28:54
deep 과 다른 것들은 서로 더 가까이 있습니다.
28:58
아주 간단한 방법으로 어떤 단어에 대한 첫 번째 근사값을 얻습니다.
29:02
벡터를 포착 할 수 있을 뿐만 아니라 포착해야 합니다.
29:07
공기관계 행렬의 SVD 방법과 관련하여 질문이 있습니까?
29:18
윈도우은 항상 대칭입니까? 라고 물었습니다. 좋은 질문입니다. 
29:20
대답은 '아니오'입니다. 실제로 비대칭 윈도우와 대칭 윈도우를 평가할 수 있습니다.
29:25
그리고 그 결과를 슬라이드 몇 장으로 보여 드리겠습니다.
29:32
자, 일단 깨닫고 나니까 와우, 이건 너무 간단한데 잘 작동하네요!
29:36
그런데 연구원이라면 늘 뭔가를 조금 향상 시키고 싶을 것입니다.
29:40
그래서 많은 다른 방법들을 사용해서
29:44
공기관계 행렬을 만들 수 있을 것입니다.
29:44
그래서, 예를 들어, 카운트를 그냥 취하는 대신에, 이렇게 하면,
29:49
이 단어 벡터의 표현력이 많이 포착될 수 있는데요.
29:55
사실, the와 he 그리고
29:58
has와 같은 고빈도 단어는 거의 항상 명사와 함께 나타납니다.
30:03
거의 모든 명사의 윈도우에 반복적으로 나타난다면
30:08
실제로 그다지 많은 정보를 제공하지는 않습니다.
30:12
다시 말해서,
30:13
그래서 우리가 할 수 있는 한 가지는 실제로 그것을 감싸고 말하는 것입니다.
30:17
좋아요. 가장 많이 발생하는 것이 무엇이든지간에, 그리고 다른 많은 것들이
30:21
이 기능어 중 하나를 100까지만 최대화하거나
30:26
이미 아시는 분도 있겠지만 그냥 몇 가지를 무시합니다.
30:30
단어가 멱법칙(power law)의 분포를 따른다는
30:33
지프의 법칙에 따르면, 기본적으로 가장 빈번하게 나타나는 단어는
30:37
다른 단어보다 훨씬 더 자주 등장하고 그 다음에는 점점 가늘어진다고 합니다.
30:40
의미 항목에서도 저빈도 단어가 긴 꼬리처럼 나타나게 됩니다.
30:46
매우 희소하게 나타나는 단어는 종종 많은 의미론적 콘텐츠를 가지고 있습니다.
30:51
바꿀 수 있는 다른 방법은,
30:54
계산하는 방식을 바꿔서 모든 단어를 똑같이 세지 않는 것입니다.
30:58
이렇게 말할 수 있을 것 같은데,
30:59
제 중심 단어 바로 옆에 나타나는 단어는 1을 계산했다면,
31:03
그 단어와 5 스텝 떨어져 있는 단어는
31:05
0.5로 계산하는 것입니다.
31:08
우리가 할 수 있는 또 다른 괜찮은 방법은
31:10
카운트를 세는 대신에 상관관계(correlations)를 계산하여 0으로 설정할 수 있습니다.
31:14
이 공기관계를 세는 것은
31:19
다양한 방법으로 계산할 수 있으며 때로는 상당히 유의미한 결과를 냅니다.
31:25
꽤 오래 전인 2005 년에 사람들은 이 SVD 방법을 사용했고
31:30
다른 많은 방법들과 비교해서
31:33
공기관계 행렬을 파악하고 수정한 결과
31:38
놀라운 것을 발견했습니다.
31:42
여기 시각화를 할 수 있는 또 다른 방법도 있는데,
31:45
매우 고차원인 공간에서
31:46
다시 말하지만, 이 벡터들은 일반적으로 약 100 차원 정도이므로
31:50
시각화하기가 어렵습니다.
31:51
그래서 2D로 투영하는 대신 여기에서는 몇 개의 단어를 선택해서
31:55
최근접 이웃들(nearest neighbours) 즉, 어떤 단어가 있다면 하면 가장 가까운 
32:00
단어를 찾아 보았는데 wrist과 ankle이 서로에게 가장 가깝다고 느꼈고
32:04
shoulder 도 가장 가까운 단어였습니다.
32:06
그리고 그 다음으로 가장 가까운 것은 arm 등이었습니다.
32:09
다른 군집들이 여기저기 모여 있는 것이 있는데, 다른
32:13
도시 이름의 군집들로 미국의 도시는
32:18
다른 나라의 도시 및 국가 이름과 서로 가깝게 위치합니다.
32:22
정말 놀랍죠?
32:23
이 윈도우 주위에 SVD와 같은 단순한 것을 통해서도,
32:27
우리는 많은 다른 종류의 정보를 포착할 수 있습니다.
32:32
심지어 통사론적인 것도
32:35
이 SVD 방법으로 포착할 수 있습니다.  패턴의 크로마틱한 종류로
32:40
show, showed, shown 또는 take, took, taken 과 같은 것도 볼 수 있습니다.
32:45
늘 비슷한 패턴은 항상 함께 있습니다.
32:52
그리고 이것은 동사에서 더욱 의미론적인 것이 되어
32:58
비슷한 종류의 관련된 명사와 함께 나타납니다.
33:04
유클리드 거리에서도 종종 비슷한 종류는 함께 나타나는데
33:09
swim과 swimmer, clean과 janitor, drive와 driver, teach와 teacher는
33:17
기본적으로 비슷한 종류의 벡터로서의 차이일 뿐입니다.
33:25
그리고 직관적으로
33:28
비슷한 문맥에서 자주 더 잘 나타날 것이라고 생각할 수 있습니다.
33:33
왜 이런 일이 일어날지에 대한 직관적인 감각으로
33:36
공기 관계를 세서 포착하려고 할 것입니다.
33:45
(질문) 어떤 언어인지가 문제가 됩니까? 
33:47
네, 어떤 방법으로요?
33:52
좋은 질문입니다.
33:53
그래서 그것이 영어가 아니라 독일어라면,
33:56
실제로 자연어 처리 연구의
34:01
대다수가 영어라는 것은 슬픈 사실입니다.
34:03
그리고 소수의 사람들이 하고 있기는 하지만
34:05
다른 많은 언어에서도 작동하고 있기는 합니다.
34:08
그러나 사람들은 종종
34:11
다른 언어 및 평가 데이터 세트의 좋은 평가 척도를 갖지 못합니다.
34:15
그러나 우리는 거의 모든 언어에서 작동한다고 믿습니다.
34:19
핀란드어나 독일어와 같은 일부 언어는
34:24
잠재적으로 많은 다른 단어들에 비해 훨씬 더 풍부한 형태(morphology)가 있습니다.
34:28
독일어에는 복합 명사가 있습니다.
34:30
그러면 더 많은 수의 희소한 단어가 있을 수 있고
34:33
희소한 낱말의 빈도를 세어도 좋은 결과를 얻기가 어렵습니다.
34:38
그래서 기본적인 방식을 사용하기는 어렵고
34:43
결국 글자 단위(character-based)로 하게 될 것입니다.
34:46
몇 주 안에 이것을 공부하겠지만
34:49
일반적으로 거의 모든 언어에서 작동합니다.
34:52
좋은 질문이었습니다.
34:54
자, 여기서 뭐가 문제였죠?
34:56
SVD는 매우 간단하면서
34:59
파이썬 코드 한 줄이면 돌아가지만 실제로 계산상 늘 훌륭한 것은 아닙니다.
35:03
더 큰 행렬을 계산해야 할 때는 특히 그렇습니다.
35:07
기본적으로 낮은 차원으로 가면 2차 손실(quadratic cost)이 생깁니다.
35:12
공기관계 행렬에 의한 단어이거나 또는
35:15
문서에 나타난 단어 하나라도, 매우 크다는 가정이 있습니다.
35:18
또 새로운 단어나 문서를 통합하기가 어렵다는 점도 있습니다.
35:23
이 전체 모델을 다시 돌려서  PCA 죄송합니다.
35:28
SVD, 즉, 특이값 분해,
35:31
그 SVD 위에서 돌릴 수 있습니다.
35:33
우리가 최적화하는 방법은 다른 downstream 딥러닝 방법과 상당히 다릅니다.
35:37
우리가 뉴럴 네트워크와 같은 방법을 쓰는데
35:40
그것은 매우 다른 종류의 최적화입니다.
35:42
그래서 word to vec의 목적 함수는 SVD와 비슷합니다.
35:45
한 번에 하나의 윈도우를 보고
35:47
한 단계 업데이트합니다.
35:48
그리고 매우 비슷하게 모델의 최적하고
35:52
이 딥러닝 자연어처리 강좌에서도 그렇게 합니다.
35:55
그래서 기본적으로 박사후 과정인 분들과
35:59
크리스 그룹, 제프리 페닝턴, 저 그리고
36:02
크리스는 두 세계의 장점을 결합하려고 시도하는 했습니다.
36:07
각각의 장점과 단점을
36:10
요약해 보면 이 두 가지 다른 점은
36:13
기본적으로 SVD는
36:16
빈도를 세서 공기 관계 매트릭스를 보는 방법이고
36:18
윈도우 기반으로 직접 예측하는 방법인
36:19
Skip-Gram 모델을 쓸 수도 있습니다.
36:24
PCA의 장점은 훈련이 비교적 빠르다는 것입니다.
36:28
행렬이 너무 커지면 의미 없지만
36:31
우리가 가진 것들을 통계 낼 때는 매우 효율적으로 사용할 수 있습니다. 맞습니까?
36:35
이론적으로, 통계를 한 번 수집하고
36:38
전체 말뭉치를 버립니다.
36:39
그 다음에 동시 발생의 빈도를 센 것을 가지고 다른 많은 것들을 시도합니다.
36:44
슬프게도, 이 때, 주로 단어 유사성(word similarity)에 포착해서,
36:49
word2vec 모델에서 포착한
36:53
평가 대상이 무엇인지 보여 주게 됩니다.
36:56
그런데 이렇게 하면 종종 이러한 큰 수에 불균형 중요성을 부여할 수도 있습니다.
37:00
그래서 이 기능어들과 고빈도 단어들이 가지고 있는 
37:04
중요성을 낮추는 다양한 방법을 시도하게 됩니다.
37:08
Skip-Gram 모델의 단점은
37:13
말뭉치 크기로 비례해서
37:16
모든 윈도우를 통과해야 한다는 것입니다.
37:18
매우 효율적이지는 않습니다. 그리고 앞으로는
37:23
전체적 데이터 집합의 통계를 효율적으로 사용할 것입니다.
37:28
실제로, 경우에 따라,
37:30
다운스트림 작업의 성능이 훨씬 향상되었습니다.
37:32
아직 확실하지 않지만
37:33
이러한 다운스트림 작업 때문에 우리는 이 분기에 전체에 대한 강연을 해야 하는 것입니다.
37:36
개체명 인식이나
37:40
품사 태깅 같이 다양한 문제들을 
37:42
문제 세트로 구현할 사항은
37:44
skip-gram과 같은 모델이 약간 더 잘 작동한다고 밝혀졌습니다.
37:49
다양하고 복잡한 패턴을 포착 할 수 있다는 점에서
37:52
매우 놀라운데,  이 강의의 두 번째 부분에 다룰 것입니다.
37:56
그리고 기본적으로,
37:57
우리가 여기에서 한 것은 이 두 세계의 장점을 결합한 것입니다.
38:02
그 결과 Global Vectors 모델인 GloVe 모델이 탄생했습니다.
38:07
이 목적 함수를 조금씩 살펴 보도록 하겠습니다.
38:09
여기서도 세타가 모든 매개변수가 됩니다.
38:13
그래서 이 경우 다시 우리는 이 U와 V 벡터를 가집니다.
38:16
그러나 그들은 지금 더 대칭입니다.
38:18
우리는 기본적으로 동시에 발생할 수 있는 모든 단어 쌍을 통과합니다.
38:23
그리고 이 매우 큰 공기 관계 매트릭스를 살펴보면,
38:26
처음에 계산했던 이것을 P라고 부릅니다.
38:29
그리고 이 전체 말뭉치에 있는 각 단어 쌍에 대해,
38:33
우리는 근본적으로 내적의 거리를 최소화하기를 원하고
38:38
여기서는 두 단어의 로그 수로 표시됩니다.
38:43
다시 말하지만, 이것은 우리가 해왔던 바로 그 종류의 행렬입니다.
38:48
그리고 다시 공기 관계 매트릭스의 모든 요소를 ​​다룰 것입니다.
38:53
그러나 커다란 SVD를 실행하는 대신,
38:56
우리는 기본적으로 한 번에 하나씩 빈도를 세는 방법으로 최적화할 것입니다.
39:02
거리를 제곱하고
39:05
또한 여기에서 f라는 용어를 사용했는데
39:09
이러한 매우 빈번한 동시 발생의 가중치를 낮추어 줄 것입니다.
39:14
예를 들어, the는 최대로 많이 나타날 것인데 우리는 
39:18
이 목적 함수 안에서 가중치를 받을 것입니다.
39:24
이해되나요?
39:25
지금 우리가 기본적으로 훈련 속도가 매우 빨라졌습니다.
39:28
deep과 learning 하나의 윈도우에서 동시에 발생하는 것을 최적화하고
39:32
그 다음에 몇 개의 윈도우문에서 다시 동시 발생하면
39:35
다시 업데이트하고, 
39:38
이 전체 말뭉치에서  deep learning이 한 번에 말로 나타나면 다시 업데이트하는 것입니다.
39:40
이제 위키피디아나 일반적으로 크롤링한
39:44
인터넷의, 놀라운, 
39:47
수십억 개의 토큰을 가진 거대한 말뭉치가 있을 것입니다.
39:50
그런 다음에, 좋아요. 이제 deep과 learning이
39:52
이 수십억 개의 문서에서 536 번 정도나
39:57
아마 지금은 훨씬 더 자주 나타나겠지만,
39:58
이것을 최적화 할 것입니다. 이 내적이 끝났을 때
40:03
전체 빈도를 센 것의 로그에 가치가 있습니다.
40:11
그 때문에, 매우 큰 말뭉치로 확장하는 게 좋습니다.
40:14
그러면, 자주 나타나지 않는 드문 단어가 나타날 수 있게 되고
40:18
매우 드문 단어의 의미도 포착할 수 있는 시간을 투자하게 됩니다.
40:23
통계의 효율적인 사용으로
40:27
작은 코퍼스들의 더 작은 벡터 크기에서도 잘 작동합니다.
40:34
개별화되는 부분이 헷갈릴 수 있겠지만,
40:37
우리가 계속 하나의 벡터만 보았지만 skip gram
40:41
벡터와 같은 v 벡터, 외부 벡터와 내부 벡터가 있습니다.
40:46
좀 헷갈리는 것을 넘어서면
40:50
여러분이 얻을 수 있는 다양한 옵션이 많이 있다는 것을 알게 될 것입니다.
40:54
결국 이 두 벡터로부터 하나의 벡터를
40:57
연결(concatenate)할 수 있지만
40:59
그것들을 합치는 것(sum)이 가장 효과가 있는 것으로 밝혀졌습니다.
41:02
본질적으로 동시 발생의 빈도를 세는 것을 포착했기 때문에
41:04
합치는 것이 실제로 가장 잘 작동하는 것으로 판명되었습니다.
41:09
직관을 파괴하는 요소가 있다면
41:13
어떤 일이 일어나야 하는데, 실제로는 가장 잘 작동하는 것으로 밝혀졌습니다.
41:17
>> [안들림] >>  (질문)U와
41:21
V는 무엇입니까? 여기 U는 모든 단어의 벡터입니다.
41:25
여기 skip-gram과 마찬가지로 내부와 외부 벡터가 있습니다.
41:30
여기서 u와 v는 열의 벡터와 행의 벡터입니다.
41:34
둘은 본질적으로 상호 교환 가능하며,
41:37
그것들을 합하는 것이 더 합리적입니다.
41:39
음, 왜 벡터 세트를 하나만 가지고 하지 않나요? 라고 말할 수 있습니다.
41:43
그러나 덜 잘 동작한 목적함수를 보게 되면 그것은
41:48
두 세트의 동일한 매개 변수 사이에 내적이 있기 때문인 것을 알게 될 것입니다.
41:54
그 이유는 분리된 벡터를 갖는 최적화의 관점에서 밝혀졌고
41:57
최적화 과정에서 최종적으로 결합하는 것이 훨씬 더 안정적이었습니다.
42:07
맞습니다.
42:08
질문은 skip-gram도 그렇습니까?
42:10
skip-gram도 합치는 것이 일반적입니까? 입니다.
42:12
네. 그리고 좋습니다. 
42:15
이러한 선택을 하게 되면 모든 프로젝트에 대해 다소 임의로 한 것처럼 보입수 있습니다.
42:19
두 가지가 있고 여러분은 최상의 것을 할 수 있는 것과 같습니다.
42:22
여러분은 저에게 와서 제가 어떻게 해야 하죠?
42:25
X 아니면 Y요? 라고 말할 수 있습니다.
42:26
그리고 진실한 답은,
42:27
특히 프로젝트에 더 가까이 다가가서 더 많은 연구와
42:31
새로운 종류의 응용 프로그램을 하고 있다면, 최선의 대답은 항상 그들 모두를 시도하라는 것입니다.
42:37
그 다음 실제 측정 항목에 모든 측정 항목의 양적 측정을 하고
42:42
마지막 프로젝트에
42:46
그것이 무엇을 했는지를 매우 구체적으로 설명하는 멋지고 작은 테이블을 그려 넣을 수 있을 것입니다.
42:50
여러 번 그렇게 하고 나면 직감을 얻을 수 있을 수 있습니다.
42:53
5 번째 프로젝트의 경우.
42:56
이 경우는 대개 합이 최상으로 작동하기 때문에 계속 그렇게 해야겠다고 생각할 것입니다.
43:00
특히 현장에 들어가면
43:01
다른 단초들과 하이퍼 파라미터를 많이 사용하는 것이 좋습니다.
43:05
>> [안들림] >> 네,
43:11
그들은 모두 같은 규모입니다.
43:13
실제로 꽤 교환 할 수 있습니다, 특히  Glove 모델은 그렇습니다.
43:34
(질문) 그게 질문이 맞습니까?
43:35
좋아요, (질문을) 반복하려고 노력하겠습니다.
43:36
이론적으로는 우리가 옳습니다.
43:39
그래서 문제는 이 벡터들의 크기가 중요하냐? 는 것인데
43:46
이 질문이 맞나요?
43:49
그렇습니다.
43:50
그러나 결국에는 기본적으로 비슷한 문맥으로 볼 수 있습니다.
43:56
여기 이 로그에
44:00
결국 로그 카운트를 포착해야 합니다. 맞습니까?
44:03
그래서 이 로그 카운트가 보통 일정 크기로 되어야 할 것입니다.
44:09
그 다음에 모델이 결국 
44:12
대략 같은 위치에 있다는 것을 알아낼 것입니다.
44:14
최적화에 있어서 어떤 벡터를 사용하여 실제로 얻을 수 있는 것은 없습니다.
44:18
물론 빈번하게 나타나는 커다란 단어의 벡터는 매우 큽니다.
44:21
그래서 이 용어를 정확히 여기에 써 놓은 이유입니다.
44:24
기본적으로 매우 빈번한 단어의 중요성을 제한한 것입니다.
44:45
네, 그렇습니다. 질문은 다시 말하면, 
44:51
skip-gram 모델은 한 번에 하나의 윈도우에서 동시 발생하는 단어를 포착하려고 합니다.
44:57
그리고 Glove 모델은
45:01
이 단어들이 얼마나 자주 함께 나타나는지 라는 전체 통계의 수를 포착합니다.
45:06
질문 하나 더 받겠습니다.
45:06
하나 더 있나요?
45:10
없나요?
45:11
좋습니다.
45:11
이제 재미있는 결과를 볼 수 있습니다.
45:14
그리고 근본적으로, 우리는 최근접 이웃(nearest neighbors)들로
45:19
개구리에 이 모든 다양한 단어들이 포함되어 있습니다.
45:21
우리는 처음에는 조금 걱정스럽게 지켜보았지만
45:24
실제로 아주 좋았습니다.
45:25
아주 드문 단어들조차도 여기서 볼 수 있습니다. Glove는
45:30
이 공간에서 최근접 이웃을 보여주고 있습니다.
45:34
평가는 다음에 하고
45:36
그 전에 우리는 Arun과 약간의 막간 시간을
45:42
보낼 것입니다.
45:47
>> [소리] 우리는 단어 벡터에 대해 이야기 해왔습니다.
45:52
다의어(Polysemy)에 대해 이야기하겠지만 잠시 우회하겠습니다.
45:57
지금까지 단어 벡터의 유사성을 인코딩한 것을 보았고
46:01
비슷한 개념이 유클리드 공간에서 서로 가깝게 분포되어 있었습니다.
46:06
그리고 제가 생각했던 질문은, 우리가 동의어에 대해서는 무엇을 할 수 있는 가라는 것이었습니다.
46:11
tie 같은 단어가 있다고 가정해 보십시오.
46:12
좋아요, tie는 게임에서 비기다와 같은 것을 의미 할 수 있습니다.
46:16
그러면 아마 이 클러스터 근처에 있어야 할 것입니다.
46:21
옷 조각일 수도 있습니다.
46:24
그렇다면 이 클러스터 근처에 있어야 합니다.
46:26
그것이 땋거나 꼬는 것과 같은 활동일 때는 이 클러스터에 근처에 있어야 합니다.
46:31
어디에 있는 것이 거짓일까요?
46:33
그래서 Sanjeev Arora와 전체 그룹은
46:37
이 논문에서 이 질문에 대답하려고 했습니다.
46:41
첫 번째 발견한 것으로
46:45
이 유의어 벡터를 분리하는 것을 생각해 볼 수 있습니다.
46:50
먼저, 스포츠 경기에 대해 이야기할 때의 tie와
46:53
옷에 대해 이야기 할 때, 두 번째 의미의 tie.
46:57
등에서 실제  tie의 모든 단어 조합의
47:02
벡터가 선형 중첩에 놓여 있게 됩니다.
47:07
어떻게  벡터가 모든 것들에 가깝습니까? 라고 의아해 할 수도 있지만
47:11
지금 이것을 2D 평면으로 투영하고 있기 때문입니다.
47:15
실제로 다른 차원에서는 더 가깝습니다.
47:19
우리는 이  tie가
47:23
서로 다른 센스의 면에서는 어떤 의미인지, 그리고
47:29
실제로 서로 다른 센스가 무엇인지가 궁금했습니다.
47:33
우리가 이 단어 tie만 볼 수 있다고 가정하고, 계산적으로 이것을 알아낼 수 있을까요?
47:38
스포츠 의류 등의 의미를 갖는 tie의 핵심 로직과
47:45
두 번째로 희소 코딩(sparse coding)
47:47
알고리즘으로
47:49
그것들을 복구 할 수 있습니다.
47:51
알고리즘이 어떻게 작동하는지 코딩하는 방법을 정확히 설명 할 시간이 없어서
47:55
모델만 설명하겠습니다.
47:56
모델은 우리가 가진 모든 단어 벡터를
48:01
컨텍스트 벡터라고 불리는 작은 선택된 수의 합으로 구성하는 것입니다.
48:08
문맥 벡터는 2000 개에 불과합니다.
48:11
전체 말뭉치의 모든 단어 걸쳐서 공통적입니다.
48:14
그러나 tie와 같은 모든 단어들은 작은 수의
48:17
컨텍스트 벡터로 구성될 뿐입니다.
48:19
따라서 컨텍스트 벡터는 스포츠와 같은 것일 수 있습니다.
48:22
노이즈를 추가했지만 중요한 것은 아닙니다.
48:26
그래서 tie 같이 나온 아웃풋에는
48:30
스포츠 등의 의복과 관련이 있었습니다.
48:34
매우 흥미롭게도 음악에 대한 결과도 볼 수 있습니다.
48:37
여러분 중 일부는 납득할 만하다고 생각할 수도 있고
48:41
질적으로 어떤지 궁금해 할 수도 있을 것입니다.
48:45
우리가 밝혀낸  센스가 얼마나 좋은지를 정량적으로 평가할 수 있는
48:48
방법이 있을까요?
48:50
그렇다고 밝혀졌습니다. 그리고 여기에 일종의 실험 설정이 있습니다.
48:56
WordNet에서 가져온 모든 단어에 대해,
49:00
관련된 센스의 약 20 세트의 번호를 선택했더니
49:05
거기에는 넥타이, 블라우스, 또는 바지 같은 감각을 대표하는 단어의 무리
49:09
가 있었고 또한 컴퓨터, 마우스, 키보드와는 전혀 무관 한 것도 있었습니다.
49:14
그래서 대학원생들을 기니피그로 삼아서 물었습니다.
49:19
이 단어들 중 어느 단어가 tie에 해당하는 것을 구분하십시오.
49:24
그래서 그들이 알고리즘을 구별 할 수 있는지 물어 보았습니다.
49:28
흥미로운 점은,
49:31
이 방법의 성능이
49:35
비 원어민 학생들과 같은 수준이었다는 것입니다.
49:40
재미있는 점은
49:42
원어민이 작업을 더 잘 수행했습니다.
49:46
요약하면 단어 벡터는 실제로 유의어를 포착 할 수 있습니다.
49:50
이 유의어의 단어 벡터는
49:53
유의어 벡터의 선형 중첩에 있습니다.
49:56
유의어 단어는 희소 코딩을 통해 센스를 복귀해 낼 수 있습니다.
50:01
그리고 우리가 복귀한 단어의 센스는
50:05
영어 원어민이 아닌 사람의 수준 정도였습니다.
50:07
고맙습니다.
50:08
>> 멋지네요, Arun 감사합니다.
50:09
>> [박수] >> 좋아요,
50:15
이제 단어 벡터를 평가하는 것을 할 차례입니다.
50:18
이제 우리는 지금 기계 (학습 실험)들을 많이 보았습니다.
50:23
그리고 말합니다. 음, 이게 실제로 얼마나 잘 작동할까요?
50:25
이 모든 하이퍼 파라미터들을 가지고 있는데
50:27
윈도우 크기는 어떻게 할 것인가?
50:29
벡터 크기는 얼마로 하죠?
50:30
와 같은 질문들을 생각해 냈습니다.
50:32
그것들을 어떻게 선택해야 할까요?
50:35
그에 대해 대답해 보면
50:37
글쎄요, 적어도 그들 중 일부는.
50:39
매우 높은 수준에서, 많은 프로젝트에서도 마찬가지입니다.
50:43
본질적으로 높은 수준의 결정을 내릴 수밖에 없는데,
50:48
프로젝트를 하더라도 내재적(=클러스트 그 자체에 대한 모델 평가)으로 외재적(=클러스터된 모델의 유용성에 대한 평가)으로  평가하게 됩니다.
50:52
그리고 단어 벡터의 경우도 마찬가지입니다.
50:56
따라서 본질적으로 평가는 보통 특정되어 있거나 중간에 있는 하위 작업이 있습니다.
51:01
예를 들어, 우리가 벡터의 차이 또는 벡터의 유사성 얼마나 잘 수행되는지 살펴보려고 하면
51:06
내적은 인간의 유사성 판단과 관련이 있습니다.
51:10
그리고 우리는 이 두 가지 종류의 평가를 통해
51:13
몇 장 뒤에 슬라이드를 통해
51:15
내재적 평가의 이점은 계산하기가
51:18
매우 빠르다는 것이다.
51:19
우리는 우리의 벡터를
51:20
유사성 상관 연구를 통해 빠르게 실행할 수 있습니다.
51:24
그리고 우리는 숫자를 얻고 매우 빨리 승리를 주장 할 수 있게 됩니다.
51:28
그 다음 모델을 수정하고  50,000 가지의 작은 단초들을 이용해서
51:33
조합하여 조정할 수 있습니다.
51:37
때로는 시스템이 어떻게 작동하는지, 시스템이 어떻게 작동하는지,
51:42
하이퍼 파라미터의 종류도 실제로 이 유사성 행렬에 영향을 미칩니다.
51:46
예를 들면.
51:48
공짜 점심은 없다는 말과 같이
51:51
분명하지는 않거나 
51:55
중간의 또는 내재적인 평가 및 개선은 실제로
51:59
실제로 사람이 관심을 가질만한 작업으로도 향상될 것입니다. 
52:02
실제 사람들은 약간 까다로운 정의인데
52:05
저는 실제 사람들을
52:06
보통의 평범한 사람들처럼 생각하고 있습니다.
52:09
기계 번역 시스템이나 질의 응답 시스템 또는 그와 관련 있는
52:14
언어학자나
52:15
자연어 처리 연구원일 필요는 없습니다.
52:18
때때로, 우리는 실제로 사람들에게 관찰하게 해서
52:22
그들의 내재적인 평가를  통해 최적화하려고 노력할 필요가 있습니다.
52:25
그 분들은 그 분들의 삶을 최적화 하느라고 여러 해를 보냈으니까
52:28
나중에 그 사람들의 개선 사항이 여러분의
52:32
내재적인 작업, 즉, 실제로 더 나은 단어 벡터를 적용하거나
52:35
개체명 인식이나 품사 태깅 또는
52:38
기계 번역 등을 개선하는 데 영향을 줄 수 있습니다. 개선을 하지 못했다고 한다면
52:41
문제는 기본적인 평가 작업은 얼마나 유용한가 하는 것입니다.
52:45
이 길을 따라 가면 많은 사람들이 프로젝트에 참여할 것입니다.
52:49
항상 우리는 이들 사이에 일종의 상관관계를 확립하고 있는지 확인하고 싶습니다.
52:53
이제 외재적인 것은 기본적으로 실제 작업에 대한 평가입니다.
52:56
그것은 정말로 아이디어가 진지해지는 것이고
53:00
무엇이든 간에 증거를 찾는 것이 될 것입니다.
53:02
문제는 매우 오랜 시간이 걸릴 수 있다는 것입니다.
53:05
여러분이 새로운 단어 벡터를 가지고 있고 
53:07
core currents matrix 의 원래 수를 세는 대신에 피어슨 상관관계를 사용했고
53:10
그것이 가장 좋은 것이라고 생각합니다.
53:13
그리고 이제 그 단어 벡터가
53:16
기계 번역에 정말로 도움이 되는지 평가하고 싶습니다.
53:17
그러면 여러분은, 좋아, 이제 나는 이 단어벡터로
53:19
기계 번역 시스템에 연결해야겠다고 생각할 것입니다.
53:21
그리고 그것은 훈련하는 데 일주일이 걸리는 것으로 판명되었습니다.
53:24
그러면 오랜 시간을 기다려야 합니다. 이제 10 개의 다른 단초들이 있습니다.
53:27
그러면 우리가 그것을 다 알기도 전에, 그 해가 끝날 수도 있습니다.
53:28
그리고 우리는 
53:32
초기의 단어 벡터에 작은 개선사항을 가지고 있을 때마다 그렇게 할 수는 없습니다.
53:36
문제는 오래 걸린다는 것입니다.
53:40
그리고 종종 사람들은 흔히
53:44
서브시스템을 튜닝 하는데 많은 실수를 범할 것입니다
53:44
그 다음에 전체 시스템의 기계 번역 같은
53:49
실제 작업을 하고
53:51
이렇게 해서 전반적으로 개선된 부분이 있습니다.
53:53
그러나 어느 부분이 실제로 향상 되었는지는 명확하지 않습니다.
53:56
어쩌면 실제로는 두 부분이 있다면 한 부분은 정말 좋아졌고 다른 부분은 나빠졌을 수도 있는데
54:00
서로를 지워나가는 것일 수도 있습니다.
54:01
그래서 기본적으로, 외부 평가를 할 때,
54:05
우리가 생각해 낸 것 중 하나만 바꾸거나
54:09
단어 벡터의 한 측면을 바꾸는데 주력하는 것이 좋습니다, 예를 들면.
54:12
전반적인 다운 스트림 작업을 개선했다면,
54:15
정말로 좋은 곳에 있는 것입니다.
54:18
그러니 좀 더 명시적으로
54:20
이러한 본질적인 단어 벡터 평가를 거쳐야 합니다.
54:25
매우 인기가 있었고 최근에 나온 것으로
54:30
word2vec 논문은 단어 벡터의 유추였습니다.
54:35
그들이 발견 한 것은 사람들을 매우 놀라게 했습니다.
54:40
의미와 통사적 유추를
54:46
벡터의 코사인 거리를 통해 포착할 수 있습니다.
54:51
예를 들어, 남자와 여자는 ​
54:56
왕과 무엇이 갖은 관계와 같은 지 물어볼 수 있을 것입니다.
55:00
기본적으로 간단한 비유로
55:03
남자 대 여자는 왕과 여왕의 관계와 같습니다.
55:06
맞습니다.
55:07
그래서 여자의 벡터를 취하면,
55:11
거기서 사람의 벡터를 빼고, 왕의 벡터를 더합니다.
55:15
그리고 가장 큰 코사인 유사성을 갖는 벡터를 찾습니다.
55:21
여왕의 벡터가 실제로
55:26
이 용어와 가장 큰 코사인 유사도를 갖는 벡터라는 것은
55:31
매우 놀랍습니다.
55:34
매우 직관적인 패턴의 많은 다른 종류 중
55:38
두어 개를 더 살펴보겠습니다.
55:40
비슷한 것으로 만약 sir과 madam이  man, woman,와 ​​비슷하다면,
55:45
또는 heir과 heires, 또는  king과 queen, emperor과 empress 등등
55:50
모두 비슷한 종류의 관계를 잘 포착할 수 있을 것입니다.
55:56
코사인 거리를 이용한 이 간단한 유클리드 뺄셈과 덧셈은
56:05
더욱 구체화됩니다.
56:06
비슷한 종류의 회사와 CEO 이름이 있습니다.
56:11
화사와 타이틀을 취해서 CEO를 빼고 다른 회사를 더하면,
56:15
그 다른 회사의 CEO 이름의 벡터를 접하게 됩니다.
56:21
그리고 그것은 의미론적 관계뿐만 아니라
56:24
통사적 관계인 slow, slower, 또는 slowest역시 glove
56:29
에서 사물은 매우 비슷한 차이를 보여주고 있습니다.
56:34
short, shorter, 그리고 shortest, 또는 strong, stronger, 그리고 strongest.
56:39
이걸로 많은 즐거움을 누릴 수 있고 더욱 재미있는 것은
56:44
Sushi- Japan + Germany은 bratwurst로 되는 등 
56:50
독일인으로서는 가볍게 불쾌감을 느낄 수 있지만
56:53
물론 어떤 면에서는 매우 직관적입니다.
56:59
그러나 또한 이런 의문도 듭니다.
57:00
어쩌면 그것은 [안들림] 
57:02
다른 전형적인 독일 음식일 수도 있을 것입니다.
57:08
이것은 매우 직관적이며 일부 사람들에게는 실제
57:13
여기에 포착된 의미론이 왜 일어났는지 궁금할 것입니다.
57:18
왜 이렇게 되었는지에 대한 수학적 증거는 없지만
57:23
직관적으로 우리는 그것을 조금 이해할 수 있습니다.
57:27
예를 들어 최상급 단어는 특정 단어 옆에 나타날 수 있습니다.
57:33
매우 유사한 경우가 있습니다.
57:37
어쩌면 대부분의 경우, 예를 들어, 많은 최상급 앞에 나타납니다.
57:43
또는 slower 또는 shorter는 
57:52
It's barely shorter than this other person처럼 특정 단어 앞에 나타날 수 있는 경우가 제한적입니다.
57:55
그래서 이러한 벡터는 핵심적인 출현으로 세기 때문에,
58:00
동시발생하는 것으로 다른 동시발생하는 것을 빼는 것으로 이해할 수 있지만
58:05
직관적인 헤아림일뿐
58:07
좋은 수학적 증거는 아닙니다.
58:11
직관적으로 우리는 비슷한 종류의 단어가 어떻게 나타나는지를 볼 수 있습니다.
58:16
따라서 우리는 비슷한 종류의 위치에 벡터 공간으로 도착하게 됩니다.
58:20
이제 처음에 두 가지를 시도해 보고 이 기술이 잘 작동한다는 것에 놀랐을 것입니다.
58:25
그 다음에 좀 더 정량적으로 만들고 싶어질 것입니다.
58:27
네
58:28
이것은 믿을 수 없을 만큼 잘 작동하는 몇 가지 단어의 질적 하위 샘플이 있었습니다.
58:33
우리가 그걸 가지고 놀면서 
58:36
잠시 동안, 우리는 비슷한 것을 발견 할 수 있을 것입니다.
58:38
독일에서 아우디를 빼면 이상한 초밥 용어 등이 나타나기도 하고
58:42
항상 의미 있는 것은 아니지만
58:44
놀라울 정도로 직관적인 것이 많이 있습니다.
58:48
그래서 사람들은 이 데이터 세트를 보고
58:53
얼마나 자주 실제로 잘 작동하는지 보기 시작했습니다.
58:58
그래서 그들은 기본적으로 이 Word Vector 유추 작업을 수집하기 시작했고
59:02
여기에 몇 가지 예가 있습니다.
59:04
이 링크에서 이 모든 것을 다운로드 할 수 있습니다.
59:06
이것은 원래 word2vec 논문이며
59:10
선형 관계를 설명하려고 한 것입니다.
59:13
그리고 그들은 기본적으로 시카고와 일리노이, 텍사스 휴스턴을 보고
59:16
그리고 어떤 도시가 그 주에 나타나는지와 같은
59:20
많은 다른 유추를 생각해 냈습니다.
59:23
물론 몇 가지 문제가 있으며 이 측정 항목을 더 많은 행렬을 추가하여 최적화하면
59:27
여러 다른 도시와 다른 주에
59:32
같은 이름들이 실제로 그 도시 이름으로 나타날 수 있습니다.
59:35
그 다음에
59:38
이것이 포착할 수 있는지 아닌지는 말뭉치에 달려 있습니다.
59:40
하지만 여전히 많은 사람들이 그것을 납득하고 있고
59:43
대부분 최소한 조금이라도 최적화해야 합니다.
59:47
다음은 이 데이터 세트에 있는 유추의 몇 가지 다른 예입니다.
59:51
세계의 도시와 같은 것도 마찬가지로, 아는 것과 같이
59:56
우리의 말뭉치가 변경되지 않으면 역시 문제가 될 것입니다.
60:01
그러나 많은 경우에 국가의 수도는 변하지 않으므로
60:04
그것은 매우 직관적이며 여기에 통사론 관계와 유추를 나타내는 몇 가지 예가 있습니다.
60:09
이를 통해 이 데이터 세트를 평가해야 합니다.
60:13
이러한 유추는 수천 가지가 있습니다.
60:16
이제, 우리는 단어 벡터를 계산하고, 단초가 될 만한 것들을 조정했고,
60:19
25 차원 대신에 hyperparameter를 변경했으며, 50 차원에서
60:23
유추를 위해 어느 것이 더 나은지 평가합니다.
60:28
그리고 다시, 과거 시제의 관계를 가진 또 다른 통사론적인 것도 있습니다.
60:33
Dancing과 danced의 관계는 going과 went와 같을 것입니다.
60:37
여기서 기본적으로 많은 다른 방법을 볼 수 있지만 다 알 수는 없고
60:41
여기 클래스 중, skip-gram SG와 Glove 모델을 알고 있습니다.
60:46
그리고 여기에 정량적인 첫 번째 평가가 있습니다.
60:52
기본적으로 의미론과 통사론 관계를 살펴보고,
60:56
그 다음 합계의 관점에서 단지 평균을 내면 됩니다.
60:59
그리고 정확히 말하면, 이 관계가 얼마나 자주 정확한지,
61:05
우리가 여기 데이터 세트에 가지고 있는 이 모든 다른 유추들에 대해서.
61:09
2013 년과 14년에  이 두 논문이 나왔을 때
61:17
기본적으로 GloVe는 이러한 관계를 포착하는 데 있어 최고였습니다.
61:22
그래서 우리는 여기서 흥미로운 것들을 관찰합니다.
61:24
하나는 크기가 더 많은 경우가 있습니다.
61:28
이러한 관계를 더 잘 포착하는 데 실제로 도움이 되지는 않습니다.
61:33
300 차원 벡터보다 1,000 차원 벡터가 더 잘 작동하지 않습니다.
61:38
또 다른 재미있는 관찰과 그것은 다소 슬프게도 사실은
61:42
거의 모든 딥러닝 모델이 데이터가 많을수록 더 잘 작동 할 것입니다.
61:48
42 억 개의 토큰에 단어 벡터를 훈련시키면,
61:52
6 억 개의 토큰보다
61:55
여러분도 아시다시피, 4 % 정도 잘 작동합니다.
61:58
여기에 우리는 똑같은 300 차원을 가지고 있습니다.
62:01
다시 말하면, 하나만 변화시키면 그 하나의 변화가
62:05
실제로 영향을 미쳤는지 알게 됩니다.
62:07
그리고 우리는 여기서 큰 차이를 보게 될 것입니다.
62:17
(질문) 좋은 질문입니다. 어떻게 성과가
62:19
내려가나요?
62:20
그것은 또한 단어 벡터를 훈련시키는 것에 달려 있다는 것을 알 수 있습니다.
62:26
예를 들어 위키피디아에는
62:30
전 세계 모든 수도에 대한 훌륭한 설명이 있습니다.
62:33
하지만 미국 뉴스를 듣고 그 뉴스에 나온
62:38
Abuja와 Ashgabat은 매우 자주 언급하지는 않을 것입니다.
62:43
그러면, 그 단어에 대한 벡터는
62:46
그들의 의미를 아주 잘 포착하지 못할 것이고 결과는 더 나빠질 것입니다.
62:49
그래서 일부는 크다고 항상 좋은 것은 아니고 
62:53
가지고 있는 데이터의 품질에 따라서도 달라집니다.
62:55
그리고 위키피디아는 일반 인터넷 텍스트보다 맞춤법 오류가 적습니다.
62:59
그리고 실제로 아주 좋은 데이터 세트입니다.
63:01
여기 하이퍼 파라미터를 어떻게 결정할 것인가와 같은 질문에 대한
63:06
평가의 일부가 있습니다.
63:10
제프리가 삼년 전에 
63:16
다양한 다른 하이퍼 파라미터에 대해 조심스럽게 분석 한 것이 같습니다.
63:21
우리도 관찰하고 지난번에 언급 된 종류의 것들이네요.
63:23
그래서
63:24
이것은 우리의 프로젝트를 위해 모방하려고 노력해야하는 훌륭한 방법입니다.
63:30
이런 플롯을 볼 때마다 아빠 미소가 지어지고
63:34
바로 성적 향상도 향상시켜 줄 것입니다.
63:36
>> [웃음] >>
63:37
플롯에서 어떤 실수를 범하기도 하는데
63:39
그 부분으로 넘어가겠습니다.
63:41
여기에서는 기본적인 대칭 컨텍스트를 살펴봅니다. 비대칭 컨텍스트는
63:46
현재 단어 뒤에 나타난 단어만 계산하는 것입니다.
63:51
우리는 이전의 것들을 무시하지만 대칭적인 것이 더 잘 작동하는 것으로 밝혀졌습니다.
63:54
여기 벡터 차원은 평가하기에 좋은 차원으로
63:59
얼마나 고차원이어야 하는지를
64:01
알려주는 매우 본질적인 차원입니다.
64:03
기본적으로 매우 작을 때의 관찰로
64:06
이러한 유추를 포착할 수 없다가 약 200,
64:11
300에서 나아지고 그리고 그 이후에는 가늘어지다가 더 이상 좋아지지 않습니다.
64:15
사실상, 그것은 300에서 600 사이의 꽤 평평하게 나타납니다.
64:21
그리고 이것은 좋은 것입니다.
64:22
그래서 우리가 자주 여기에서 보는 주된 숫자는 전체 정확도는
64:26
여기 빨간색입니다.
64:27
그리고 그것은 평평합니다.
64:29
플롯을 만들 때 실수 할 수 있는 부분은
64:34
우리가 어떤 하이퍼 파라미터를 가지고 있고 어떤 종류의 정확성이 있음을 증명할 수 있습니다.
64:39
이것은 벡터 크기도 될 수 있으므로 멋진 플롯을 만들고 나서
64:42
여러분은 결과가 나아졌다고 말 할 수 있습니다.
64:45
제가 이 플롯을 보았다면 제 의견은,
64:49
왜 이 방향으로는 나아가지 않고
64:51
단지 위로 올라가고만 있는 것처럼 보일까요? 입니다.
64:53
이렇게 되는 것은 그다지 좋지 않습니다.
64:56
실제로 플롯을 점점 안 좋아 질 때까지  찾아야 합니다, 그리고
64:59
지금 당장 하이퍼 파라미터에 대한 최적의 값을 찾았다고 하더라도
65:05
여기에서 평가할 또 다른 중요한 점은
65:11
때때로 윈도우 크기의 문제를 고려해야 한다는 것입니다.
65:16
단어 벡터 예를 들어, 아마도 200에서
65:21
전보다 약간 좋았거나 300에서는 약간 더 좋았습니다.
65:25
그러나 더 큰 단어 벡터는 더 많은 RAM을 씁니다. 맞습니까?
65:28
그러면 소프트웨어가 더 많은 데이터를 저장해야 합니다.
65:32
그런데 핸드폰으로 볼 필요도 있고 그렇게 하고 싶을 수도 있습니다.
65:36
내재적인 작업에 2 %의 향상을 얻으려면
65:42
RAM 요구량도 30 % 더 높아집니다.
65:46
음, 저는 2 %의
65:48
내재적인 과제에 대한 정확성에 대해서는 신경 쓰지 않겠습니다.
65:51
저는 여전히 작은 단어 벡터를 선택하겠습니다.
65:53
합리적인 주장이라고 생각하지만, 일반적으로 여기에서,
65:56
이 측정 항목을 최적화하려고 합니다.
66:00
그래서 우리는 이것이 무엇인지 주의 깊게 보고 싶습니다.
66:02
이제 윈도우 사이즈에서, 다시 한 번 말씀드리지만 이것은 우리가 예측하고 계산하는 
66:06
각각의 중심 단어의 왼쪽과 오른쪽에 있는 
66:12
단어의 개수입니다
66:14
약 8 정도 나옵니다. 가장 높은 점수를 얻습니다.
66:18
그리고 또 다시 복잡성과 훈련 시간을 증가시킵니다.
66:23
윈도우문이 길수록,
66:25
이런 종류의 표현을 더 많이 계산해야합니다.
66:30
그리고 비대칭적인 맥락에서는,
66:32
약간 다른 윈도우 크기가 더 잘 작동합니다.
66:37
좋아요, 평가에 대한 질문 있나요?
66:44
네 좋습니다.
66:45
glove와 skip gram 모델을 비교하는 것은 실제로 매우 어렵습니다.
66:50
그들은 많이 다른 종류의 훈련 체제입니다.
66:54
하나는 한 번에 하나의 윈도우를 통과하며,
66:56
다른 하나는 먼저 모든 빈도를 세서 계산 한 다음  그 빈도를 센 것을 기반으로 작동하도록  합니다.
67:01
하지만 노력해서
67:05
직접 비교하는 리뷰어의 질문에 답할 수 있습니다.
67:09
그래서 우리가 여기서 네거티브 샘플을 보았습니다.
67:11
합과 목적 함수가 있었던 게 기억나나요?
67:14
많은 단어가 skip gram 모델의 윈도우문에 나타나지 않았기 때문에 
67:18
우리가 확률을 푸시다운 했었고
67:21
이것은 트레이닝 시간을 늘리는 방법으로 이론적으로는 목표를 더 잘 수행했습니다.
67:28
반면에 다른 반복 시간으로 얼마나 자주 이 동시발생 단어로 나타났는지를 세는 것이고
67:33
GloVe에 대한 공기관계 매트릭스의 각 쌍을 최적화하도록 계산합니다.
67:38
그리고 이 평가에서 GloVe는
67:40
두 모델을 모두 교육한 시간과 상관없이 더 나은 성과를 거두었습니다.
67:45
이미 논의한 것처럼 더 많은 데이터가 도움이 됩니다.
67:52
특히 위키피디아와
67:55
Gigaword는 주로 뉴스 말뭉치라고 생각됩니다.
67:57
그래서 뉴스는, 실제로는 더 좋지만 전반적으로는 효과가 없습니다.
68:05
특히 관계 및 유추를 위한 것이 아니라 의미에서는 더 그렇습니다,
68:11
42 억 개의 토큰으로 구성된 초대형 데이터 세트인 크롤링 자료가 가장 효과적입니다.
68:18
좋습니다. king 빼기 man 더하기 woman은 놀라운 유사성 덕분에
68:21
매우 흥미로웠습니다.
68:25
그 전에 사람들은 종종 상관관계 판단만을 사용했습니다.
68:30
그래서 기본적으로 그들은 많은 사람들, 특히 대학원생에게 질문했습니다. 
68:35
1에서 10까지의 척도로 말하면 이 두 단어는 얼마나 비슷한가?
68:41
그래서 호랑이와 고양이를 주고, 한 명에서 세 명이나 다섯 명에게
68:45
그들이 얼마나 비슷한 지 물어보면 한 명은 7을 주고, 다른 사람은 8이나
68:50
6을 주면 그 평균은 내는 식으로 진행했습니다.
68:53
그래서 여기 컴퓨터와
68:57
인터넷은 7점이고
68:58
그러나 주식과 CD는 비슷하지 않습니다.
69:02
그래서 한 무리의 사람들이 1에서 10까지의 규모로 말할 것이며, 평균은 1.3에 불과합니다.
69:06
>> [안들림] >> 그리고 지금,
69:10
우리는 기본적으로 
69:13
높은 상관관계를 갖도록 단어 벡터를 훈련시키고 싶고
69:19
그 거리는 코사인 유사성 또는 유클리드 거리를 씁니다.
69:24
또는 얼마나 가까운지 보기 위해서 다른 거리 행렬을 시도해 볼 수도 있습니다.
69:29
여기에는 그러한 예가 하나 있습니다.
69:31
우리는 스웨덴의 단어와 코사인 유사도과
69:36
매우 가까운 단어로
69:40
가장 큰 코사인 유사성을 가지는 것을 찾았습니다.
69:46
노르웨이와 덴마크가 가장 비슷하게 나왔습니다.
69:50
이러한 종류의 데이터 집합이 많은 것이,
69:55
WordSim353은 기본적으로 353 개의 단어 쌍이 있습니다.
70:00
그리고 우리는
70:04
벡터 거리와 인간 판단의 상관관계를 잘 볼 수 있습니다.
70:09
상관관계가 높을수록
70:12
우리가 생각하기에 더 직관적인 것은 이 큰 벡터 공간에서의 거리입니다.
70:17
그리고 다시 한 번, Glove는
70:22
WordSim 353과 같은 다른 종류의 데이터 세트에서
70:27
다시 나오지만, 가장 큰 훈련 데이터 세트에 Glove에 가장 점수가 좋습니다.
70:32
단어 벡터 유사성 및 상관관계에 대한 질문이 있읍니까?
70:38
없어요? 네 그럼
70:40
자, 근본적으로, 내재적 평가는 거대한 문제를 가지고 있습니다. 맞습니까?
70:47
우리는 이 좋은 유사성을 가지고 있다고 해도 누가 알겠습니까?
70:49
어쩌면 실제로 우리가 궁금해 하는 실제 작업을 향상시키지 못할 수도 있습니다.
70:53
그리고 가장 좋은 종류의 평가들로 비용이 많이 발생하는
70:57
실제 작업에 하거나 downstream 과제 같은 후속 작업을 할 수 있는 종류들이 있습니다.
71:02
하나의 예제는 개체명 인식이라고 부릅니다.
71:04
상당히 간단하기 때문에 좋고
71:07
실제로 충분히 유용합니다.
71:09
개체명 인식 시스템으로
71:12
회사 이메일이나
71:14
어떤 사람이 어떤 회사와 관계가 있는지를 파악하고,
71:17
그들은 어디에 살고 있는지와 거기에 사는 다른 사람들의 위치 등등을 알 수 있습니다.
71:21
개체명 인식 시스템을 사용하는 것은 실제로 유용한 시스템입니다.
71:26
우리는 기본적인 실제 모델로
71:29
다음 강연에서 개체명 인식을 수행할 것입니다.
71:34
우리가 시작할 단어 벡터의 다운스트림 모델은
71:37
다른 단어 벡터들 중에서
71:41
GloVe벡터가 아주 잘 작동합니다.
71:48
외재적 방법에 관한 질문이 있나요?
71:50
나중에 여기서 작동하는 실제 모델을 살펴보겠습니다.
72:10
(질문) 네 맞습니다.
72:11
글쎄, 우리가 여기에서 어떤 것도 최적화하지 않고 단지 평가만 했습니다.
72:16
아무것도 훈련하지 않았습니다.
72:17
skip-gram의 목적 함수를 사용하여 단어 벡터를 학습했고
72:21
고치고 평가할 수 있습니다.
72:24
그래서 지금 여기서 여러분이 평가하는 것은 스웨덴과
72:28
노르웨이이고 그 사이에 일정한 거리가 있습니다.
72:33
그럼 우리가 기본적으로 
72:37
사람이 두 단어가 얼마나 비슷한 지에 대한 척도를 측정하고 싶고
72:41
인간의 판단이
72:46
벡터의 코사인 거리의 유사성과 비교해서 서로 잘 관련되기를 원합니다.
72:50
그리고 상호 연관성이 있을 때, 벡터들은
72:53
사람들이 가지고 있는 똑같은 종류의 직관이 있다고 판단하므로 잘해야 합니다.
72:57
그리고 직관적으로 스웨덴이
73:01
좋은 코사인 유사성을 가지고 있으며 다른 다운스트림 시스템에 연결했을 때,
73:05
그 시스템이 명명된 개체명을 포착하면 더 좋아질 것입니다.
73:09
어쩌면 트레이닝 시간에 스웨덴의 벡터를 보고
73:13
테스트 시간에 노르웨이의 벡터를 본다면,
73:16
트레이닝 시간에 우리는 스웨덴이 위치라고 알려주면, 테스트 시간에는
73:19
노르웨이 또는 덴마크를 위치로 올바르게 식별할 확률이 더 높습니다.
73:24
왜냐하면 그들은 벡터 공간에서 실제로 가까이 있기 때문입니다.
73:28
그리고 다음 수업에서는 단어 벡터를 훈련시키는 방법의 예를
73:32
진행 할 것입니다.
73:33
또는 다운 스트림 작업도 할 것입니다.
73:35
우리 수업이 5:50까지니까 8 분 정도 남았네요.
73:40
단어 분류에 대해서서 간단히 살펴 보겠습니다.
73:47
이 단어 벡터에 대해 이야기했고 기본적으로
73:54
이 매우 간단한 동시 발생의 빈도를 세는 것으로 시작해서
73:58
Word2vec와 같은 조밀한 벡터를 갖는 것에 비해 매우 희소한 대형 벡터를 봤습니다.
74:04
주요 이점은 기본적으로 유사한 단어 cluster를
74:09
더 강건하게 분류 할 수 있습니다.
74:17
우리가 훈련 데이터 세트에서 볼 수 없는 다양한 종류의 단어,
74:22
예를 들어, 국가 이름들이 함께 모이기 때문에
74:25
우리의 목표는 위치 단어를 분류하는 것입니다.
74:29
이 모든 국가 단어는 벡터 공간의 비슷한 부분에 도록 초기화하면 더 잘할 것입니다.
74:35
나중에 우리는 이 벡터들을 실제로 조정할 것입니다.
74:39
그래서 지금 비감독 목적 함수(unsupervised objective function.)를 배웠습니다.
74:43
즉, 사람이 각 입력 부분에 수동으로 작업한 레이블 없이
74:47
기본적으로 큰 말뭉치를 취해서
74:52
비감독 목적 함수로 가르쳤습니다.
74:56
하지만 실제로  다른 작업에서는 잘 작동하지 않습니다.
74:59
예를 들어 감성 분석은
75:06
좋은 단어와 나쁜 단어가 실제로 단어 벡터에서는 유사한 문맥에 나타날 수 있기 때문에
75:12
가령, 나는 이 영화가 정말로 좋았거나 나쁘다고 생각했다가
75:16
따라서 다운 스트림 작업이 감성 분석 일 때
75:19
무작위로 단어 벡터가 초기화될 수 있습니다.
75:23
이것은 듣고 나서
75:26
그러면 어떻게 단어 벡터가 훈련되어야하는지에 대해서
75:30
너무 초조해할 필요가 없습니다. 많은 경우 단어 벡터가 여러분의 첫 걸음에 도움이 될 것입니다.
75:35
그러나 항상 그런 것은 아닙니다.
75:38
그리고 그것이 우리가 평가할 수 있는 것이 될 것입니다.
75:41
그럼 단어를 그냥 무작위로 초기화 할 수 있을까요?
75:43
Word2vec 또는 glove 모델로 초기화해야 할까요?
75:47
우리가 단어를 분류하려고 할 때, 사용할 것은 softmax입니다.
75:52
이 강의의 첫 번째 슬라이드에서
75:56
우리는 이미 이 방정식을 보았습니다.
75:57
표기법을 조금 바꿀 텐데, 
76:02
이런 종류의 표기법을 사용하는 것이 더 쉬울 것입니다.
76:06
이것이 우리가 최적화 할 Softmax가 될 것입니다.
76:11
기본적으로는 로지스틱 회귀라는 단어의 다른 용어일 뿐입니다.
76:15
그리고 우리는 많은 경우에 일반적으로 다른 클래스에 대해 행렬 W를 사용합니다.
76:22
예를 들어, x는 단순한 형태의 단어 벡터일 뿐입니다.
76:27
우리는 단지 단어 벡터를 다른 컨텍스트 없이
76:31
이 위치인지 아닌지를 가지고 분류하려고합니다.
76:32
아주 유용하지는 않지만 교육적인 이유로 
76:37
x라는 입력은 단어 벡터일 뿐입니다.
76:39
그리고 위치인지 아니면 위치가 아닌지로 분류하려고 합니다.
76:43
그 다음에 기본적으로 다른 종류의 단어 벡터를 제공합니다.
76:47
예를 들어 스웨덴과 노르웨이를 계산한 다음
76:52
현재 핀란드, 스위스의 위치에서 예 또는 아니오로 계산하려고 합니다.
76:56
이것이 바로 과제입니다.
76:58
여기 있는 소프트 맥스는 가장 단순한 경우로 두 가지가 있습니다.
77:04
두 가지가 실제로 의미가 없으므로 여러 개의 다른 클래스가 있다고 가정해 보겠습니다.
77:09
각 클래스에는 여기에 하나의 행벡터가 있습니다.
77:12
그래서 이 표기법은 본질적으로 우리가 가지고 있는 행의 수입니다.
77:18
그래서 우리가 가진 특정 행이겠죠.
77:20
그리고 우리는 여기에이 rho 벡터를 갖는 내적에 이 열 벡터 x를 곱합니다.
77:26
그 다음에 우리가 항상 하는 것처럼 정규화를 하고
77:30
여기에 전반적인 벡터를 구하기 위한 로지스틱 회귀를 합니다.
77:35
모든 클래스의 합은 1이 될 것입니다.
77:38
따라서  W는 일반적으로 분류를 위한 C 와 d 차원의 행렬이 될 것이다.
77:44
여기서 d는 우리의 입력이고 C는 우리가 가지고 있는 클래스의 수입니다.
77:50
그리고 다시, 로지스틱 회귀는 softmax 분류에 대한 다른 용어일 뿐으로
77:57
그리고 softmax에 대한 좋은 점은
78:03
여러 다른 클래스에 대해서도 일반화할 수 있다는 것입니다.
78:05
그래서 기본적으로 이것은 우리가 이미 다루었던 것입니다.
78:11
따라서 손실 함수는 모든 이후 강의에서 유사한 용어로 사용할 것입니다.
78:15
손실 함수, 비용 함수 및 목적 함수는  상호 교환적으로 사용할 것입니다.
78:20
그리고 softmax를 최적화하기 위해 우리가 사용하는 것은 cross entropy loss 입니다.
78:25
이제 끝내야 할 것 같네요.
78:29
1 분 남았지만 지금 시작하면 너무 늦을 것 같아서
78:33
여기까지 하겠습니다. 감사합니다.
78:36
[박수]